[{"content":"After switching from Windows to Mac, I have reconsidered a few of the applications I use. To my own surprise I ended up with a paid one that’s only available on Apple devices – Bear.\nThe itch I’ve been using Microsoft OneNote for over a decade and I’ve been fairly happy with it. It’s free, feature-rich, works on all devices and so on. There are two things that I find a bit frustrating though.\n Keyboard navigation is limited and flaky. Here is an example where I try to check a checkbox using ⌘1 \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Font family and size gets mixed when I edit the same note on different devices Wanted a better writing experience with the possibility to copy and paste to Markdown What I tried I dismissed a few other note taking apps in the process.\nApple’s Notes. If you’re going to limit yourself to the Apple eco system, you have to consider Apple’s Notes, but I find the keyboard navigation limited and clunky (normal paste, without formatting, is ⌥⇧⌘V 😱). Copying to Markdown also isn’t a thing.\nEvernote. Haven’t tried this, but coming from OneNote, this seems to be pretty similar.\nSimplenote. Doesn’t support images.\nBoostnote. Focused on code and no iOS app.\nStandard Notes. Have used this for a while in offline mode, but lacks keyboard navigation and doesn’t look very good.\nWhy I chose Bear Bear is the one I’m most impressed with and the one I switched to.\nGreat keyboard navigation puts Bear on top the mentioned competitors for me. I can find my way around without touching the mouse or trackpad, like I try to do in most apps.\nIt looks really good, with a simple but functional interface (⌃1 hides everything except the current note). Also has a bunch of themes to choose from.\nThe way Markdown syntax is shown with the applied formatting (without a separate preview mode, is the best I’ve seen an editor handle Markdown. This means it’s pleasant to write in Markdown (using Markdown syntax or keyboard shortcuts) and see the formatted result at the same time. This post is written in Bear before copied into my Hugo repo with some minor formatting changes for images.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r The share implementation on iOS is a killer feature I discovered after I made the decision to use Bear. For many years I’ve followed the Getting Things Done methodology, but never really found an inbox page useful within my digital notes. When I occasionally want to save a link for later from on my phone, I now have a place to dump it (where I know I will look later). Bear lets me paste a link into the beginning or end of a note I select, so I have one for that purpose.\n\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rInkorg 🇸🇪 = Inbox 🇬🇧\r\r\r\r\r\r\rInkorg 🇸🇪 = Inbox 🇬🇧\r\r\r\n","dateformatted":"6, January 2021","dateiso":"2021-01-06T21:34:00+02:00","ref":"/my-favourite-note-taking-app-for-mac-and-iphone/","summary":"After switching from Windows to Mac, I have reconsidered a few of the applications I use. To my own surprise I ended up with a paid one that’s only available on Apple devices – Bear.","tags":["Bear"],"title":"My Favourite Note Taking App for Mac and iPhone"},{"content":"From the past months of iterating on a CLI for managing micro services in our company, I\u0026rsquo;ve drawn some conclusions regarding usability.\nThe problem As the number of commands, arguments and flags have grown, so has the risk of confusion. Naming things is not the easiest thing in our industry – should the command for creating a service be called create, init or add? Since it\u0026rsquo;s idempotent (can be run multiple times), should it instead be called ensure?\nAfter changing this back and forth, to ensure/create/add confusion, we sort of gave up and used aliases.\nThe solution We use Oclif (The Open CLI Framework), which provides a few features we can use.\nAliases By using aliases, you don\u0026rsquo;t have to think too hard on whether it\u0026rsquo;s supposed to be get/set, show/edit or list/add/remove.\nexport default class UpstreamsShow extends Command { static aliases = [\u0026#39;upstreams:get\u0026#39;, \u0026#39;upstreams:list\u0026#39;] static description = \u0026#39;Show service upstreams\u0026#39; static usage = \u0026#39;upstreams:show\u0026#39; static examples = [ `$ aw upstreams:show -s hello-cats -e dev Lists the upstream services for service \u0026#39;hello-cats\u0026#39; in environment \u0026#39;dev\u0026#39;.`, `$ aw upstreams:show Lists the upstream services for current service (from repo). You will be prompted for environment.`, ] ... } Help By making sure all commands have a description, usage and possibly examples, --help becomes better. Examples are really good for commands with several arguments or flags. When using topics in Oclif, it\u0026rsquo;s also helpful to add command descriptions in package.json as described on the topics documentation page\nAutocomplete I also find the autocomplete plugin helpful, since you can just tab your way to the full command.\n\r","dateformatted":"15, November 2020","dateiso":"2020-11-15T20:34:00+02:00","ref":"/usability-learnings-from-building-a-cli/","summary":"From the past months of iterating on a CLI for managing micro services in our company, I've drawn some conclusions regarding usability. I'll describe the features of OCLIF we've used to address the challenges.","tags":["oclif"],"title":"Usability Learnings from Building a CLI"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r As new to Jenkins, I made a mistake that led to confusing errors. Working with .groovy files in Jenkins shared libraries isn\u0026rsquo;t the most joyful experience I\u0026rsquo;ve had in my career, but this makes total sense once I saw the obvious.\nWe have bunch of build agents that perform the actual builds in Jenkins (Docker containers running in Nomad). The initial pipeline defined an agent for every stage, resulting in \u0026ldquo;random\u0026rdquo; build failures. Sometimes you would get the same agent in all stages and everything was fine, but other times you would get different agents in a single pipeline (revealed by the build logs).\nThis isn\u0026rsquo;t very good when you have stages that depend on state (on the local file system) from the previous stage.\nBad version \r\r\r#!/usr/bin/env groovy def call(Map config) { pipeline { agent none stage(\u0026#39;Build\u0026#39;) { agent {node {label \u0026#39;nomad-jenkins-agent\u0026#39;}} steps { script { // stuff that changes local folder (workspace) } } } stage(\u0026#39;Approve\u0026#39;) { agent {node {label \u0026#39;jenkins\u0026#39;}} timeout(60) { script { def approve = input id:\u0026#39;deploy\u0026#39;, message: \u0026#39;Ok to deploy?\u0026#39;, ok: \u0026#39;Ok\u0026#39;, parameters: [choice(name: \u0026#39;Select\u0026#39;, choices: \u0026#39;yes\\nno\u0026#39;, description: \u0026#39;Deploy?\u0026#39;)], submitterParameter: \u0026#39;user\u0026#39; env.DEPLOY = \u0026#34;${approve.Select}\u0026#34; echo \u0026#34;Selected action: ${env.DEPLOY}\u0026#34; } } script { if(\u0026#34;${env.DEPLOY}\u0026#34; == \u0026#39;no\u0026#39;){ currentBuild.result = \u0026#39;ABORTED\u0026#39; } } } stage(\u0026#39;Deploy\u0026#39;) { when { environment name: \u0026#39;DEPLOY\u0026#39;, value: \u0026#39;yes\u0026#39; } agent {node {label \u0026#39;nomad-jenkins-agent\u0026#39;}} steps { // trying to use stuff from local folder (workspace) } } } }\r\r By simply defining an agent at the top level, the issues were gone. We can still \u0026ldquo;override\u0026rdquo; that value in a stage if needed, like in our Approve step.\nBetter version \r\r\r#!/usr/bin/env groovy def call(Map config) { pipeline { agent {node {label \u0026#39;nomad-jenkins-agent\u0026#39;}} stage(\u0026#39;Build\u0026#39;) { steps { script { // stuff that changes local folder (workspace) } } } stage(\u0026#39;Approve\u0026#39;) { agent {node {label \u0026#39;jenkins\u0026#39;}} timeout(60) { script { def approve = input id:\u0026#39;deploy\u0026#39;, message: \u0026#39;Ok to deploy?\u0026#39;, ok: \u0026#39;Ok\u0026#39;, parameters: [choice(name: \u0026#39;Select\u0026#39;, choices: \u0026#39;yes\\nno\u0026#39;, description: \u0026#39;Deploy?\u0026#39;)], submitterParameter: \u0026#39;user\u0026#39; env.DEPLOY = \u0026#34;${approve.Select}\u0026#34; echo \u0026#34;Selected action: ${env.DEPLOY}\u0026#34; } } script { if(\u0026#34;${env.DEPLOY}\u0026#34; == \u0026#39;no\u0026#39;){ currentBuild.result = \u0026#39;ABORTED\u0026#39; } } } stage(\u0026#39;Deploy\u0026#39;) { when { environment name: \u0026#39;DEPLOY\u0026#39;, value: \u0026#39;yes\u0026#39; } steps { // trying to use stuff from local folder (workspace) } } } }\r\r ","dateformatted":"25, October 2020","dateiso":"2020-10-25T11:34:00+02:00","ref":"/using-same-node-in-jenkins-groovy-pipeline/","summary":"As new to Jenkins, I made a mistake that led to confusing errors. Working with .groovy files in Jenkins shared libraries isn’t the most joyful experience I’ve had in my career, but this makes total sense once I saw the obvious.","tags":["Jenkins"],"title":"Using Same Node in Jenkins Groovy Pipeline"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Marten Newhall.\r\r\r\r\r\r\rPhoto by Marten Newhall.\r\r\r When you have a static website, there are a few things that you usually don\u0026rsquo;t have out-of-the-box. One such thing is search. You can argue that you don\u0026rsquo;t need it, but if you want it and your site isn\u0026rsquo;t that large, I\u0026rsquo;ll describe how I\u0026rsquo;ve set it up without an external service.\nThis post is part 5 of my Hugo Pipeline Series, so I\u0026rsquo;ll use Hugo as the example here, but I\u0026rsquo;ve done a similar setup with this Gatsby plugin as well.\nThe steps I use are the following:\n Create a json file with everything I want in my search index (Hugo) Create a search index from the json file (NodeJS) Download and load the index (Web Browser) Perform search and present results (Web Browser) 1. Create file to index I have a Hugo layout for the indexable content where I output all pages of the types I want. The type article is what all blog posts use and shortcuts-windows7 is a special layout I want to include in search (see it here, if you\u0026rsquo;re curious). My About page is not included, since I figure you can find that anyway if you can find the search feature 🤪\nTitle, relative permalink, tags, the full content as plain text, the summary (excerpt) and the date (formatted and raw), are the fields I picked as searchable + available for search result presentation.\nI also exclude the list page named Articles (that I don\u0026rsquo;t know how to get rid of, please create a PR if you know how and want to help).\nlayouts/search-index/single.html\n\r\r\r{{-$.Scratch.Add\u0026#34;index\u0026#34;slice-}} {{-rangewhere.Site.Pages\u0026#34;Type\u0026#34;\u0026#34;in\u0026#34;(slice\u0026#34;article\u0026#34;\u0026#34;shortcuts-windows7\u0026#34;)-}} {{-ifne.Title\u0026#34;Articles\u0026#34;-}} {{-$.Scratch.Add\u0026#34;index\u0026#34;(dict\u0026#34;title\u0026#34;.Title\u0026#34;ref\u0026#34;.RelPermalink\u0026#34;tags\u0026#34;.Params.tags\u0026#34;content\u0026#34;.Plain\u0026#34;summary\u0026#34;(partial\u0026#34;summary.html\u0026#34;.)\u0026#34;dateformatted\u0026#34;(dateFormat\u0026#34;2, January 2006\u0026#34;.Date)\u0026#34;dateiso\u0026#34;(time.Date))-}} {{-end-}} {{-end-}} {{-$.Scratch.Get\u0026#34;index\u0026#34;|jsonify-}}\r\r This layout needs to be referenced and for that I have search-index.md which is empty, except for the frontmatter.\n--- date: \u0026#34;2017-06-21T06:51:27+02:00\u0026#34; title: \u0026#34;search index\u0026#34; type: \u0026#34;search-index\u0026#34; url: \u0026#34;data-to-index.json\u0026#34; --- 2. Create index Now that we have something to index, it\u0026rsquo;s time to switch to NodeJS land and install Lunr, yarn add lunr. I have a script that reads the file created in the previous step (data-to-index.json) and creates a new file, search-index.json in the output directory (public). This is also the place to configure Lunr with boosting and such. I\u0026rsquo;m not good att tweaking search, so these settings are pretty basic. This was written before I got more heavily into NodeJS development, but it has worked without problems for a few years now.\n\r\r\r\u0026#39;use strict\u0026#39; const lunr = require(\u0026#39;lunr\u0026#39;); const fs = require(\u0026#39;fs\u0026#39;); const path = require(\u0026#39;path\u0026#39;); const outputFilePathParameter = process.argv \u0026amp;\u0026amp; process.argv.slice(2)[0]; const inputFilePath = path.resolve(__dirname, \u0026#39;../public/data-to-index.json\u0026#39;); const outputFilePath = outputFilePathParameter || path.resolve(__dirname, \u0026#39;../public/search-index.json\u0026#39;); console.log(\u0026#39;Reading \u0026#39; + inputFilePath); const documentsToIndex = require(inputFilePath); const store = {}; console.log(\u0026#39;Indexing \u0026#39; + inputFilePath); const searchIndex = lunr(function () { this.ref(\u0026#39;ref\u0026#39;) this.field(\u0026#39;title\u0026#39;, {boost:10}), this.field(\u0026#39;tags\u0026#39;, {boost:5}), this.field(\u0026#39;content\u0026#39;) documentsToIndex.forEach(function (doc) { store[doc.ref] = { \u0026#39;title\u0026#39;: doc.title, \u0026#39;summary\u0026#39;: doc.summary, \u0026#39;dateiso\u0026#39;: doc.dateiso, \u0026#39;dateformatted\u0026#39;: doc.dateformatted }; this.add(doc) }, this) }) console.log(\u0026#39;Saving index at \u0026#39; + outputFilePath); const dataToSave = JSON.stringify({ index: searchIndex, store: store }); fs.unlink(outputFilePath, function(err){ if (err \u0026amp;\u0026amp; err.code !== \u0026#39;ENOENT\u0026#39;) throw err; const options = { flag : \u0026#39;w\u0026#39; }; fs.writeFile(outputFilePath, dataToSave, options, function(err) { if (err) console.error(err); else console.log(\u0026#39;Saved index at \u0026#39; + outputFilePath); }); });\r\r This is run with an npm script after Hugo has produced its output.\n\u0026gt; node build/index-search.js public/search-index.json Reading /Users/henrik/Code/blog-hugo/public/data-to-index.json Indexing /Users/henrik/Code/blog-hugo/public/data-to-index.json Saving index at public/search-index.json Saved index at public/search-index.json ✨ Done in 0.52s. To have the search index available during development, I run the Hugo command twice. This isn\u0026rsquo;t perfect, but since hugo server (like most dev servers) doesn\u0026rsquo;t save the files on disk, this is necessary and not really a problem. The npm script looks like this: hugo \u0026amp;\u0026amp; npm run index \u0026amp;\u0026amp; npm run hugo:watch (see full package.json here).\n3. Loading index Most of my visitors come straight to a post from a Google search, so I\u0026rsquo;m probably the biggest user of the site search myself (maybe the only one 😳). Therefor I don\u0026rsquo;t want the search index to be downloaded before the user has shown an intention to use the search feature. The index is currently a download of 134 kB (compressed), which I think is fine considering that people are watching video on web pages and that the alternative of using an external service has several other drawbacks (complexity, cost, etc). Still, the index size is worth keeping an eye on and this setup requires error handling (if the download fails or the user has started to type before the download is complete).\nThe index are downloaded through a regular fetch call when the search dialog is opened (the open function).\n\r\r\rconst search = { isOpen: false, textInSearchBox: \u0026#39;\u0026#39;, index: null, store: null, indexLoadFailed: false, indexLoading: false, hits: [], open: function () { blog.isModalOpen = true; this.isOpen = true; this.textInSearchBox = \u0026#39;\u0026#39;; this.indexLoadFailed = false; this.downloadIndex(); }, downloadIndex: function () { if (this.index) return; this.indexLoading = true; this.fetchIndex().then(({ index, store }) =\u0026gt; { this.index = window.lunr.Index.load(index); this.store = store; this.indexLoading = false; this.searchBoxChanged(this.textInSearchBox); console.log(\u0026#34;🔍 Search index downloaded\u0026#34;) }); }, fetchIndex: function () { return fetch(\u0026#39;/search-index.json\u0026#39;) .then(this.handleFetchResponse) .catch(this.handleFetchResponse); }, handleFetchResponse: function (response) { this.indexLoadFailed = !response.ok; return response.ok \u0026amp;\u0026amp; response.json ? response.json() : this.index; }, ... }\r\r 4. Searching and presenting results I have touched on this in my previous post about Alpine.js, so go there for more code, but this is simply about calling the search function on the Lunr index. Since everything is in memory, I call the search function on every keypress in the searchbox.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Good luck in implementing your own site search!\n","dateformatted":"23, July 2020","dateiso":"2020-07-23T18:47:00+02:00","ref":"/search-static-website-without-external-service/","summary":"When you have a static website, there are a few things that you usually don’t have out-of-the-box. One such thing is search. You can argue that you don’t need it, but if you want it and your site isn’t that large, I’ll describe how I’ve set it up without an external service.","tags":["Lunr","Hugo","NodeJS","JavaScript"],"title":"Search for Static Website Without External Service"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r I happened to buy a domain name that could be used for short URL\u0026rsquo;s and thought: why not, let\u0026rsquo;s give it a try!\nA search for \u0026ldquo;URL shortener\u0026rdquo; on dev.to, quickly directed me down a rabbit hole, like how to build a URL shortener service at FAANG scale (for billions of users). I don\u0026rsquo;t need that, just something for myself. And thinking one step further, I don\u0026rsquo;t even need a URL shortener. Creating the links is something I can do manually to start with, the core is to have the redirects working in a fairly performant way on the public Internet without breaking the bank. Since my goal was to have personal short links, I don\u0026rsquo;t have to support trillions of links.\nThen I found Gijo Varghese\u0026rsquo;s excellent Netlify URL Shortener. I copied the concept and created a new repo on GitHub with the following content:\n_redirects\r404.html\rindex.html\rI deployed the thing with Netlify and\u0026hellip;done ✅\nUsing a _redirects file with Netlify means that they take care of the hard parts. The only thing I have to do is to add a new line to the _redirects file in my repo. It\u0026rsquo;s also case-sensitive, so I could automate this with an Alfred Workflow that generates a Base 62 hash and updates the _redirects file in the repository, but I\u0026rsquo;ll save that for some future rainy day.\nHere is a link to this post: https://henriks.link/t4Xz\n","dateformatted":"10, July 2020","dateiso":"2020-07-10T14:45:00+02:00","ref":"/simple-short-url-service/","summary":"I happened to buy a domain name that could be used for short URL's and thought – why not, let's give it a try!","tags":["Short URL's"],"title":"Simple Short URL Service"},{"content":"This post is part 4 in the Hugo Pipeline Series, but the benefits and limitations I discuss are not specific to using Alpine.js together with Hugo.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Kaspar Allenbach.\r\r\r\r\r\r\rPhoto by Kaspar Allenbach.\r\r\r For the custom JavaScript code on my Hugo blog (the one you\u0026rsquo;re on right now) I use Alpine.js. I\u0026rsquo;ll discuss the benefits and the limitations in this post.\nWhat is Alpine.js and why? Alpine.js is meant to be used with an existing HTML document (server-side rendered HTML that isn\u0026rsquo;t produced by a JavaScript framework) , just like plain JavaScript or jQuery. This is the way it was done when JavaScript was introduced and how it\u0026rsquo;s done today when using Hugo.\nI use JavaScript for the following features on my blog:\n Opening and closing a hamburger menu Lazy loading of images Lightbox for images and code Providing site search Easter eggs Comments and analytics (but that\u0026rsquo;s not my code) I started out using jQuery when migrating the blog from WordPress to Hugo, which I think was the obvious choice at the time. Later I migrated to plain JavaScript. That was fairly straight forward and the code looked quite similar after the migration, although a bit lengthier. This worked fine and I didn\u0026rsquo;t need a library at all, so why add one again?\nLooking at what the JavaScript I have is doing, we can see where I can benefit from using a library:\n Changing CSS classes on an element, mostly body Adding event listeners to handle interactions Rendering search results In all these areas I benefit from using declarative code, it\u0026rsquo;s just less code and easier to read. This is where Alpine.js comes in. Alpine.js has borrowed a lot from Vue.js when it comes to syntax, but work with an existing DOM. I haven\u0026rsquo;t used Vue.js and that can make you feed a bit excluded when the documentation explains something by saying that \u0026ldquo;it works just like in Vue.js\u0026rdquo;. It is however a small API, so I found it easy to get started with.\nAn example with keyboard navigation This is the relevant (simplified) code I use for showing/hiding outline for the element that has focus, based on whether the user is navigating by mouse or keyboard.\nHTML \u0026lt;body x-data=\u0026#34;window.blog\u0026#34; :class=\u0026#34;{ \u0026#39;keyboard-navigation\u0026#39; : keyboardNavigation }\u0026#34; @mouseup=\u0026#34;keyboardNavigation = false\u0026#34; @keydown.tab=\u0026#34;keyboardNavigation = true\u0026#34;\u0026gt; … \u0026lt;/body\u0026gt; JavaScript window.blog = { keyboardNavigation: false } CSS body.keyboard-navigation a:focus { outline: 2px solid var(--accent-color); } Doing this with imperative code is simply messier, so this is one example where Alpine.js helps.\nAn example with search results Another example is the search results I present. This is a situation where I could just add any of the popular JavaScript frameworks, since this part of the page creates the HTML in JavaScript. This is also a situation where pure JavaScript quickly gets messy, like concatenating strings and setting innerHTML, especially if you need event listeners on those new elements.\n\r\r\r\u0026lt;div class=\u0026#34;search-results-container\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;search-output\u0026#34; x-show=\u0026#34;search.textInSearchBox\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;no-results-message\u0026#34; x-show=\u0026#34;search.store \u0026amp;\u0026amp; search.textInSearchBox \u0026amp;\u0026amp; !search.hits.length\u0026#34;\u0026gt; No matching posts found. You can use wildcards and search only in titles, e.g. \u0026lt;code\u0026gt;title:iot\u0026lt;/code\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;index-loading-message\u0026#34; x-show=\u0026#34;!search.indexLoadFailed \u0026amp;\u0026amp; search.indexLoading \u0026amp;\u0026amp; search.textInSearchBox\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;icon-spinner\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; Loading search index, please wait... \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;index-failed-message\u0026#34; x-show=\u0026#34;search.indexLoadFailed \u0026amp;\u0026amp; search.textInSearchBox\u0026#34;\u0026gt; Search index failed to download 😢 \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;number-of-hits-message\u0026#34; x-text=\u0026#34;search.getHitsText()\u0026#34; x-show=\u0026#34;search.hits.length\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;ol class=\u0026#34;result-list\u0026#34; x-show=\u0026#34;search.hits.length\u0026#34; x-ref=\u0026#34;hits\u0026#34;\u0026gt; \u0026lt;template x-for=\u0026#34;hit in search.hits\u0026#34; :key=\u0026#34;hit.ref\u0026#34;\u0026gt; \u0026lt;li\u0026gt; \u0026lt;h2\u0026gt;\u0026lt;a :href=\u0026#39;hit.ref\u0026#39; x-text=\u0026#34;search.fromStore(hit).title\u0026#34;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; \u0026lt;div class=\u0026#34;entry-meta\u0026#34;\u0026gt; \u0026lt;time class=\u0026#34;published\u0026#34; :datetime=\u0026#34;search.fromStore(hit).dateiso\u0026#34;\u0026gt; \u0026lt;svg class=\u0026#34;icon icon-calendar\u0026#34;\u0026gt;\u0026lt;use xlink:href=\u0026#34;#icon-calendar\u0026#34;\u0026gt;\u0026lt;/use\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;span x-text=\u0026#34;search.fromStore(hit).dateformatted\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;p x-text=\u0026#34;search.fromStore(hit).summary\u0026#34;\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;/ol\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;\r\r search is the object that contains the functions and properties being referenced in this mark-up. It\u0026rsquo;s in a separate JavaScript file not included here, but hopefully you get the point of the declarative approach instead of doing this in imperative JavaScript.\nBenefits Hopefully I\u0026rsquo;ve been able to highlight some of the benefits in the examples above, but to conclude:\n Easy to get started with Same kind of declarative data binding that we love with other JavaScript frameworks Limitations Now to the interesting stuff, things that Alpine.js not so good for – the stuff you generally don\u0026rsquo;t find in documentation or tutorials.\n You cannot have nested components or have them communicate easily. The page (DOM) isn\u0026rsquo;t updated when updates are triggered by non-interactive events. Doesn\u0026rsquo;t work with Turbolinks In the case of my blog, I made the body tag the Alpine component, which works just fine as I\u0026rsquo;m mostly setting different CSS classes on the body tag anyway. For a more complex use, A guide to Alpine.js component communication describes how you can have sibling components talk to each other and have the DOM react to non-interactive events, see answer on GitHub. A non-interactive event is when the user hasn\u0026rsquo;t clicked or typed anything, such as when data is fetched (a promise is resolved) and you set that data to a property.\nThe theme switcher I have reacts to theme (light/dark) changes in the operating system and also when the theme setting in localStorage is changed. The code I have to listen for those events can update a property bound to the Alpine component, but it won\u0026rsquo;t update the DOM. Rather than implementing some involved dispatch mechanism, I prefer to use pure DOM manipulation for these situation, starting with document.getElementById() and setting the element\u0026rsquo;s properties.\nAnother thing to note is that if you\u0026rsquo;re using Turbolinks (which gives navigation without full page reloads), it doesn\u0026rsquo;t seem to work with Alpine.js. It seems to work now 👏\nConclusion Overall, I think the migration from plain JavaScript to Alpine.js was worth it for me. The code is easier to read now and that\u0026rsquo;s what I was aiming for. I just wish I understood the limitations earlier, that would have saved some time.\nThere are also features of Alpine.js that I don\u0026rsquo;t use, namely animations and x-ref when using a .js file. Maybe this is because I came from a world of plain JavaScript and animations in CSS. It seems that the convention when using Alpine.js is to include all JavaScript in \u0026lt;script\u0026gt; tags rather than separate .js files. I didn\u0026rsquo;t go that route and found that document.getElementById() works just as well as passing x-refs around (they don\u0026rsquo;t seem to work in .js files otherwise). Using a \u0026lt;script\u0026gt; tag is probably better as long as the code is as simple as in my keyboard navigation example above, but as it grows, I find it better to use a separate file 🤷‍♂️\nAwesome Alpine is a great collection of resources.\n","dateformatted":"29, June 2020","dateiso":"2020-06-29T06:21:00+02:00","ref":"/alpinejs-benefits-and-limitations/","summary":"For the custom JavaScript code on my Hugo blog I use Alpine.js. I’ll discuss the benefits and the limitations in this post. This post is part 4 in the Hugo Pipeline Series, but the benefits and limitations I discuss are not specific to using Alpine.js together with Hugo.","tags":["Alpine.js","JavaScript","Hugo"],"title":"Alpine.js – Benefits and Limitations"},{"content":"This post is part 3 in the Hugo Pipeline Series.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r In terms of making code changes to my Hugo site, I\u0026rsquo;ll focus on the JavaScript parts, since Hugo templates and CSS isn\u0026rsquo;t much to talk about. I use a few libraries that I\u0026rsquo;ve installed with npm and those need to be processed before they are sent to the browser. The JavaScript code I have written myself, does not have that requirement. In that case it\u0026rsquo;s just a matter of how old browsers I want to support.\nSo, by splitting libraries (installed through npm) from my own code, I\u0026rsquo;m able to rely solely on Hugo\u0026rsquo;s file watcher with live reload. For development (using hugo server), the libraries are built once, and the file with custom code is served to the browser as is. I use Browserify for the libraries and that\u0026rsquo;s good enough for my needs. If you need something more powerful, you might consider Victor Hugo that comes with Webpack preconfigured. I could also have referenced the libraries directly from a public CDN, but that would make it harder to see when there\u0026rsquo;s a new version of a library and more importantly – my site wouldn\u0026rsquo;t work on localhost without an Internet connection.\n\r\r\r{{if.Site.Params.MinifyBundles}} {{$opts:=(dict\u0026#34;minified\u0026#34;true\u0026#34;compact\u0026#34;true\u0026#34;noComments\u0026#34;true)}} {{$main:=resources.Get\u0026#34;main.js\u0026#34;|babel$opts}} {{$libs:=resources.Get\u0026#34;libraries.js\u0026#34;}} {{$bundle:=slice$libs$main|resources.Concat\u0026#34;bundle.js\u0026#34;|resources.Fingerprint\u0026#34;sha512\u0026#34;}} \u0026lt;script src=\u0026#34;{{$bundle.RelPermalink}}\u0026#34; integrity=\u0026#34;{{$bundle.Data.Integrity}}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{else}} {{$libs:=resources.Get\u0026#34;libraries.js\u0026#34;|resources.Fingerprint\u0026#34;sha512\u0026#34;}} \u0026lt;script src=\u0026#34;{{$libs.RelPermalink}}\u0026#34; integrity=\u0026#34;{{$libs.Data.Integrity}}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{$mainDev:=resources.Get\u0026#34;main.js\u0026#34;|resources.Fingerprint\u0026#34;sha512\u0026#34;}} \u0026lt;script src=\u0026#34;{{$mainDev.RelPermalink}}\u0026#34; integrity=\u0026#34;{{$mainDev.Data.Integrity}}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{end}}\r\r For the \u0026ldquo;production build\u0026rdquo;, I just minify the code and concatenate the libraries with my custom code into a single file. I use Babel from within the Hugo template (available since Hugo 0.70) which removes the need for yet another npm script.\nWhen doing code changes I always create a new branch in Git. As I described in the previous post, anything pushed to master is automatically built and deployed (if build is successful) to the live site without any tests. By creating a pull request and using GitHub Actions, I can feel confident that I haven\u0026rsquo;t broken anything when the checks are green.\nThese are the steps: \rfigure svg {\rwidth: 100%;\rheight: auto;\r}\rfigure.use-theme svg path {\rstroke: var(--color)\r}\rfigure.use-theme svg text {\rfill: var(--color)\r}\rfigure.use-theme svg rect {\rfill: transparent\r}\r\r\rimage/svg+xml\r\r\r\r\r@font-face {\rfont-family: \u0026quot;Virgil\u0026quot;;\rsrc: url(\u0026quot;https://excalidraw.com/FG_Virgil.woff2\u0026quot;);\r}\r@font-face {\rfont-family: \u0026quot;Cascadia\u0026quot;;\rsrc: url(\u0026quot;https://excalidraw.com/Cascadia.woff2\u0026quot;);\r}\r\r\r\rCreate feature branch in Git\r\r\rDevelop the new feature\rin the new branch\r\r\rCreate Pull Request from\rfeature branch to master branch\r(through GitHub)\r\r\r\rNetlify builds \u0026amp; deploys a canary\rrelease on a separate URL\r(this is reported back to the\rPR in GitHub\r\r\rWhen the canary release URL\rresponds with a 200 status code,\rCypress tests are run in\rGitHub Actions\r\r\rTest results are shown in the PR in\rGitHub. If green, feature branch\ris merged into master\r\r\rNetlify deploys the latest code\rfrom the master branch to\rthe live site (Prod)\r\r\rGitHub Actions workflow waits 2 min\rand then performs a Lighthouse audit\ragainst the live site\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\r\nWhen I create a pull request, either through GitHub\u0026rsquo;s web interface or preferably by using their CLI (gh pr create), two things happen. (1) Netlify deploys a preview of the site (canary release) and (2) GitHub Actions runs my Cypress tests against that deployed site.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Running the tests against a deployed site (as opposed to a dev server on localhost) transform the tests from function tests to end-to-end tests. Before I had this setup I once broke the comments feature (using Disqus) by fiddling with Content Security Policy headers, something I would have caught today when running the tests against a deployed site.\nSince both building + deploying a site and installing Cypress to run the tests take some time, it\u0026rsquo;s great that it can run in parallel. The yarn install run in GitHub Actions is typically done before the canary release is deployed by Netlify, so I use an action that waits for a 200 response from Netlify and then run the tests. Super happy with this setup!\nmaster-pull-request.yml \r\r\rname:Testson:pull_request:branches:- masterjobs:tests:runs-on:ubuntu-lateststeps:- uses:actions/checkout@v1- name:Installrun:|yarn install- name:Waiting for 200 from the Netlify Previewuses:jakepartusch/wait-for-netlify-action@v1.1id:waitFor200with:site_name:\u0026#34;henriksommerfeld\u0026#34;max_timeout:300- name:Integration testsrun:|CYPRESS_BASE_URL=${{ steps.waitFor200.outputs.url }} npm run cypress:cli - name:Run Lighthouseuses:foo-software/lighthouse-check-action@masterid:lighthouseCheckwith:accessToken:${{ secrets.LIGHTHOUSE_CHECK_GITHUB_ACCESS_TOKEN }}author:${{ github.actor }} branch:${{ github.ref }}urls:${{ steps.waitFor200.outputs.url }}sha:${{ github.sha }}prCommentEnabled:trueslackWebhookUrl:${{ secrets.LIGHTHOUSE_CHECK_WEBHOOK_URL }}\r\r As you can see from my workflow file above, I get a notification in Slack (I have a personal workspace) when the canary release is deployed and when a Lighthouse audit is completed (after the Cypress tests). If I\u0026rsquo;ve made changes to the look and feel (like a CSS change), I naturally take a look at the deployed canary release, but if not, I\u0026rsquo;ll just go ahead and merge (which triggers a deploy to the live site).\nOne thing to say about the Lighthouse audits is that they are of questionable value for the canary releases. The performance score is always lower on the first page load, so you need to load at least two pages to get a fair result. Secondly, the SEO score won\u0026rsquo;t say much since the canary release is purpously blocked from indexing and has a different URL than the canonical URL set for the site. For this reason, I run a Lighthouse audit on the live site as well. In this case I can\u0026rsquo;t wait for a 200 response, so I\u0026rsquo;ll just wait a bit longer than a deploy normally takes and run the audit after that.\nmaster-push.yml \r\r\rname:Testson:push:branches:- masterjobs:tests:runs-on:ubuntu-lateststeps:- name:Wait 2 min for Netlify Deploy to completeuses:jakejarvis/wait-action@masterwith:time:\u0026#39;2m\u0026#39;- name:Hit onceuses:wei/curl@masterwith:args:https://www.henriksommerfeld.se- name:Run Lighthouseuses:foo-software/lighthouse-check-action@masterid:lighthouseCheckwith:accessToken:${{ secrets.LIGHTHOUSE_CHECK_GITHUB_ACCESS_TOKEN }}author:${{ github.actor }} branch:${{ github.ref }}urls:\u0026#39;https://www.henriksommerfeld.se/about,https://www.henriksommerfeld.se\u0026#39;sha:${{ github.sha }}prCommentEnabled:trueslackWebhookUrl:${{ secrets.LIGHTHOUSE_CHECK_WEBHOOK_URL }}\r\r The Slack notifications from a \u0026ldquo;production deploy\u0026rdquo; looks something like this:\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r Next post: Alpine.js – Benefits and Limitations.\n","dateformatted":"22, June 2020","dateiso":"2020-06-22T06:21:00+02:00","ref":"/hugo-pipeline-series-developing-and-deploying/","summary":"In terms of developing my Hugo site, I'll focus on the JavaScript parts, since Hugo templates and CSS isn't much to talk about. I use a few libraries that I've installed with npm and those need to be processed before they are sent to the browser. The JavaScript code I have written myself, does not have that requirement. In that case it's just a matter of how old browsers I want to support.","tags":["Hugo"],"title":"Hugo Pipeline Series – Developing and Deploying"},{"content":"This post is part 2 in the Hugo Pipeline Series.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by RetroSupply.\r\r\r\r\r\r\rPhoto by RetroSupply.\r\r\r In the first post in this series I said that I don\u0026rsquo;t use a proper Content Management System (CMS). To manage my content I use a code editor (VS Code) and Git (GitHub). The advantage of having my content in text files in the same repository as the code is huge. No database to backup or sync between environments.\nSee Scott Hanselman looking at all his blog posts from 2005 and onwards. It\u0026rsquo;s all XML files, which might not be trendy today, but is still human readable and easily convertible to another text format. I use Markdown now, but the important thing is that it\u0026rsquo;s text files that can be converted if I need to. I have already changed the code highlighting once, which was an easy search-and-replace operation. When I did the same in WordPress some years ago, it was…harder.\nDifferent workflows for content and code changes Since correcting a simple spelling mistake using a static site generator requires a new build, it\u0026rsquo;s beneficial to reduce the amount of stuff that needs to happen between a push and a deploy. Therefore I have split content changes and code changes (see next post) into two different workflows.\nReduce the stuff to build I have some npm packages and have split dependencies from devDependencies to reduce the amount of packages that need to be installed for a content change. Installing dependencies with yarn install --production installs 44 MB of node_modules, while yarn install installs 110 MB of node_modules.\nUpdate: After switching to building JavaScript with Hugo (using ESBuild), the production install is 3.7 MB of node_modules and the full install is 96 MB.\nMy netlify.toml file:\n[build.environment] HUGO_VERSION = \u0026#34;0.72.0\u0026#34; YARN_VERSION = \u0026#34;1.22.4\u0026#34; YARN_FLAGS = \u0026#34;--production\u0026#34; NODE_ENV = \u0026#34;production\u0026#34; My content deployment workflow consists of pushing directly to the master branch. That triggers a web hook that does a build and deploy by Netlify. This requires the discipline not to push code changes directly to the master branch, but since I\u0026rsquo;m the only developer, that\u0026rsquo;s a policy easy to enforce.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Images The biggest speed difference, both in building on my own laptop and in deploying, is how images are handled. This is strikingly obvious when I compare to the two Gatsby sites I have. When writing or editing a blog post, I have the Hugo development server running. When I add a new image, Hugo creates the different versions (sizes) of the image that my templates specify – once. Then I commit the Markdown file and the images (original and generated) to Git. That\u0026rsquo;s it, the image processing for that specific image is done, it won\u0026rsquo;t ever have to be processed again unless I want to. No CPU cycles are spent on any server to generate images and that saves time in deployment. It also saves time against the free build minutes quota I have, currently 300 minutes per month on Netlify.\nThe local dev server never have to re-generate images it has already processed, so it starts quicker as well. Just as a test when I created a new shortcode with new image sizes, I ran Hugo with the option to re-generate everything from scratch (Markdown, SCSS and images), hugo --ignoreCache:\n | EN -------------------+------\rPages | 197 Paginator pages | 42 Non-page files | 97 Static files | 27 Processed images | 459 Aliases | 63 Sitemaps | 1 Cleaned | 0 Total in 39741 ms\rAll of the 8 cores on my laptop where working here and it took 40 seconds. This is something I won\u0026rsquo;t most likely ever have to do again, so 40 minutes would have been fine in this extreme situation as well.\nTotal build \u0026amp; deploy time As I mentioned, I use Netlify to both build and deploy. That process takes about 2 minutes, which regardless of everything else, is better than the 25 minutes Wes Bos is experiencing.\nNext post: Developing and Deploying.\n","dateformatted":"14, June 2020","dateiso":"2020-06-14T22:12:00+02:00","ref":"/hugo-pipeline-series-editing-and-deploying/","summary":"This is how I write and deploy content on this blog. I'll describe how I work with images and JavaScript dependencies to keep building and deploying as quick as possible.","tags":["Hugo","VS Code","JavaScript"],"title":"Hugo Pipeline Series – Editing and Deploying"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Quinten de Graaf.\r\r\r\r\r\r\rPhoto by Quinten de Graaf.\r\r\r Who is this for? This is the first part in a series of posts where I\u0026rsquo;ll describe how I created my personal blog with a fair amount of work to achieve simplicity. The intended audience is:\n Myself (as documentation) Someone with their own Hugo blog, looking to improve automation Someone interesting in learning one way of working with a Hugo blog This will be an overview of the different pieces I use, it won\u0026rsquo;t be a detailed description or tutorial, but I\u0026rsquo;ll try to link to libraries and concepts.\nTL;DR Repo is here. README shows how to run it locally A build \u0026amp; deploy log is here Site characteristics The site I\u0026rsquo;ll be describing has the following characteristics:\n Annual running cost of nothing, except for the domain name Static site (built with Hugo) 400+ images \u0026lt; 2 minutes combined build \u0026amp; deploy time Custom domain with HTTPS Lazy loading of images (with blurry preview when JS is available) Search with Lunr Lightbox for images and code Dark/light mode Alpine.js for custom JavaScript code Cypress tests that run automatically on pull requests Lighthouse audits automatically run after deploy All code is in a public repo on GitHub Runs locally without Internet access What it doesn\u0026rsquo;t have:\n Dynamic data loading (except for comments and analytics) SPA characteristics (it has full page reloads) Offline availability (it\u0026rsquo;s not a PWA) A proper CMS Configurable UI, like themes intended for re-use tend to have A fashion blogger (or other non-technical friend) might view this setup as horribly complicated, but that\u0026rsquo;s because the complexity is hidden to the user in a typical blogging platform.\nAdmittedly, I\u0026rsquo;ve spent some time on this over the years, with small adjustment every now and then, to achieve this level of sophistication and simplicity. It might sound arrogant with \u0026ldquo;sophistication\u0026rdquo; and \u0026ldquo;simplicity\u0026rdquo; but storing the content as text files instead of in a database and having a static site that can run from a CDN is simple.\nIt\u0026rsquo;s sophisticated in that the pipeline is automated and has tests in the same way as our professional applications we develop at work. I\u0026rsquo;ll create a new branch, do code or design change and make a pull request (PR) to myself. If the tests that run automatically when a PR is created are green, I merge without hesitation. More details coming in future posts… (here is part 2)\n","dateformatted":"14, June 2020","dateiso":"2020-06-14T22:11:00+02:00","ref":"/hugo-pipeline-series-intro/","summary":"In this post(s) I'll describe how I created my personal blog with a fair amount of work to achieve simplicity.","tags":["Hugo"],"title":"Hugo Pipeline Series – Intro"},{"content":"I\u0026rsquo;m sure there are thousands of posts on this topic already, but since this turned out to be mostly general advice, I might as well publish it to save a few keystrokes if I get a similar request in the future.\nContext I was recently asked for advice by a student struggling with his web development assignment from school. This was an exercise from the level below university – high school, upper secondary school, or whatever it\u0026rsquo;s called in your country.\nI got a zip file with the assignment as a PDF file and the code in its current state where the student was stuck. Unsurprisingly, it wasn\u0026rsquo;t one specific thing that wasn\u0026rsquo;t working with a clear question on how to solve that specific problem. There were errors in the web browser console and long functions with wrong indentation that made it all hard to understand.\nI\u0026rsquo;ve never been able to \u0026ldquo;take a quick look\u0026rdquo; at a problem like this and give valuable feedback. To be able to give some helpful advice, I need to understand the assignment and see how far from a solution the current state is. In this case I ended up making my own implementation and then writing down my advice. We then had a screen sharing session walking through the problem step by step, until only some minor stuff remained to implement.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by NESA by Makers.\r\r\r\r\r\r\rPhoto by NESA by Makers.\r\r\r My general advice There was some specific advice I could give based on the assignment, but the general stuff was as follows.\n1. Variable declarations A variable should be declared, either with var, let or const, but avoid\nvar – see https://hackernoon.com/why-you-shouldnt-use-var-anymore-f109a58b9b70.\n2. Naming Giving functions and variables good names is one of the most difficult and most important things to get understandable code. Generally, I recommend using English names throughout.\nIf it makes it easier for you to reason about the problem domain in your own language, then you might be better off using your native language, but if you do – use the whole alphabet (like umlauts) and not some crippled version of your language. Most importantly: be consistent.\nHere is a clear walkthrough of conventions you benefit from following: https://www.robinwieruch.de/javascript-naming-conventions\n3. Indenting Indenting is, just like naming, something that helps or hinders the brain when reading code. It might feel trivial, but it does make a difference, maybe to your grade on this exercise as well.\nCode doesn\u0026rsquo;t have to be pretty before it works, but wrong indentation can make you put something inside instead of outside a block ({}) – and just like that, 10 minutes is wasted on troubleshooting.\n4. Short functions Make sure your functions fit on the screen without scrolling. Especially if you have many levels of indentation, it\u0026rsquo;s a sign you might be able to extract some of it into a new function, like the contents of an if statement or a for loop.\nWhat I\u0026rsquo;m reaching for here isn\u0026rsquo;t code aesthetics that will give bonus points. I assume you\u0026rsquo;re only interested in getting it to work at this point, and that\u0026rsquo;s when these general things help me proceed in the right direction.\n5. Limit state manipulation Use as few global variables (declared outside of any function) as possible and keep them in one place. Try to set them in as few places as possible.\n6. Baby steps Keep the web browser developer tools (F12) open to spot errors in the console. If you have an error, stop what you\u0026rsquo;re doing and fix the error. Save and test often.\nMake sure you address one issue at a time. Even if you\u0026rsquo;re not \u0026ldquo;done\u0026rdquo; with the feature you\u0026rsquo;re working on, you can watch your progress by adding console.log(variable) statements, inspecting the HTML in the browser or setting breakpoints and stepping through the code (debugging).\n7. Backup when something works Ideally you should use a version control system (like Git), but if you haven\u0026rsquo;t been taught how to use that, you can always copy the code folder and give it a sensible name, whenever you\u0026rsquo;ve managed something to run like it should. It\u0026rsquo;s just as easy to break something that worked before, as it\u0026rsquo;s frustrating when it happens.\n8. Google (verb) If you know what you want to achieve, but not how to type it out – google it, we all do.\nConclusion Having gone through the assignment, solving it together with the student in a two-hour screen sharing session, I conclude that methodical problem-solving skills is the most important. Of course you have to know the basics of the programming language you\u0026rsquo;re using and have an understanding of the assignment to be solved, but there are no shortcuts.\nThe other obvious insight is that apart from my first JavaScript specific advice on variables, this applies to most of coding – not only most programming languages, but most of programming you will ever do in your career.\n","dateformatted":"30, May 2020","dateiso":"2020-05-30T10:21:00+02:00","ref":"/javascript-recommendations-to-a-struggling-student/","summary":"I recently helpt a student struggling with a web development assignment. Not surprisingly, it wasn't one specific thing that wasn't working with a clear question on how to solve that specific problem. There were errors in the web browser console, long functions with wrong indentation that made it all hard to understand.","tags":["JavaScript","Code Quality"],"title":"8 JavaScript Recommendations to a Struggling Student"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Dimitar Yanchev (my cropping).\r\r\r\r\r\r\rPhoto by Dimitar Yanchev (my cropping).\r\r\r Approaching 40 years old and two years since I changed direction as a software developer, I conclude that the grass really is greener on the other side. Perhaps I should have jumped earlier.\nWhere I\u0026rsquo;m coming from Most of my career I\u0026rsquo;ve been a consultant working with .Net development and specialised in SharePoint (described in Getting a Divorce From SharePoint). When I read The Phoenix Project I strongly identified with the pre-transformation description and the different characters described in that book. At one time, me and a colleague had a great laugh comparing our work situation to the film The Martian (where the main character is stuck alone on Mars, click the link and see the trailer to get a feeling for what I mean). Our job felt just as complex and challenging – and we were only maintaining a bloody website! To be fair, it was a website selling products for a couple of hundred million EUR annually, but still.\nNot all problems should be solved, some should be avoided. When things are complicated due to accidental technical debt rather than complex due to supporting a complex world, walk away.\nAnother aspect of your career is how much you learn over time. Do I have x years of experience or 1 year of experience x times?\nSample tech:\n Visual Studio [year] (Not responding)™ .Net Framework 4.x Team Foundation Server (using TFVC) SharePoint Server MS SQL Server Windows Server Virtual machines weighing 100+ GB Lenovo P50 README for monthly \u0026ldquo;deploy weekend\u0026rdquo; Where I\u0026rsquo;m now Now I\u0026rsquo;m in Ops, part of a team named Tooling that provides the infrastructure and tools for other development teams to run their applications on. I\u0026rsquo;m the least experienced in the team when it comes to infrastructure and the tools we use, so I learn a ton of new stuff all the time. Having taken a web interface for granted for almost all of my career, the primary user interface our team is building right now is a CLI.\nAs a developer in our organisation, you create a new service (API, web app, etc.) by using our CLI. It will ask you a few questions: name and description for the service, what team owns it, select the upstream services it depends on etc. That takes care of all the boring stuff you need, but don\u0026rsquo;t want to deal with as an application developer, like logging, monitoring, TLS certificates etc.\nNaturally, the idea of infrastructure as code isn\u0026rsquo;t something you have to sell to someone like me with a coding background, but seeing it in action is still fantastic. Even if this isn\u0026rsquo;t feasible for a small company, we\u0026rsquo;re certainly not as big as Google, Amazon, Microsoft or Facebook.\nI find it delightful to primarily work with open source products. We buy commercial software when we find that\u0026rsquo;s the best option, but we don\u0026rsquo;t buy a product that claims it do everything. There is a big difference in choosing the pieces that best fits the puzzle you\u0026rsquo;re trying to assemble, vs buying a complete puzzle that you have to retrofit your existing pieces into.\nSample tech:\n Visual Studio Code (still avoid Vim) JavaScript/TypeScript Go Node JS with koa or Apollo Server Docker, Terraform, Nomad, Consul, Jenkins, Neo4j, Grafana, Graylog, Prometheus\u0026hellip; Mob station with large TV and Ubuntu Linux NUC + personal MacBook Pro Next actions I will keep evaluating my learning and how fun I think my job is. In hindsight I should probably have made a change earlier, but I wasn\u0026rsquo;t unhappy before either and I guess that\u0026rsquo;s the tricky part when it isn\u0026rsquo;t as clear as black and white.\nI hope I have contributed to better ways of working earlier in my career as well, but some things are hard to change within a specific role or organisation. Some people prefer stability and familiarity, but \u0026ldquo;git hooks\u0026rdquo; are better than \u0026ldquo;deployment weekends\u0026rdquo;!\nRead (or listen to the audio books) The Phoenix Project and The Unicorn Project and listen to Rich Hickey\u0026rsquo;s talk Simple Made Easy if you haven\u0026rsquo;t, that\u0026rsquo;s my next actions for you 😉\n","dateformatted":"3, May 2020","dateiso":"2020-05-03T10:02:21+02:00","ref":"/the-grass-is-greener-on-the-other-side/","summary":"Approaching 40 years old and two years since I changed direction as a software developer, I conclude that the grass really is greener on the other side. Perhaps I should have jumped earlier. Git hooks are better than deployment weekends!","tags":["Career"],"title":"The Grass is Greener on the Other Side"},{"content":"I have an Asus RT-AC68U router at home. I\u0026rsquo;ve previously used the build-in update checker together with a notification script that ran on the router itself. Recently I noticed that I hadn\u0026rsquo;t got any update notifications in a long time, one of the downsides of silent failures.\nWhen I ran a manual check using the router\u0026rsquo;s web interface, it just said: \u0026ldquo;Temporarily unable to get the latest firmware information. Please try again later.\u0026quot; It doesn\u0026rsquo;t seem to be that temporary though.\nTL;DR The code is in this GitHub repo and the scheduling piece with cron is described a the end of this post.\nBuilding my own update checker Since the project website of Asuswrt-Merlin presents the latest version in an easily parsable way, I decided to write my own checker using screen scraping in NodeJS.\nVersion checker To find the latest version, I just looked at the website, inspected the HTML, installed the packages request-promise and cheerio, and finally extracted the version number of interest.\n\r\r\rimport rp from \u0026#39;request-promise\u0026#39;; import $ from \u0026#39;cheerio\u0026#39;; export async function getLatestStableVersion() { try { const html = await rp(\u0026#39;https://www.asuswrt-merlin.net/\u0026#39;); const text = $(\u0026#39;#block-currentrelease\u0026#39;, html).text(); const rows = text.split(\u0026#39;\\n\u0026#39;); const stableOthers = rows.find(r =\u0026gt; r.startsWith(\u0026#39;Others:\u0026#39;)); const stableVersion = stableOthers.split(\u0026#39;Others:\u0026#39;)[1].trim(); return stableVersion; } catch (error) { Promise.reject(error); } }\r\r Saving last checked version In order to know if there is a new version since my last check, I of course need to keep track of what the version was the last time I checked. I did this with a simple text file on disk.\n\r\r\rimport { existsSync, readFileSync, writeFileSync } from \u0026#39;fs\u0026#39;; import * as path from \u0026#39;path\u0026#39;; const savedVersionFilePath = path.resolve(path.resolve(\u0026#39;\u0026#39;), \u0026#39;./last-checked-version.txt\u0026#39;); export function getLastCheckedVersion() { if (!existsSync(savedVersionFilePath)) { return \u0026#34;0.0\u0026#34; } const lastCheckedVersion = readFileSync(savedVersionFilePath, \u0026#39;utf8\u0026#39;); return lastCheckedVersion; } export function saveLastCheckedVersion(version) { writeFileSync(savedVersionFilePath, version); }\r\r Notifier I already had a working notification script using the service Pushover that I ported from Bash to NodeJS.\n\r\r\rimport dotenv from \u0026#39;dotenv\u0026#39;; import rp from \u0026#39;request-promise\u0026#39;; export function sendPushoverNotification(message) { dotenv.config(); var options = { method: \u0026#39;POST\u0026#39;, uri: \u0026#39;https://api.pushover.net/1/messages.json\u0026#39;, body: { token: `${process.env.PUSHOVER_TOKEN}`, user: `${process.env.PUSHOVER_USER}`, message: message }, json: true }; rp(options) .then(function (parsedBody) { console.log(`Pushover notification sent: ${message}`); }) .catch(function (err) { console.error(err); }); }\r\r Gluing it together By sending a notification both when there is no update and when an error occurs, I won\u0026rsquo;t have any silent failures unless I made a mistake here somewhere.\n\r\r\rimport { getLatestStableVersion } from \u0026#39;./latest-version-checker.js\u0026#39;; import { sendPushoverNotification } from \u0026#39;./notify.js\u0026#39;; import { getLastCheckedVersion, saveLastCheckedVersion } from \u0026#39;./localFile.js\u0026#39;; async function main() { try { const lastCheckedVersion = getLastCheckedVersion(); console.log(\u0026#34;main -\u0026gt; lastCheckedVersion\u0026#34;, lastCheckedVersion) const latestVersion = await getLatestStableVersion(); if (latestVersion !== lastCheckedVersion) { const message = `🔔 New firmware version ${latestVersion}is now available at https://www.asuswrt-merlin.net/`; sendPushoverNotification(message); saveLastCheckedVersion(latestVersion); } else { const message = `🤷‍♂️ No firmware released. ${latestVersion}is the latest.`; sendPushoverNotification(message); } } catch (error) { console.log(\u0026#34;main -\u0026gt; error\u0026#34;, error) const message = `⚠️ Router firmware update check failed`; sendPushoverNotification(message); } } main();\r\r Scheduling the update checker I\u0026rsquo;m running this on a RaspberryPi and it\u0026rsquo;s scheduled to run once a week, 18:10 on Wednesdays. I found https://crontab.guru to be helpful for not mixing up the time settings.\ncrontab -e\n10 18 * * 3 /home/pi/router-update-check.sh \u0026gt;\u0026gt; /home/pi/router-update-check.log\rThe trickiest thing for me as a terrible Linux admin, was to get the cron scheduling working. Adding the output of echo $PATH at the top of the script did the trick. Logging the output (to router-update-check.log in this case), also helped.\nrouter-update-check.sh script contains the following:\n\r\r\r#!/bin/bash PATH=/home/pi/.nvm/versions/node/v13.12.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games:/snap/bin cd /home/pi/Code/asuswrt-merlin-update-check/ node ./main.js\r\r The result of running the script once every minute (while troubleshooting) showed up in my phone like this. I now have an update checker that I can only blame myself if it doesn\u0026rsquo;t work. Great success!\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r\r\r\r ","dateformatted":"8, April 2020","dateiso":"2020-04-08T14:02:21+02:00","ref":"/asuswrt-merlin-firmware-update-checker/","summary":"I have an Asus RT-AC68U router at home. I’ve previously used the build-in update checker together with a notification script that ran on the router itself. Recently I noticed that I hadn’t got any update notifications in a long time, one of the downsides of silent failures. This is my custom code that looks for updates.","tags":["Networking","Scripting","NodeJS"],"title":"Asuswrt-Merlin Firmware Update Checker"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Ante Hamersmit . Not exactly the Notepad++ logo, but close.\r\r\r\r\r\r\rPhoto by Ante Hamersmit . Not exactly the Notepad++ logo, but close.\r\r\r Having switched from Windows to Mac quite recently, there is one application in particular that I\u0026rsquo;m missing – Notepad++.\nAfter trying Notes and Stickes, Brackets and a bunch of others, I had to rethink what I was actually using Notepad++ for and which characteristics I was appreciating the most.\nWhat I was looking for Paste text unformatted Plain text by default/only Saves automatically Free of charge I concluded that I used Notepad++ for short snippets of text where the low barrier to entry was key. Quickly opening the app and pasting something; or writing a text for a web form that I suspect will just reload and loose my text.\nSurprisingly many text editing tools don\u0026rsquo;t paste unformatted text by default. The common macOS key combination to paste unformatted seems to be ⌥ ⇧ ⌘ V (try doing that with one hand!). Both Stickes and Notes also change single quote (') to some other character ( ‘ ), which messes up my Cypher queries.\nI already use a heavy-weight app/service for my notes (OneNote) an several windows of VS Code for coding, so I wanted something else for these kind of quick notes.\nWhat I was NOT looking for An editor that greets me with a login form Todos, reminders or other feature bloat 👑 Best so far Standard Notes is my choice for now, installable by brew cask install standard-notes.\nAny suggestions that you like is welcome, please comment.\n","dateformatted":"23, March 2020","dateiso":"2020-03-23T06:22:21+01:00","ref":"/macos-alternative-to-notepad-plus-plus/","summary":"Having switched from Windows to Mac quite recently, there is one application in particular that I’m missing – Notepad++. After trying Notes and Stickes, Brackets and a bunch of others, I had to rethink what I was actually using Notepad++ for and which characteristics I was appreciating the most.","tags":["macOS","Text editor","Notepad++"],"title":"MacOS Alternative to Notepad++"},{"content":"My team recently had a discussion trying to agree on a few rules of thumb for our ambition level regarding code quality and automated tests for our legacy systems. These systems are business-critical, but planned to be replaced in a Big Bang fashion.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Alex Motoc\r\r\r\r\r\r\rPhoto by Alex Motoc\r\r\r One of my colleagues challenged our usual code of conduct of continuous improvement of code quality in regards to these systems. For a code base that has a set timeline for decommissioning, I agreed that we shouldn\u0026rsquo;t strive to improve the code quality. This is especially true when we\u0026rsquo;re talking about a code base that\u0026rsquo;s fragile and where any change is more or less risky.\nOur mission is first and foremost to keep these systems running, second to fix high priority bugs and where possible, make highly requested minor improvements.\nCode quality The code of conduct we came up with for these systems, is something along these lines:\n Strive for a code quality that won\u0026rsquo;t bite us later\n Meaning, if we believe we will have to come back to a particular piece of code and make another change later – apply good practices so that we are nice to our future selves, but don\u0026rsquo;t do any refactoring of surrounding code. If adding another if statement solves the problem – do it and move on.\nAutomated tests For automated tests, we said the following:\n Write a test when we\u0026rsquo;re fixing a bug, if it\u0026rsquo;s reasonable\n Creating an automated test or two as part of fixing a bug is great to avoid regressions later. But we also realise that for some systems it isn\u0026rsquo;t worth the effort. This is when there are no existing tests, meaning no testing infrastructure in place. Some of our old systems have a lot of logic in stored procedures in the database, and to add tests for that just doesn\u0026rsquo;t make sense in our situation.\n Write tests where it\u0026rsquo;s making development easier\n The most obvious case for writing tests are when it helps development directly. This could be a validation function or some other non-trivial pure function where unit tests make the function easier to write. So even if the tests are never run again, it would still be worth the effort.\n Write tests that have as good pay off as possible\n An example of this is UI tests that can catch many possible errors in relation to the amount of test code. This is of course given that we already have a stable set of tests, we don\u0026rsquo;t want to spend a lot of time troubleshooting flaky UI tests.\nConclusion 🤷‍♂️ Is this just common sense or too vague to be useful? Maybe it\u0026rsquo;s vague, but given the diversity of the systems we\u0026rsquo;re talking about here, we couldn\u0026rsquo;t find a way to make it more concrete without making it system specific. Doesn\u0026rsquo;t this apply to all systems? Well, sort of. I think the only relevant difference is the amount of time we\u0026rsquo;re willing to spend on improving existing code – and that amount of time is more of a gut feeling than a discrete number. But in any case, I think it\u0026rsquo;s good we had the discussion and hopefully we can waste less time trying to improve code that will die fairly soon anyway.\n","dateformatted":"9, March 2020","dateiso":"2020-03-09T14:22:21+01:00","ref":"/code-quality-and-automated-tests-for-legacy-systems/","summary":"My team recently had a discussion trying to agree on a few rules of thumb for our ambition level regarding code quality and automated tests for our legacy systems. These systems are business-critical, but planned to be replaced in a Big Bang fashion.","tags":["Testing","Code Quality"],"title":"Code Quality and Automated Tests for Legacy Systems"},{"content":"I recently bought a new machine, a MacBook Pro 16\u0026quot;, and this is my initial configuration. I\u0026rsquo;m documenting this for my future self so I have something to cherry-pick from if I will ever configure a new Mac from scratch. Some of these things might not be relevant in future versions of macOS or for future versions of myself, so I don\u0026rsquo;t see a point in automating it.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rMy new MacBook Pro 16\" with the Roxxlyn slate skin made from real stone.\r\r\r\r\r\r\rMy new MacBook Pro 16\" with the Roxxlyn slate skin made from real stone.\r\r\r Changing machine name By default my new machine had a very long name, which mostly becomes an issue with my bluetooth headset that speaks out the name of the devices it\u0026rsquo;s connected to. This setting is under Sharing. I called this machine BigMac.\nTrackpad settings I found two settings that improve the great trackpad on MacBooks.\nEnabling tap to click for trackpad Enabling clicking by tapping on the trackpad rather than having to press it with force is an obvious thing I definitely want to change. Easy to find in System Preferences -\u0026gt; Trackpad -\u0026gt; Point \u0026amp; Click -\u0026gt; Tap to click.\nEnabling three finger move of windows The other trackpad change I make is to enable moving windows with a three finger gesture. I don\u0026rsquo;t know why this isn\u0026rsquo;t enabled by default and why it\u0026rsquo;s buried so deep in the accessibility settings – very inaccessible.\n System Preferences -\u0026gt; Accessibility -\u0026gt; Pointer Control -\u0026gt; Mouse \u0026amp; Trackpad -\u0026gt; Trackpad options\u0026hellip; -\u0026gt; Enable dragging -\u0026gt; three finger drag \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Installing applications I prefer to install as much as possible through Homebrew. I copied my old Brewfile to my home directory (~/Brewfile), made some adjustment and ran brew bundle.\nI try to keep a fairly updated version of my Brewfile in this gist, but this is the one I used now:\ntap \u0026quot;github/gh\u0026quot;\rtap \u0026quot;homebrew/bundle\u0026quot;\rtap \u0026quot;homebrew/cask\u0026quot;\rtap \u0026quot;homebrew/cask-drivers\u0026quot;\rtap \u0026quot;homebrew/cask-versions\u0026quot;\rtap \u0026quot;homebrew/core\u0026quot;\rbrew \u0026quot;bash\u0026quot;\rbrew \u0026quot;bat\u0026quot;\rbrew \u0026quot;cask\u0026quot;\rbrew \u0026quot;cowsay\u0026quot;\rbrew \u0026quot;exa\u0026quot;\rbrew \u0026quot;ffmpeg\u0026quot;\rbrew \u0026quot;git\u0026quot;\rbrew \u0026quot;gource\u0026quot;\rbrew \u0026quot;htop\u0026quot;\rbrew \u0026quot;hugo\u0026quot;\rbrew \u0026quot;node\u0026quot;\rbrew \u0026quot;nvm\u0026quot;\rbrew \u0026quot;tldr\u0026quot;\rbrew \u0026quot;trash\u0026quot;\rbrew \u0026quot;vcprompt\u0026quot;\rbrew \u0026quot;wget\u0026quot;\rbrew \u0026quot;yarn\u0026quot;\rbrew \u0026quot;youtube-dl\u0026quot;\rbrew \u0026quot;github/gh/gh\u0026quot;\rcask \u0026quot;1password\u0026quot;\rcask \u0026quot;1password-cli\u0026quot;\rcask \u0026quot;adobe-creative-cloud\u0026quot;\rcask \u0026quot;alfred\u0026quot;\rcask \u0026quot;baretorrent\u0026quot;\rcask \u0026quot;brackets\u0026quot;\rcask \u0026quot;firefox\u0026quot;\rcask \u0026quot;fork\u0026quot;\rcask \u0026quot;inkscape\u0026quot;\rcask \u0026quot;iterm2\u0026quot;\rcask \u0026quot;kap\u0026quot;\rcask \u0026quot;keka\u0026quot;\rcask \u0026quot;logitech-options\u0026quot;\rcask \u0026quot;menumeters\u0026quot;\rcask \u0026quot;microsoft-edge\u0026quot;\rcask \u0026quot;mullvadvpn\u0026quot;\rcask \u0026quot;onedrive\u0026quot;\rcask \u0026quot;overkill\u0026quot;\rcask \u0026quot;rectangle\u0026quot;\rcask \u0026quot;sensiblesidebuttons\u0026quot;\rcask \u0026quot;soundflower\u0026quot;\rcask \u0026quot;soundflowerbed\u0026quot;\rcask \u0026quot;spotify\u0026quot;\rcask \u0026quot;teamviewer\u0026quot;\rcask \u0026quot;visual-studio-code\u0026quot;\rcask \u0026quot;vlc\u0026quot;\rTouch Bar The Touch Bar is probably my least favourite part of the MacBook Pro, maybe after the very low-quality web cam. I find most of the default configuration to aim for aiding non-tech savvy users with simple tasks. In most web browsers, the Touch Bar shows a big button that sets focus on the address bar.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r To me this is a clear indication that it\u0026rsquo;s made for those who don\u0026rsquo;t know that ⌘ + L does the same. It is possible to have the Touch Bar always show the Fn-keys, but for now I\u0026rsquo;ll give it the benefit of the doubt.\nReplacing Siri with Screenshot Since I don\u0026rsquo;t use Siri, I replaced that button with the one for taking a screen shot. This is done by opening an app that supports the Touch Bar, such as Finder, and then in the menu bar go to View -\u0026gt; Customise Touch Bar. Tapping the part you want to customise on the Touch Bar itself, opens those options.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Once I did my first change, this is easy in the way Apple likes to think of their products. My new controls: \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\nUsing function keys in Visual Studio Code I find the function keys much more useful than anything else in VS Code, so I set an app specific rule for using the function keys, as shown below.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r And\u0026hellip;look at this! We\u0026rsquo;ve gone full circle with the Touch Bar in VS Code: \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\nGeneral desktop changes There are a bunch of smaller changes I\u0026rsquo;ve done and I don\u0026rsquo;t remember all of them, but here are a few.\n Show path and status bar in Finder Remove the Stocks widget from Notification Centre. I find it astounding that an app for stock exchange rates is pre-installed and impossible to remove. Sorting in Finder Default setting for Finder is apparently to show files in no particular order. To fix this to sort by name, that setting can be changed in the dialogue opened by ⌘J. This then only applies to the current folder, so to make it the default, there is a special button for that. \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\nArrange windows For a MacBook with a small screen, this might not be that useful, but with a larger display I find it really helpful. It\u0026rsquo;s basically to enable the possibilities that has been in Windows for many years, like putting two windows next to each other. There are a number of applications that does this. I use Rectangle because it\u0026rsquo;s free and open source. Though it is possible to align windows next to each other out-of-the-box as well, that involves precision clicking on the green maximise button, so it doesn\u0026rsquo;t count.\nRemoving irrelevant search results I use Alfred for searching apps and files, but to remove irrelevant files from the results, you have to go into the Spotlight settings, under the Privacy tab. I have these paths removed:\n ~/Code (which is where I clone all repos and where node_modules end up) ~/go (where Go stuff ends up) /System/Volumes/Data (nothing I\u0026rsquo;m looking for is here) \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Keyboard navigation To be able to use the keyboard in dialogue windows and such, you need to enable a setting because\u0026hellip;I wish I knew.\nSystem Preferences -\u0026gt; Keyboard -\u0026gt; Shortcuts -\u0026gt; Use keyboard navigation to move focus between controls.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Allow keyboard focus on links in Firefox Using Firefox, there is an extra setting you have to do to be able to use keyboard navigation (moving focus to links).\nIn addition to the All controls setting in macOS described above, you also have to type about:config in Firefox and create a setting with the key accessibility.tabfocus and set it to true, see Stack Overflow question.\nAdding key bindings for external keyboard I have the full size keyboard with numerical pad from Apple. What I found was that the keys that are called Home and End on other keyboards (not from Apple), don\u0026rsquo;t do what I expect in a text file (moving the cursor to beginning or end of line).\nTo fix this I created the file ~/Library/KeyBindings/DefaultKeyBinding.dict with the following content:\n{\r\u0026quot;\\UF729\u0026quot; = moveToBeginningOfLine:;\r\u0026quot;\\UF72B\u0026quot; = moveToEndOfLine:;\r\u0026quot;$\\UF729\u0026quot; = moveToBeginningOfLineAndModifySelection:;\r\u0026quot;$\\UF72B\u0026quot; = moveToEndOfLineAndModifySelection:;\r\u0026quot;^@\\UF701\u0026quot; = \u0026quot;noop:\u0026quot;;\r\u0026quot;^@\\UF702\u0026quot; = \u0026quot;noop:\u0026quot;;\r\u0026quot;^@\\UF703\u0026quot; = \u0026quot;noop:\u0026quot;;\r}\rEach app needs a restart to make it take effect, see Stack Exchange. The \u0026ldquo;noop\u0026rdquo; settings prevents the system from beeping when pressing ⌘⌃↓.\nToggling integrated terminal focus in Visual Studio Code A thing I have found useful is to be able to toggle focus between the editor and the integrated terminal in VS Code with the keyboard. There is currently no built-in shortcut for this, so I have added one. This is for my keyboard (a Swedish one), but I think that\u0026rsquo;s okay since most other shortcut descriptions on the web assumes an English keyboard. Showing and hiding the terminal is ⌘J, btw.\nKeyboard shortcuts are accessed from Code -\u0026gt; Preferences -\u0026gt; Keyboard Shortcuts [⌘K ⌘S]. The commands are called Terminal: Focus Terminal and View: Focus Active Editor Group. I use the lovely shortcut ⌘Ö for both of them (toggling) and the conditions are !terminalFocus and terminalFocus respectively.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r The resulting configuration in keybindings.json:\n{ \u0026#34;key\u0026#34;: \u0026#34;cmd+[Semicolon]\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;workbench.action.terminal.focus\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;!terminalFocus\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;cmd+[Semicolon]\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;workbench.action.focusActiveEditorGroup\u0026#34;, \u0026#34;when\u0026#34;: \u0026#34;terminalFocus\u0026#34; }, Controlling sound volume on external display I have an external Dell display that connects through HDMI. At first, all sound output through the display was at max volume. To adjust the volume, I had to dive into the fiddly settings on the display itself. By installing Soundflower and SoundflowerBed, I can now control the volume with the built-in controls of macOS.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rSelecting my Dell display as sound output and being able to control volume with built-in macOS controls.\r\r\r\r\r\r\rSelecting my Dell display as sound output and being able to control volume with built-in macOS controls.\r\r\r That\u0026rsquo;s it, at least for now!\n","dateformatted":"26, February 2020","dateiso":"2020-02-26T10:22:21+01:00","ref":"/settings-for-new-macbook-pro/","summary":"I recently bought a new machine, a MacBook Pro 16\", and this is my initial configuration. I'm documenting this for my future self so I have something to cherry-pick from if I will ever configure a new Mac from scratch. Some of these things might not be relevant in future versions of macOS or for future versions of myself, so I don't see a point in automating it.","tags":["macOS","Hardware"],"title":"Settings for new MacBook Pro"},{"content":"Approaching the end of my parental leave, I\u0026rsquo;ll take a moment reflecting on how I tackled the occasional boredom of a longer absence from work.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Picsea\r\r\r\r\r\r\rPhoto by Picsea\r\r\r Background I live in Sweden where we have good possibilities for taking long parental leaves. In my case it has been 7 months – 8 if I count the summer vacation. While I\u0026rsquo;m absolutely thankful for the possibility to stay home with my daughter and getting to know her, I\u0026rsquo;ll never get a second chance etc etc 🙄, it\u0026rsquo;s still boring as hell from time to time. Having an ocean of time, but all the time being consumed by baby care was a new and frustrating experience for me.\nWhat saved me from becoming chairman of the bored was the small technology projects I could work on during the short periods of focus time (like when the child was asleep). Problem solving during stroller walks was also a great way to make progress even in pure AFK days (Away From Keyboard).\nWhat worked for me Looking back at these months, I\u0026rsquo;m quite satisfied with my accomplishments. In short:\n Rebuilt personal website for my wife from WordPress to Gatsby, saving us the annual fee for WordPress.com and ended up with a more personal site. Added dark mode to my own blog Added lazy loading of images on my blog Built a family recipe collection site with a CMS Kept the daughter alive (happy and well) 👶🏻 Small projects I think a key success factor was to start small, keep the scope smaller than what you think you\u0026rsquo;ll manage to complete (because you\u0026rsquo;ll be wrong, you\u0026rsquo;ll always be able to do less than what you initially think).\nMVP Define a Minimal Viable Product (MVP) and keep track of nice to have stuff separately. When you have other stakeholders, like with my wife\u0026rsquo;s personal website, it can be a good idea to explain the concept of an MVP – what\u0026rsquo;s simplest version I can build that you can use and we still can kill the old site?\nOnce the new site was live, I could continue to do small improvements without time pressure (when the WordPress.com subscription was already cancelled). These were things that was easy to do without longer periods of focus.\nHave your laptop with you Since you\u0026rsquo;ll need a bag with diapers, change of clothes, food, water and so on, you might as well throw in your laptop when leaving home. That way you can make use of the random chunks of time that appears when the kid falls asleep.\nEnsure you can take notes on the go When you get a great idea during a walk, it\u0026rsquo;s nice to be able to easily catch that so that you can process it later. I carry the David Allen Notetaker wallet (not sold any more) since many years, but I guess most people take notes on their phone.\nFinal words I\u0026rsquo;m not saying that everyone should do as much work related stuff as possible during their parental leave. For you the best decision might be to disconnect from work as much as possible, but for me this was a way of staying sane. Thanks for reading!\n","dateformatted":"12, February 2020","dateiso":"2020-02-12T04:42:21+01:00","ref":"/how-i-tackled-parental-leave-boredom-with-code/","summary":"Approaching the end of my parental leave, I'll take a moment reflecting on how I tackled the occasional boredom of a longer absence from work.","tags":[],"title":"How I Tackled Parental Leave Boredom With Code"},{"content":"When writing another post, I realised that I hadn\u0026rsquo;t documented/described my image lazy loading implementation in Hugo anywhere, so here it comes.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Drew Coffman\r\r\r\r\r\r\rPhoto by Drew Coffman\r\r\r The first thing we need are responsive images, not the thing you get when setting the width to 100% in CSS, but different versions of the same image in different resolutions so that the web browser can pick the best one (using srcset).\nChanging folder structure The structure I had before implementing this was all the Markdown files in the same folder with the images in a common static/images folder. To use image processing in Hugo we need to use Page Resources, meaning we have to create a folder for each post where we put both the Markdown file and the images related to that post.\nBefore:\ncontent\r- articles\r- 2019-11-15-dark-mode.md\r- 2019-11-17-netlify-cms.md\rAfter:\ncontent\r- articles\r- 2019-11-15-dark-mode\r- index.md\r- jay-wennington-loAgTdeDcIU-unsplash.jpg\r- 2019-11-17-netlify-cms\r- hero-image.png\r- index.md\r- insert-image.png\rWriting a shortcode To be able to reference the images in the Markdown files, we need a shortcode. This is the trickiest part to explain since the code is a bit lengthy. This is because I have the possibility to pass parameters for whether or not to have a border, a custom width, a lightbox and caption. It might also be because my Go template skills are questionable.\nI use the shortcode in my index.md file to reference the image like this: \r\r\r{{\u0026lt;post-imageimage=\u0026#34;hero-image.png\u0026#34;/\u0026gt;}}\r\r\nWhat happens in the shortcode implementation below is that I create a blurred version of the image resized to 48px wide (keeping the aspect ratio). This is the image you will see before a better version is fetched. This small image is embedded into the HTML as a Base64 encoded string, so that there is no extra request required to get the initial image.\nA few different versions are then generated, but only if the original image is larger than the version I\u0026rsquo;m trying to generate. I don\u0026rsquo;t want to upscale an image (it will be blurry). The variable $src_set is appended with each version and will contain all versions (sizes) when the HTML part begins.\nShortcode post-image.html: \r\r\r{{$image:=(.Page.Resources.GetMatch(index.Params.image))}} {{$alt:=.Get\u0026#34;alt\u0026#34;}} {{$width:=.Get\u0026#34;width\u0026#34;}} {{$borderless:=.Get\u0026#34;borderless\u0026#34;}} {{$placeholder:=($image.Resize\u0026#34;48x q20\u0026#34;)|images.Filter(images.GaussianBlur6)}} {{$src:=$image}} {{$src_set:=\u0026#34;\u0026#34;}} {{$src_set=(print$image.RelPermalink\u0026#34; \u0026#34;$image.Width\u0026#34;w\u0026#34;)}} {{$src:=$image}} {{ifge$image.Width\u0026#34;500\u0026#34;}} {{$x_small:=$image.Resize\u0026#34;500x\u0026#34;}} {{$src_set=(print$src_set\u0026#34;, \u0026#34;$x_small.RelPermalink\u0026#34; 500w\u0026#34;)}} {{end}} {{ifge$image.Width\u0026#34;800\u0026#34;}} {{$small:=$image.Resize\u0026#34;800x\u0026#34;}} {{$src_set=(print$src_set\u0026#34;, \u0026#34;$small.RelPermalink\u0026#34; 800w\u0026#34;)}} {{end}} {{ifge$image.Width\u0026#34;1200\u0026#34;}} {{$medium:=$image.Resize\u0026#34;1200x\u0026#34;}} {{$src_set=(print$src_set\u0026#34;, \u0026#34;$medium.RelPermalink\u0026#34; 1200w\u0026#34;)}} {{end}} {{ifgt$image.Width\u0026#34;1500\u0026#34;}} {{$large:=$image.Resize\u0026#34;1500x\u0026#34;}} {{$src_set=(print$src_set\u0026#34;, \u0026#34;$large.RelPermalink\u0026#34; 1500w\u0026#34;)}} {{end}} {{$border_class:=\u0026#34;image-border\u0026#34;}} {{if$borderless}} {{$border_class=\u0026#34;\u0026#34;}} {{end}} \u0026lt;noscript\u0026gt; \u0026lt;style\u0026gt; figure.lazy { display: none; } \u0026lt;/style\u0026gt; \u0026lt;figure class=\u0026#34;{{$border_class}}\u0026#34;\u0026gt; {{if.Get\u0026#34;lightbox\u0026#34;}} \u0026lt;a href=\u0026#39;{{$image.RelPermalink}}\u0026#39;\u0026gt; {{end}} \u0026lt;img src=\u0026#34;{{$src.RelPermalink}}\u0026#34; {{if$width}}width=\u0026#34;{{$width}}\u0026#34;{{end}} /\u0026gt; {{if.Get\u0026#34;lightbox\u0026#34;}} \u0026lt;/a\u0026gt; {{end}} \u0026lt;figcaption\u0026gt; \u0026lt;em\u0026gt;{{.Inner}}\u0026lt;/em\u0026gt; \u0026lt;/figcaption\u0026gt; \u0026lt;/figure\u0026gt; \u0026lt;/noscript\u0026gt; \u0026lt;figure class=\u0026#34;{{$border_class}}lazy\u0026#34;\u0026gt; {{if.Get\u0026#34;lightbox\u0026#34;}} \u0026lt;a href=\u0026#39;{{$image.RelPermalink}}\u0026#39;\u0026gt; {{end}} \u0026lt;img class=\u0026#34;lazyload\u0026#34; data-sizes=\u0026#34;auto\u0026#34; src=\u0026#34;{{$src.RelPermalink}}\u0026#34; {{if$width}}width=\u0026#34;{{$width}}\u0026#34;{{end}} srcset=\u0026#34;data:image/jpeg;base64,{{$placeholder.Content|base64Encode}}\u0026#34; data-src=\u0026#34;{{$src.RelPermalink}}\u0026#34; data-srcset=\u0026#34;{{$src_set}}\u0026#34; width=\u0026#34;{{$image.Width}}\u0026#34; height=\u0026#34;{{$image.Height}}\u0026#34; alt=\u0026#34;{{$alt}}\u0026#34; /\u0026gt; {{if.Get\u0026#34;lightbox\u0026#34;}} \u0026lt;/a\u0026gt; {{end}} {{if.Inner}} \u0026lt;figcaption\u0026gt; \u0026lt;em\u0026gt;{{.Inner}}\u0026lt;/em\u0026gt; \u0026lt;/figcaption\u0026gt; {{end}} \u0026lt;/figure\u0026gt;\r\r\nAdding Javascript There are a few CSS classes set here that acts as a signal to different Javascript features. lightbox is such a thing, but the interesting one here is lazyload. I use a Javascript library called lazysizes that is included in my Javascript bundle and I have created the HTML to work with that library.\nYou might have noticed that the code above looks somewhat duplicated, that\u0026rsquo;s because I have a noscript tag for those with Javascript disabled. In that case, a srcset tag is still used to provide the best image, but without the lazy loading.\nYou might also have noted that there is no \u0026ldquo;blur up effect\u0026rdquo; here. That\u0026rsquo;s a conscious decision, since this is a bit simpler to implement and feels faster, in my opinion.\nPerformance consideration In Hugo\u0026rsquo;s documentation for image processing it\u0026rsquo;s clearly stated that it\u0026rsquo;s recommended to include the generated images in source control. This is to avoid generating the same images over and over again. On a large site with lots of images, this can make a big difference.\n","dateformatted":"5, February 2020","dateiso":"2020-02-05T04:42:21+01:00","ref":"/lazy-loading-images-in-hugo/","summary":"When writing another post, I realised that I hadn't documented/described my image lazy loading implementation in Hugo anywhere, so here it comes. The first thing we need are responsive images, not the thing you get when setting the width to 100% in CSS, but different versions of the same image in different resolutions so that the web browser can pick the best one (using srcset).","tags":["Hugo"],"title":"Lazy Loading Images in Hugo"},{"content":"So, I upgraded Cypress from 3.7.0 to 3.8.2 and ran everything locally - no problems.\nBut the build failed both in GitHub Actions and Netlify with the following messages:\n7:07:06 PM: [3/4] Linking dependencies...\r7:07:12 PM: [4/4] Building fresh packages...\r7:07:16 PM: error /opt/build/repo/node_modules/@types/testing-library__cypress/node_modules/cypress: Command failed.\r7:07:16 PM: Exit code: 1\r7:07:16 PM: Command: node index.js --exec install\r7:07:16 PM: Arguments:\r7:07:16 PM: Directory: /opt/build/repo/node_modules/@types/testing-library__cypress/node_modules/cypress\r7:07:16 PM: Output:\r7:07:16 PM: Installing Cypress (version: 3.7.0)\r7:07:16 PM: [18:07:13] Downloading Cypress [started]\r7:07:16 PM: [18:07:13] Downloading Cypress 0% 0s [title changed]\r7:07:16 PM: failed during stage 'building site': Build script returned non-zero exit code: 1\rTwo things stuck out here:\n It\u0026rsquo;s types for testing-library__cypress that fails Cypress 3.7.0 is being downloaded for installation, even though I just upgraded to 3.8.2. So, the types for Cypress Testing Library has a dependency to version 3.7.0 of Cypress, but of course I only want one version of it. When using Yarn, this can be solved with Selective dependency resolutions, by adding the following to package.json:\n\u0026quot;resolutions\u0026quot;: {\r\u0026quot;cypress\u0026quot;: \u0026quot;3.8.2\u0026quot;\r}\r🎉 Success!\nI also learned that Yarn has a why command that one can use to find out why a package is installed. yarn why cypress tells me this:\n=\u0026gt; Found \u0026quot;cypress@3.8.2\u0026quot;\rinfo Has been hoisted to \u0026quot;cypress\u0026quot;\rinfo Reasons this module exists\r- Specified in \u0026quot;devDependencies\u0026quot;\r- Hoisted from \u0026quot;@testing-library#cypress#@types#testing-library__cypress#cypress\u0026quot;\r","dateformatted":"29, January 2020","dateiso":"2020-01-29T11:30:09+01:00","ref":"/yarn-resolutions-to-fix-build-error-with-multiple-versions-of-npm-package/","summary":"So, I upgraded Cypress from 3.7.0 to 3.8.2 and ran everything locally - no problems. But the build failed both in GitHub Actions and Netlify. I now had two conflicting versions. When using Yarn, this can be solved with Selective dependency resolutions, by adding the following to package.json","tags":["NodeJS"],"title":"Yarn Resolutions to Fix Build Error With Multiple Versions of NPM Package"},{"content":"This is an appeal to any of you who is coding for, or otherwise developing connected devices, to put some extra thought into your network traffic. Maybe also a warning to consumers and an encouragement to us tech savvy people to monitor your connected devices.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rYamaha YAS-109 soundbar\r\r\r\r\r\r\r\rYamaha YAS-109 soundbar\r\r\r I recently bought a soundbar for my TV, a Yamaha YAS-109. When I looked at my Pi-hole dashboard a few weeks later, I was surprised. The soundbar was by far the most active device on my network, event though it has mostly been \u0026ldquo;turned off\u0026rdquo; (standby mode).\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rThe most active device I have in my network is apparently the soundbar (192.168.1.197), by far.\r\r\r\r\r\r\r\rThe most active device I have in my network is apparently the soundbar (192.168.1.197), by far.\r\r\r Looking at what all these requests are, filtered on the soundbar only, shows the following.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r Clearly it\u0026rsquo;s calling the Alexa service a lot and I want to point out that I have hit the mute button, that according to the manual, should ensure privacy (disable Alexa). The manual also states that I have to configure an Amazon account if I want to use the Alexa service, which I haven\u0026rsquo;t done.\nResetting Pi-hole statistics and just looking at a few minutes of traffic, I saw the following requests:\n ap.spotify.com (~14 times/min) avpro.global.yamaha.com (~8 times/min) avs-alexa-na.amazon.com (~8 times/min) www.google.com (~3 times/min) various NTP servers (~3 times/min) Note that this was logged when the device was in standby mode and no traffic is blocked, so it shouldn\u0026rsquo;t be a bad retry logic gone wild. The soundbar has Spotify Connect, but I haven\u0026rsquo;t used that feature. Why it needs to contact www.google.com so often, or frankly - at all, is beyond my understanding. Although I have a bad habit of checking my wrist watch a bit too often, I think calling a time server several times a minute for a soundbar in standby mode is just ridiculous.\nThis isn\u0026rsquo;t the worst example of an Internet of Shit device, since the core functionality of being a speaker works perfectly fine without an Internet connection (that\u0026rsquo;s the way I run it now). I don\u0026rsquo;t know why it\u0026rsquo;s \u0026ldquo;calling home\u0026rdquo; so frequently, it might not be evil, but it sure abuses my network in a non-justifiable way.\nSo, if your company produces similar consumer electronics and especially if you\u0026rsquo;re writing the code for it, please don\u0026rsquo;t do this. If you\u0026rsquo;re not monitoring your home network, please do. Setting up Pi-hole is really simple!\n","dateformatted":"21, January 2020","dateiso":"2020-01-21T11:30:09+01:00","ref":"/yamaha-yas-109-soundbar-has-excessive-network-traffic/","summary":"This is an appeal to any of you who is coding for, or otherwise developing connected devices, to put some extra thought into your network traffic. Maybe also a warning to consumers and an encouragement to us tech savvy people to monitor your connected devices.","tags":["Internet of Things","Hardware"],"title":"Yamaha YAS-109 Soundbar has Excessive Network Traffic"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rOriginal photo by Jon Tyson\n\r\r\r\r\r\rOriginal photo by Jon Tyson\n\r\r Having built two personal websites/blogs that are fairly similar, one using Gatsby and one using Hugo, I\u0026rsquo;ll take a moment to compare my experiences.\nCommon Both these sites are essentially blogs with a few extra pages. They don\u0026rsquo;t have a lot of content, 40 and 50 blog posts respectively. They\u0026rsquo;re also single-language, one in English and one in Swedish. All CSS is written from scratch for both sites, no framework. They both have categories and/or tags that you can use to find related posts. No server-side API is used, so once the build is done, everything is static. Well, Google Analytics and Disqus are used on both sites, but those are third-party API\u0026rsquo;s that I don\u0026rsquo;t have to manage. Both are also hosted at Netlify.\nBoth sites are lazy loading images that are generated in different sizes during build. There is also in-browser search provided with Lunr. A search index file is created during build and shipped to the browser, where the actual searching is performed.\nIn case you wonder, these are the sites I\u0026rsquo;m talking about:\n www.isabelsommerfeld.com (Gatsby) www.henriksommerfeld.se (Hugo) Differences While the sites I\u0026rsquo;ve built are pretty much the same, Gatsby and Hugo are two different animals, despite both being static site generators.\nHugo \u0026ldquo;The world’s fastest framework for building websites\u0026rdquo;\n Hugo is a single binary you download. It parses your templates, compiles Sass to CSS, concatenates bundles and does all sorts of image processing. What it does not do is anything related to JavaScript. Go templates, HTML and CSS is the kind of development you do that concerns Hugo. Any JavaScript you need will be DOM manipulation. I started with jQuery and recently migrated to VanillaJS.\nThe most odd part for most web developers is probably the Go templates you use to write your logic. If you have previous experience with Go, this is of course familiar territory. I haven\u0026rsquo;t, but have managed to get things working through trial and error.\nWhen Hugo brags about performance, it claims to be \u0026ldquo;The world’s fastest framework for building websites\u0026rdquo;, it\u0026rsquo;s all about build time. The build times are really fantastic, but anything that happens in the browser is beyond Hugo\u0026rsquo;s interest.\nI like the simplicity of Hugo. I can control every character of the generated HTML and there are no plugins to install. Especially compared to Gatsby, Hugo feels really simple. If you start to add a lot of JavaScript (like the JS based search I have), you\u0026rsquo;re on your own, but of course NPM has much to offer even if there are no packages made specifically for Hugo sites.\nThe documentation is okay, but sometimes I miss the context needed to understand where a piece of code is supposed to go in my project.\nGatsby \u0026ldquo;Fast in every way that matters\u0026rdquo;\n Compared to Hugo, Gatsby is more of a dev environment. If you\u0026rsquo;re used to React and the NPM ecosystem, Gatsby will feel familiar. Gatsby\u0026rsquo;s official blog starter has 2182 dependencies (according to npm ls --parseable | wc -l), that\u0026rsquo;s 2181 more than Hugo\u0026rsquo;s single binary. But that also comes with some advantages: there\u0026rsquo;s a plugin for almost anything you want to do and all the optimisations for pre-loading content for other routes, only loading things above the scroll and much more, is taken care of.\nThe documentation is outstanding, which really helps in taking advantage of the ecosystem around Gatsby. Many plugins have both a deployed demo site and the code for it easily accessible so that you can see how to use it in context of a real site, or clone it locally and run it.\nGatsby is sort of the opposite to Hugo when talking about performance. It states \u0026ldquo;Fast in every way that matters\u0026rdquo;, which refers to everything but build time. Build time is probably the biggest weakness apart from its complexity.\nConclusion I will use Hugo in the future when I know I won\u0026rsquo;t have to do much with JavaScript - that will be simpler to maintain.\nI will use Gatsby when I know I will use a fair amount of JavaScript on the site or that I might need to later on. With Gatsby I can grow and continue to add functionality with plugins. In those cases I can also live with the extra maintenance cost of keeping the NPM packages up-to-date.\n","dateformatted":"5, December 2019","dateiso":"2019-12-05T22:20:34+01:00","ref":"/gatsby-vs-hugo-for-personal-blog/","summary":"Having built two personal websites/blogs that are fairly similar, one using Gatsby and one using Hugo, I’ll take a moment to compare my experiences. All CSS is written from scratch for both sites, no framework. They both have categories and tags that you can use to find related posts. No server-side API is used, so once the build is done, everything is static. Well, I use Google Analytics and Disqus on both sites, but those are third-party API's that I don't have to manage.","tags":["Hugo","GatsbyJS","JavaScript","ReactJS"],"title":"Gatsby vs Hugo for a Personal Blog"},{"content":" timed out initializing value. This is most likely a circular loop in a shortcode\n I have been getting a few random build errors with Hugo on Netlify recently. This is a bit strange, since it builds fine on my local machine and on Github Actions. Re-running the same build also worked a few times. The build log on Netlify says the following:\nHugo Static Site Generator v0.59.1-D5DAB232/extended linux/amd64 BuildDate: 2019-10-31T15:28:09Z\rBuilding sites …\rTotal in 25937 ms\rError: Error building site: \u0026quot;/opt/build/repo/content/article/2016-11-01-my-iot-exploration-part-2-raspberry-pi-sense-hat/index.md:1:1\u0026quot;: timed out initializing value. This is most likely a circular loop in a shortcode\rThe timeout part is correct, but the guess about a loop in a shortcode is not. I recently added image processing and I\u0026rsquo;m doing that in a shortcode, so it was pretty clear this problem was related to that.\nThe actual problem is that the build with image processing takes longer than 10 seconds, which it the default timeout for a build. Increasing the timeout in my config.toml solved the problem:\ntimeout = 60000\rUpdate: I mention that this was caused by adding image processing, but when you do that you should commit the generated images files to source control, according to Hugo’s documentation for image processing. I hadn\u0026rsquo;t done that, and that\u0026rsquo;s why the builds were suddenly taking a lot longer than before.\n","dateformatted":"30, November 2019","dateiso":"2019-11-30T18:19:09+01:00","ref":"/hugo-timeout-not-a-circular-loop-in-shortcode/","summary":"I have been getting a few random build errors with Hugo on Netlify recently. This is a bit strange, since it builds fine on my local machine and with Github Actions. Re-running the same build also worked a few times. The build log on Netlify says the following...","tags":["Hugo"],"title":"Hugo Timeout Not a Circular Loop in Shortcode"},{"content":"This is a story about my personal computer (PC) experience, from the 1990\u0026rsquo;s up to my new love for a six year old laptop.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r In November 2019 my wife bought a new laptop (MacBook Air) and I took over the old one. It\u0026rsquo;s a 2013 MacBook Pro that I have borrowed a few times for debugging web pages on iOS, but beyond that I don\u0026rsquo;t have much macOS experience. I saw the opportunity to take it and learn macOS for real for the first time.\nHistory I grew up with DOS (Disk Operating System, not Denial of Service) and Windows during Microsoft\u0026rsquo;s glory days of total dominance in desktop computing. Computers were fascinating by themselves without thinking about alternatives to Windows for Workgroups 3.11.\n It strikes me that we sometimes talk about Desktop and Mobile nowadays. By Desktop we then mean Laptop. Those were once two different things.\n At university, a new world of Solaris and Linux came to my awareness. I ran Slackware as my primary OS for quite some time when the computer was mostly used to learn computers. For that use, it was perfectly fine to spend most of the time in config files and compiling custom kernels.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rMy student room with a computer on the floor, a \"full tower\", and some other important equipment.\r\r\r\r\r\r\r\rMy student room with a computer on the floor, a \"full tower\", and some other important equipment.\r\r\r I found this photo of my student room, probably from 2002 when I was moving in. It reveals a bunch of important equipment at the time:\n Two purchased versions of Suse Linux (I remember that was my entry drug to Linux) A bunch of computer related books, of which I can read \u0026ldquo;C++\u0026rdquo; on one of them A 3.5\u0026quot; floppy disk 💾 Large number of CD\u0026rsquo;s for\u0026hellip;2002\u0026rsquo;s version of a film offline watch list. A bottle of Jägermeister As I started working after graduation, Windows was again the default choice and still is where I work today. Especially as a .Net developer specialised in SharePoint, Windows was a requirement, not a choice.\nMy New Mac 😍 My new MacBook Pro is fantastic. It runs the latest version of macOS (Catalina) and I\u0026rsquo;m automatically logged in with my Apple Watch! I\u0026rsquo;ll mostly compare this machine to my work laptop, an HP EliteBook 840 G5 from 2018 (that has a fingerprint reader, that is disabled by a domain policy).\nShell I use Oh My Zsh which is unsurprisingly quite like its PowerShell counterpart oh-my-posh, that I\u0026rsquo;m used to. With just a couple of lines (below) in my ~/.zshrc, ZSH feels like an upgrade.\nplugins=(git zsh-nvm zsh-autosuggestions)\rsource $ZSH/oh-my-zsh.sh\ralias ls=\u0026quot;exa\u0026quot;\rautoload -U compinit\rcompinit\rText in the terminal also look great (I\u0026rsquo;ve always wondered who at Microsoft decided that red error messages on dark blue background was the way to go).\nEven emojis work! I had to pause for a moment after my first Homebrew installation and admire the beer mug 🍺 The emojis look good too, there is no thick black border around them, amazing!\nAesthetics It\u0026rsquo;s not just emojis, everything looks good. It\u0026rsquo;s like there is only one design system used, even for third-party apps. I can\u0026rsquo;t even find an ugly icon, nothing that reminds me of my days as a student.\nPackage Manager Hombrew is great and the given choice. Being able to install and uninstall both CLI tools and GUI apps of current versions through the command-line is great. I\u0026rsquo;ve only used Homebrew for about two weeks now, but I haven\u0026rsquo;t once seen a big chunk of red XML in the console (like I occasionally do with Chocolatey).\nI\u0026rsquo;ve saved my currently installed apps in a Brewfile (with brew bundle dump) as a reference. If I install from scratch next time, I guess I\u0026rsquo;ll copy that file, make some changes and run brew bundle – all done.\nPerformance Performance is better than I expected, CPU load is lower than on my HP laptop, doing roughly the same things. I have a few Electron apps running and I\u0026rsquo;ve mostly been fiddling with a Gatsby project. I was a bit worried about RAM, since the Mac has 8 gb and the HP has 16, but so far so good. I even installed Adobe Photoshop 2020 and have used it for simpler edits. I\u0026rsquo;m sure it will be slow for heavier use, but since I have a licence, I might as well use it. What I\u0026rsquo;m not running on the Mac is WMI Provider Host and Antimalware Service Executable, which is using a significant amount of resources on my Windows machine.\nTrackpad Have you heard a reviewer on YouTube reviewing a Windows laptop and not comparing the trackpad to the Mac\u0026rsquo;s? I haven\u0026rsquo;t, and there\u0026rsquo;s a reason for that - it\u0026rsquo;s great!\nDisplay The display on my 2013 MacBook Pro is so much better than on my 2018 HP EliteBook. Colour, sharpness, brightness, everything. Sure there are Windows laptops with great displays, but the plethora of Windows laptops with non-great displays often attract those responsible of budget (maybe at your employer).\niMessage Since I was already an iPhone user, having iMessage on the computer is great. I\u0026rsquo;ve been trying to get people to write to me through WhatsApp or Facebook Messenger before, so that I could reply from my computer, but iPhone users keep going back to iMessage.\nThe Future Looking ahead, I can see myself continuing with Macs. The high prices, brittle keyboards, dongle dependency, replacing Esc and Fn keys with Touch Bar etc, have kept me away from Macs. None of those problems exist on my 2013 MacBook, and if Apple continue in the direction set by the 16\u0026quot; MacBook Pro of making their products better, I might buy one when this 2013 one goes to its final rest.\n","dateformatted":"28, November 2019","dateiso":"2019-11-28T21:29:10+01:00","ref":"/i-love-my-new-2013-macbook-pro/","summary":"This is a story about my personal computer (PC) experience, from the 1990’s up to my new love for a six year old laptop.\nIn November 2019 my wife bought a new laptop (MacBook Air) and I took over the old one. It’s a 2013 MacBook Pro that I have borrowed a few times for debugging web pages on iOS, but beyond that I don’t have much macOS experience. I saw the opportunity to take it and learn macOS for real for the first time.","tags":["macOS","Hardware"],"title":"I ❤️ My \"New\" 2013 MacBook Pro"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r When I needed a CMS for a Gatsby site, my choice became Netlify CMS. I\u0026rsquo;ll talk about my criteria, pros and cons.\nThis was a personal website made for a non-technical person (why I needed a CMS at all in the first place) who associates websites with WordPress. She, my wife, is fairly tech savvy, but wouldn\u0026rsquo;t accept editing markdown or other \u0026ldquo;complicated things\u0026rdquo; 🙃 My hypothesis was that I could somewhat compensate for unintuitive features with some \u0026ldquo;on site training\u0026rdquo;.\nCriteria Easy to set up authentication/authorisation Use Markdown for storage in a location I decide Customisable, for good user experience Good developer experience Cheap (ideally free) So, the things I needed were the above. Pretty much all hosted services tick the first checkbox of easiness to create an account and managing identity. That\u0026rsquo;s the thing you need a server for, which you might not already have when your site is static.\nBut looking at the available headless CMS options, like on headlesscms.org, there aren\u0026rsquo;t many alternatives if you want a Git based CMS that is open source. One of the things I was aiming for was using Markdown for content, since I imagine that will be easier to migrate in the future than the experience I had migrating this content from WordPress (and especially WordPress.com). I can also mention that TinaCMS wasn\u0026rsquo;t released at the time I did the initial research.\nAnyway, since I had previous great experience with Netlify, Netlify CMS was already at the top of my list. After reading the post Gatsby and Netlify CMS: First Impressions, I decided to start with the one click installation button that Netlify provides.\nSometime later when I have the site up and running, I can reflect on my Netlify CMS experience. I\u0026rsquo;ll start with the positive.\nPros 😄 Deploys with the website (no external hosting) Free with Netlify hosting up to 5 users Easy to add route specific edit link Instant previews that you can code yourself Easy to configure page types and fields Possibility to add custom editor controls Netlify CMS is just an extra dependency (Gatsby plugin) that lives together with your site. When you add a field to a page, the site changes and CMS changes required, can go in the same commit and deploy. That\u0026rsquo;s great, automatically in sync.\nWhen hosting your site on Netlify, you can add up to 5 additional users that can log in to the CMS as editors/admins. This is configurable through your account on netlify.com as easily as any other setting.\nAdding a link on the site that takes you to the edit mode of that specific page is a matter of just using the right URL. Maybe not the most important feature, but a small thing that makes a site owner\u0026rsquo;s life a bit easier.\nThe preview of a page you\u0026rsquo;re editing is shown right next to the rich text editor as you type. How these previews should look is up to you (the developer). It took me some fiddling to figure out how I could reuse the styling from the Gatsby site, where I\u0026rsquo;m using styled components, but it\u0026rsquo;s great that this is just a matter of coding.\nPages and their fields can be configured in a YAML file, see the documentation for Collection Types. Apart from being a YAML file (that\u0026rsquo;s easy to mess up), it works as expected. Adding custom editor controls (called widgets) is also documented at Creating Custom Widgets.\nCons 🙁 Saving changes locally requires extra configuration Many open issues on GitHub Rich Text Editor in need of love Only possible to preview the part you\u0026rsquo;re editing Easy to break preview with Gatsby Bad HTML makes automated UI testing harder Bad Default DX There are a couple of downsides as well. The first and most obvious one is that by default all changes done through the CMS\u0026rsquo;s UI, even running on localhost, is done against your remote Git repository. This was a big hurdle for me initially before I got the local configuration to work and I seriously started to look for other alternatives. I cannot see how anyone could live with this during development, I think it\u0026rsquo;s very unintuitive and I\u0026rsquo;m not alone.\nThere are a fairly high number of open issues on GitHub for this code base. Some things are small and can be worked around in Gatsby, but it absolutely doesn\u0026rsquo;t feel as polished as some of the commercial alternatives.\nExample: I have a tags field which is entered as a comma separated list and saved as a list in markdown. If I don\u0026rsquo;t have any tags in a post, Netlify CMS saves this as a list of one item with the value of an empty string. When Gatsby then tries to create a route for each tag and the tags provided are [\u0026quot;\u0026quot;], it\u0026rsquo;s a problem. Of course, it\u0026rsquo;s possible to compensate for such things, but small things like this is something you will need to handle.\nRich Text Editor The Rich Text Editor could be improved in a number of ways. It uses an older version of Slate that has some issues solved in later versions, but an upgrade seems complicated. Also, there is no way to customise the editor. Take a look at this screenshot (the red lines are my hints).\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Inserting an image is accessed by expanding extra controls. Yet there are two always visible buttons for adding code. Who needs a Rich Text Editor? People who want to add code or people who want to add images? 🤔 All right, those aren\u0026rsquo;t mutually exclusive, but admit it\u0026rsquo;s a bit strange.\nPreviews Previews only have access to the data you enter in the current edit view, it can\u0026rsquo;t show the whole website. A consequence of that is that it\u0026rsquo;s quite easy to break the previews when developing the website (and not looking in the CMS). You want to render the same React components for previews as you do on the website, but you can\u0026rsquo;t have components with GraphQL queries. I have broken the previews a number of times during development and not realised it until quite some time later when I was checking the CMS. The text \u0026ldquo;Error: The result of this StaticQuery could not be fetched\u0026rdquo; is all too familiar to me. If only Cypress could fix this issue (open since May 2016), I could at least write tests to see if I have messed up the previews.\nOne more thing can be said about testability. If the HTML had been better, it would have been easier to write automated tests, since I wouldn\u0026rsquo;t have to use brittle selectors. On the other hand, I have yet to see a CMS with good HTML and I have a pretty extensive test suite that edits pages and creates a new blog post through the CMS and verifies it on the website.\nConclusion 🙂 All in all, I think Netlify CMS is a great choice given the criteria I had in this case. If I had a bigger budget and harder requirements on editorial features I might have come to another conclusion, but I\u0026rsquo;m perfectly satisfied for this use case.\nGiven that the product owner/solo editor/site admin/my wife had such a strong bias towards WordPress before I showed Netlify CMS, I was positively surprised how easy it was to sell it to her. Not being able to quickly preview the whole page wasn\u0026rsquo;t a problem at all and she instead commented on how great it was to see the preview update immediately as she typed. The web interface was also complemented in comparison with the WordPress editor: \u0026ldquo;there\u0026rsquo;s a lot less going on here\u0026rdquo;.\nFinally, I have to say that I love the kind of services (primarily from Netlify, GitHub and Gatsby in this case) that have generous free tiers that I can use and learn about for personal use to then be aware of during architectural discussions at work. I hope such business model is sustainable even though there are leechers like me, thank you!\n","dateformatted":"22, November 2019","dateiso":"2019-11-22T00:02:10+01:00","ref":"/netlify-cms/","summary":"When I needed a CMS for a Gatsby site, my choice became Netlify CMS. I’ll talk about my criteria, pros and cons.\nThis was a personal website made for a non-technical person (why I needed a CMS at all in the first place) who associates websites with WordPress. She, my wife, is fairly tech savvy, but wouldn’t accept editing markdown or other “complicated things” 🙃 My hypothesis was that I could somewhat compensate for unintuitive features with some “on-site training”.","tags":["Netlify CMS","GatsbyJS","ReactJS"],"title":"Netlify CMS with Gatsby - Best Option with Some Quirks"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Jay Wennington on Unsplash. \r\r\r\r\r\rPhoto by Jay Wennington on Unsplash. \r\r So, I decided to implement dark mode on my website (to tackle parental leave boredom). I\u0026rsquo;ll describe what I did and what I could have done differently in hindsight in these areas:\n Different CSS files VS body selector Provide a switch between modes? Messing up Sass variables and CSS variables Adjust images with filter Background Supporting different themes on a website is very simple, I remember that I built three different themes that you could choose between on my website almost 20 years ago when I was learning about CSS and the semantic web. It\u0026rsquo;s just a matter of switching .css files. Those themes had different layout where the navigation would be placed differently and much more. Dark/light mode is just a matter of changing colours.\nOf course the web has evolved a bit since the Web\u0026rsquo;s youth. The basic techniques you need today are:\n CSS variables Media queries My starting point was the extensive blog post named Hello darkness, my old friend. It gives a background and describes an implementation that I pretty much followed.\nIt uses a light.css and a dark.css which are conditionally loaded with media queries. These files define the same set of CSS variables that are then used in a main.css.\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;/dark.css\u0026quot; media=\u0026quot;(prefers-color-scheme: dark)\u0026quot;\u0026gt;\r\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;/light.css\u0026quot; media=\u0026quot;(prefers-color-scheme: no-preference), (prefers-color-scheme: light)\u0026quot;\u0026gt; This works great and uses the theme of the operating system, here light theme is the default if the browser doesn\u0026rsquo;t provide this information. It\u0026rsquo;s when you add the possibility to manually switch between themes it becomes a bit more complicated.\nDifferent CSS files VS body selector A slightly different approach is to place the media query inside a main.css and include all the CSS variables there. My site is build with Hugo and uses one main CSS file, rather than a component system, so I don\u0026rsquo;t have any deferring of non-critical CSS anyway. Given that, I think a few extra CSS variables wouldn\u0026rsquo;t make difference.\nIn that case I could set a class or data attribute on the body tag and use that as my condition in CSS.\n\u0026lt;body class=\u0026quot;dark-theme\u0026quot;\u0026gt;\rProvide a switch between modes? Do you really need a switch or is it sufficient with the device setting for light/dark? If I hadn\u0026rsquo;t taken the extra steps of adding a switch, it would have been a lot simpler, and maybe good enough, in hindsight. Given that I have a switch, borrowed from Pure CSS Smooth Toggle Swtich Demo, I think it would have been easier to toggle a CSS class or data attribute than modifying the CSS link tags like I do.\nMessing up Sass variables and CSS variables A stupid mistake I made a couple of times during development was to confuse the syntax of Sass/SCSS variables and CSS variables, especially when using Sass functions such as lighten, darken and transparentize. Since CSS variables aren\u0026rsquo;t checked at compile time, a few Sass variables sneaked into the resulting CSS file.\n\r\r\r--color: #CBD5E0; --accent-color: #ED8936; --color-published: #{darken(var(--color), 10%)}; /* somewhat tricky syntax */ --background-color: #2D3748; --blockquote-color: var(--accent-color); --header-text-shadow: rgba(26,32,44,0.5);\r\r Adjust images with filter A bonus feature I borrowed straight from Hello darkness, my old friend is to mute images a bit. I didn\u0026rsquo;t want to go to the extremes, but I do decrease brightness and saturation somewhat in dark mode.\n\r\r\r/* light.scss */ :root { --image-filter: none; } /* dark.scss */ :root { --image-filter: brightness(0.9) saturate(0.9) drop-shadow(0px 0px 2px #{$gray900}); } /* main.scss */ figure img { filter: var(--image-filter); }\r\r ","dateformatted":"15, November 2019","dateiso":"2019-11-15T14:14:10+01:00","ref":"/dark-mode-learnings/","summary":"So, I decided to implement dark mode on my website (to tackle parental leave boredom). I'll describe what I did and what I could have done differently in hindsight.","tags":["Dark mode","CSS"],"title":"Dark Mode Learnings 🌙"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rScreenshot of the end result.\n\r\r\r\r\r\rScreenshot of the end result.\n\r\r Intro I was rewriting a WordPress site in Gatsby that had embedded tweets using an embed script (WordPress plugin). The product owner (my wife), required the new site to also show her tweets. I didn\u0026rsquo;t like the idea of an embed script (that would slow down the site and spy on visitors), so I started to look into Gatsby source plugins. I ended up writing my own.\nAn important and maybe obvious reflection is that reading tweets at build time will require a lot of builds if you\u0026rsquo;re a frequent tweeter. Then a simple embed code might be a better option and you save yourself some development time.\n unfurl.js is used to fetch additional metadata (Twitter Card / Open Graph) from linked websites. Images are downloaded so they can be served with gatsby-image\n Initial Attempt I tried gatsby-source-twitter, which fetches data from Twitter\u0026rsquo;s API at build time and makes it available through GraphQL. This worked liked promised, but I found a couple deal breaking flaws:\n Breaks build if no Twitter credentials are available Breaks build if data changes (dynamic GQL schema) Only fetches links, not the actual content (images etc.) My way of tackling these flaws wouldn\u0026rsquo;t work in a reusable plugin, so I just made different decisions about pros and cons.\n I should mention that this site I was building was my first ever encounter with Gatsby\n Breaks build if no Twitter credentials are available Twitter\u0026rsquo;s API requires authentication, so credentials must be provided in the plugin\u0026rsquo;s config. But since you don\u0026rsquo;t want to include credentials in version control, it means that you can\u0026rsquo;t simply clone a repo that uses this plugin and build the project (without going through the app registration process on Twitter\u0026rsquo;s Developer site).\nFurthermore I like to be able to disable this data fetching, like when developing and running repeated builds locally.\nBreaks build if data changes (dynamic GQL schema) This is a real deal breaker that I\u0026rsquo;ve struggled with even when querying markdown content (not at all related to this plugin). The GraphQL schema is created dynamically based on the content, so if you query a property for uploaded images in a tweet and none of the returned tweets have an image, the query fails and you have a broken build. When querying an external data source that you have no control over, this is unbearable.\nOnly fetches links, not the actual content (images etc.) This last point is by no means a criticism, but what I found when I got it all working (a working Tweets React component) was that a tweet that is only a link, e.g. https://t.co/DM6et4kaZP, doesn\u0026rsquo;t say that much. If you look at the same tweet at twitter.com, it automatically includes the linked site\u0026rsquo;s Twitter card/Open Graph data (title, description and image).\nI guess this could be a transformer plugin and not the responsibility of a source plugin, but I felt I needed this (more details below).\n The site builds fine without the tweets section\n I should mention that this site I was building was my first ever encounter with Gatsby, so I consider myself an enthusiastic beginner. I\u0026rsquo;d be happy for any corrections of my possible misunderstandings.\nWhat I ended up doing was creating a local plugin by shamelessly copying the gatsby-source-twitter plugin and customising it.\nOnly supporting one query Since this plugin is local and I only had one specific use-case, I removed support for all but the query I was going to use.\nAlways returning a dummy tweet By always returning one hard-coded dummy tweet that has all the properties I query for in my React component, I avoid the potential build errors. I realise this is more of a workaround than a proper \u0026ldquo;solution\u0026rdquo;, but it work for me. The downside is that I have to filter out this dummy tweet in my component, making the plugin non-reusable.\nChecking for credentials If any of the three required Twitter API credentials (key, secret, token) isn\u0026rsquo;t provided, only the dummy tweet is returned and the site builds fine without the tweets section.\nEnriching tweets After fetching the data from Twitter\u0026rsquo;s API, unfurl.js is used to fetch additional metadata (Twitter Card / Open Graph) from linked websites. Images are downloaded so they can be served with gatsby-image.\n\r\r\r\rexports.sourceNodes = async ( { actions, createContentDigest, reporter }, { query, credentials } ) =\u0026gt; { if (!query) { reporter.warn(`No Twitter query found. Please check your configuration`); return Promise.resolve(); } const tweetsQueryResult = await getTweets(query, credentials, reporter); const enrichedTweets = await fetchMetadataFromLinkedSites( tweetsQueryResult.results, reporter ); await fetchImagesFromTweets(enrichedTweets, reporter); const resultsWithDummy = [dummyTweet, ...enrichedTweets]; tweetsQueryResult.results = resultsWithDummy; await createNodesForTweets( tweetsQueryResult, actions, createContentDigest, reporter ); return Promise.resolve(); };\r\r Core of plugin\u0026rsquo;s gatsby-node.js\n\rThe repo of the entire site can be found at https://github.com/henriksommerfeld/isabel-blog\n","dateformatted":"31, October 2019","dateiso":"2019-10-31T00:20:10+01:00","ref":"/gatsby-source-plugin-for-twitter/","summary":"I was rewriting a WordPress site in Gatsby that had embedded tweets using an embed script (WordPress plugin). The product owner (my wife), required the new site to also show her tweets. I didn’t like the idea of an embed script (that would slow down the site and spy on visitors), so I started to look into Gatsby source plugins.","tags":["GatsbyJS","JavaScript","TypeScript","NodeJS","ReactJS"],"title":"Gatsby Source Plugin for Twitter"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Free To Use Sounds on Unsplash.\n\r\r\r\r\r\rPhoto by Free To Use Sounds on Unsplash.\n\r\r I recently built a new personal website/blog for my wife using GatsbyJS and I really enjoyed all the cool stuff you can do easily with available plugins. Since didn\u0026rsquo;t really have any idea about the disadvantages with Progressive Web Apps (PWA), I naively just installed the manifest and offline plugins and verified my success with Lighthouse. Boom, I had a PWA! 🎉\nThen I tried the PWA on my iPhone 🙁 I discovered there are a number of serious issues with PWA\u0026rsquo;s on iOS, since Apple doesn\u0026rsquo;t like the web, I suppose. I am using iOS 13.1.3 at the time of writing this.\niOS PWA issues for me In-app browser for external links Navigation is different No address bar No reload. Errors persist File download weirdness External links External links are always opened in an in-app browser rather than Safari, Chrome, Firefox or whatever you prefer. For a PWA that\u0026rsquo;s perceived as a website rather than an app, even though it technically is an app, this is strange and confusing.\nNavigation You don\u0026rsquo;t have the Back and Forward buttons you\u0026rsquo;re used to, instead you can do some finger swiping that I don\u0026rsquo;t think I\u0026rsquo;ve ever made in an app (had to google it to realise that\u0026rsquo;s what I was supposed to do). The lack of an address bar also means you can\u0026rsquo;t just copy the URL to the page/route you\u0026rsquo;re on.\nPersistent errors The worst flaw I experienced was related to the offline support. After the issues described above I questioned why I really made a PWA in the first place. Offline support was the fist feature that came to mind, so I tried that.\nI activated Flight mode and clicked a few links, worked great. Then a few more links.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\rSafari cannot open the page (screenshot in Swedish)\r\r\r\r\r\rSafari cannot open the page (screenshot in Swedish)\r\r\r Hmm, okay, the error message could be a bit friendlier but an error is expected since not all resources can be prefetched. So I toggled off flight mode and\u0026hellip;\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r \u0026hellip;no difference. How do I reload the page? Force-quit the app and reopened - still the same thing. After quitting and reopening a few times I eventually got to the start page, but if I wasn\u0026rsquo;t the developer of this I would have given up earlier.\nFile download wierdness Sort of the same thing happened when I tried to download a large image (there are press images on the site). I chose to save the file and then\u0026hellip;no feedback at all. When I looked in Photos, the image was there, but how to get out of the \u0026ldquo;download mode\u0026rdquo;?\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r I tried the same thing in Chrome and that just worked like I expected, like on every web page.\nConclusion My ignorance around PWA\u0026rsquo;s became apparent as soon as I tried it and I certainly didn\u0026rsquo;t expect it to work this poorly. I guess this works better on Android, but in my opinion a website should not only work on one platform. So, my conclusion is that a PWA might be a good option if you really have an app, where you can use the things that do work on both Android and iOS, but for a regular website I will avoid it.\n","dateformatted":"18, October 2019","dateiso":"2019-10-18T12:19:10+02:00","ref":"/pwa-bad-for-blog/","summary":"I recently built a new personal website/blog for my wife using GatsbyJS and I really enjoyed all the cool stuff you can do easily with available plugins. Since didn’t really have any idea about the disadvantages with Progressive Web Apps (PWA), I naively just installed the manifest and offline plugins and verified my success with Lighthouse. Boom, I had a PWA! 🎉","tags":["Progressive Web App","GatsbyJS"],"title":"Progressive Web App - Bad Idea for a Blog"},{"content":"I\u0026rsquo;m writing this in case I forget it later.\nContext I\u0026rsquo;m migrating my wife\u0026rsquo;s blog from WordPress to GatsbyJS and since she\u0026rsquo;s not comfortable with Markdown or Git, I need a CMS. I aim at a zero-cost solution and I want the content to be part of the repository, rather than a separate thing (like what an API driven CMS provides).\nProblem Netlify CMS crashes when the frontmatter is followed by a newline and an image. The CMS itself doesn\u0026rsquo;t add this newline, but Prettier for VS Code does. I\u0026rsquo;ve been manually updating a url property when migrating old posts to keep the same URL, and doing that in VS Code inserts this \u0026ldquo;error\u0026rdquo; (perfectly valid markdown).\ndate: 2011-09-11T07:26:42.164Z\r---\r\u0026lt;==== Here\r![](/uploads/lrvlsi6ilp1r22t2co1_400.jpg \u0026quot;Some title\u0026quot;)\rThere is an issue registered at GitHub, but it seems to wait for some larger overhaul issue, so we might have to live with this for some time.\nWorkaround Remove the newline with an editor that doesn\u0026rsquo;t add it back on save.\ndate: 2011-09-11T07:26:42.164Z\r---\r![](/uploads/lrvlsi6ilp1r22t2co1_400.jpg \u0026quot;Some title\u0026quot;)\r","dateformatted":"10, September 2019","dateiso":"2019-09-10T07:36:10+02:00","ref":"/netlify-cms-incorrectly-eaten-value/","summary":"I\u0026rsquo;m writing this in case I forget it later.\nContext I\u0026rsquo;m migrating my wife\u0026rsquo;s blog from WordPress to GatsbyJS and since she\u0026rsquo;s not comfortable with Markdown or Git, I need a CMS. I aim at a zero-cost solution and I want the content to be part of the repository, rather than a separate thing (like what an API driven CMS provides).\nProblem Netlify CMS crashes when the frontmatter is followed by a newline and an image.","tags":["Netlify CMS"],"title":"Netlify CMS - Incorrectly eaten value"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Sabri Tuzcu on Unsplash. Creating depth with z-index. Mine is most important, so I set it to 999999.\n\r\r\r\r\r\rPhoto by Sabri Tuzcu on Unsplash. Creating depth with z-index. Mine is most important, so I set it to 999999.\n\r\r Ever found a z-index of 999 in CSS and wondered if there really are 998 other elements below it?\n One time I even stumbled upon a z-index of 2147483647\n One time I even stumbled upon a z-index of 2147483647. I thought that number looked familiar, \u0026ldquo;isn\u0026rsquo;t this Int32 max value? 🤔\u0026rdquo; Yes, a google search later my suspicion was confirmed: the highest z-index you can use is the maximum value of a 32 bit integer.\nBut, even if you can use really high numbers for z-index, why should you? The answer is simple: you shouldn\u0026rsquo;t! High numbers are unnecessary and just makes it harder to see what\u0026rsquo;s higher and lower, the difference between 99999 and 148851 can be hard to spot a glance. So, how can we make something better?\nMy refactoring The first thing I did in the code base I was working on, was to collect all the indexes in a constants file. We were using styled components, so my example is in javascript, but of course the same thing can be done in Sass.\n\r\r\rexport const zIndexes = { AlertBox: 10, ModalOpacity: 100, PresentationContainer: 745, HamburgerMenu: 999, MenuOpacity: 999, Arrows: 9999, MenuWrapper: 12500, FullScreenButton: 99999, ProgressBar: 999999 };\r\r Note that I ordered them by ascending index. By putting all z-indexes in the same file, we have a great overview of them. The place I copied the values from, are now referencing a constant instead of having the actual value. I think this is also good from a readability standpoint - you don\u0026rsquo;t have to care what the value is when looking at the styles for a component/class.\nNext step is to take those numbers down to something simpler. If we have a new element that needs a value between the existing ones, we can just re-assign all of them, no biggie.\n\r\r\rexport const zIndexes = { AlertBox: 1, ModalOpacity: 2, PresentationContainer: 3, HamburgerMenu: 4, MenuOpacity: 4, Arrows: 5, MenuWrapper: 6, FullScreenButton: 7, ProgressBar: 8 };\r\r ","dateformatted":"9, June 2019","dateiso":"2019-06-09T20:41:10+02:00","ref":"/refactoring-z-index/","summary":"Ever found a z-index of 999 in CSS and wondered if there really are 998 other elements below it? This is a post about how I have refactored CSS z-indexes to become sane.","tags":["CSS"],"title":"Refactoring z-index"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by David Kovalenko on Unsplash. Importance of error handling - how did we end up here?\n\r\r\r\r\r\rPhoto by David Kovalenko on Unsplash. Importance of error handling - how did we end up here?\n\r\r One thing that has struck me with Javascript\u0026rsquo;s fetch function is that it often looks so simple.\n\r\r\rfetch(\u0026#39;/something.json\u0026#39;) .then(res =\u0026gt; res.json()) .then(json =\u0026gt; { // do something useful here with json... });\r\r When I see the same thing in a pull request, I\u0026rsquo;m not overly impressed. No, just because this works on a sunny day doesn\u0026rsquo;t mean you\u0026rsquo;re done. You need error handling as well! Being explicit about how to handle errors is so much better than giving users an infinite spinner to stare at.\nSince fetch doesn\u0026rsquo;t throw you into the catch clause for non-2xx responses, you need to check the ok property or check status for a specific status code. But both then and catch can use the same function for error handling.\n\r\r\rlet isLoading = true; let hasError = false; let data = {}; function handleFetchResponse(response) { hasError = !response.ok; isLoading = false; return response.ok \u0026amp;\u0026amp; response.json ? response.json() : data; } function fetchData() { return fetch(url) .then(handleFetchResponse) .catch(handleFetchResponse); } fetchData().then(data =\u0026gt; { // do something useful here with data... });\r\r Of course it all depends on your application, but to me this is minimal error handling. To have it be used by the team throughout an application, I\u0026rsquo;ve found it necessary to encapsulate it into a reusable function. I\u0026rsquo;m currently working in a React code base, so this is the custom hook I wrote.\n\r\r\rimport { useEffect, useState } from \u0026#34;react\u0026#34;; /* Example initialUrl: \u0026#34;/_api/jobs\u0026#34; initialData: [] //usually empty array or object */ export const useOurApi = (initialUrl, initialData) =\u0026gt; { const [url, setUrl] = useState(initialUrl); const [isLoading, setIsLoading] = useState(true); const [hasError, setHasError] = useState(false); const [fetchedData, setFetchedData] = useState(initialData); useEffect(() =\u0026gt; { let unmounted = false; const handleFetchResponse = response =\u0026gt; { if (unmounted) return initialData; setHasError(!response.ok); setIsLoading(false); return response.ok \u0026amp;\u0026amp; response.json ? response.json() : initialData; }; const fetchData = () =\u0026gt; { setIsLoading(true); return fetch(url, { credentials: \u0026#39;include\u0026#39; }) .then(handleFetchResponse) .catch(handleFetchResponse); }; if (initialUrl \u0026amp;\u0026amp; !unmounted) fetchData().then(data =\u0026gt; !unmounted \u0026amp;\u0026amp; setFetchedData(data)); return () =\u0026gt; { unmounted = true; }; }, [url]); return { isLoading, hasError, setUrl, data: fetchedData }; };\r\r This way, you get an error indicator and a loading indicator out-of-the-box when using this data fetching function. Used like this in a (simplified) Jobs.jsx.\n\r\r\rimport React from \u0026#34;react\u0026#34;; import { useOurApi } from \u0026#34;../Common/Services/HttpService\u0026#34;; import { Spinner } from \u0026#34;../Common/Components/Spinner\u0026#34;; import { ErrorMessage } from \u0026#34;../Common/Components/ErrorMessage\u0026#34;; import { JobFeed } from \u0026#34;./JobFeed\u0026#34;; export default function Jobs() { const url = `/_api/jobs`; const { data, isLoading, hasError } = useOurApi(url, {}); if (isLoading) return \u0026lt;Spinner /\u0026gt;; if (hasError) return \u0026lt;ErrorMessage message={`Failed to fetch open jobs 😟`} /\u0026gt;; return ( \u0026lt;div className=\u0026#34;our-grid\u0026#34;\u0026gt; \u0026lt;JobFeed jobs={data} /\u0026gt; \u0026lt;/div\u0026gt; ); }\r\r ","dateformatted":"19, March 2019","dateiso":"2019-03-19T21:35:58+01:00","ref":"/error-handling-with-fetch/","summary":"One thing that has struck me with Javascript's fetch function is that it often looks so simple. When I see the same thing in a pull request, I'm not overly impressed. No, just because this works on a sunny day doesn't mean you're done. You need error handling as well!","tags":["JavaScript","ReactJS"],"title":"Error Handling with Fetch (and custom React hook)"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Tomáš Malík on Unsplash. My modern development journey, pausing to reflect on Git.\n\r\r\r\r\r\rPhoto by Tomáš Malík on Unsplash. My modern development journey, pausing to reflect on Git.\n\r\r When I was about to quit my previous job for the job I have now, I mentioned a few things that I anticipated I would have to learn at the new job. One of the most obvious things was Git.\nAt my previous job we used TFVC and even though I had used Git for personal stuff, like this blog (see source), I hadn\u0026rsquo;t learned branching or how to use the Git CLI. If you\u0026rsquo;ve only used Git, TFVC is managed from within Visual Studio with Microsoft\u0026rsquo;s traditional GUI-first approach.\nJudging from the number of articles I had seen about Git, like heated discussions about whether or not you should use rebasing, Gitflow, configuring command line aliases etc, I got the impression that there was quite a bit I had to learn to get productive. Just skim this article named Useful git commands for everyday use!\nMy conclusion so far is that I haven\u0026rsquo;t been forced to learn that much at all about Git. Sure there are advanced possibilities if you need them, but we don\u0026rsquo;t. We use feature branching with pull requests within the team and between teams when we have to make changes to code that another team owns.\nA combination of CLI and GUI works best for me, but I haven\u0026rsquo;t yet needed anything more than what VS Code provides me with in terms of GUI. What I do find useful is a \u0026ldquo;Git aware\u0026rdquo; command line. Since I work in several different repositories on a daily basis and we use feature branching, I like the ability to see which branch I\u0026rsquo;m on and if I\u0026rsquo;m up to date.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r To get this decent console experience on Windows I\u0026rsquo;ve used oh-my-posh.\n","dateformatted":"10, January 2019","dateiso":"2019-01-10T19:52:58+01:00","ref":"/git-wasnt-that-complicated/","summary":"When I was about to quit my previous job for the job I have now, I mentioned a few things that I anticipated I would have to learn at the new job. One of the most obvious things was Git....","tags":[],"title":"Git wasn't that Complicated"},{"content":"\r\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by Jeremy Bishop on Unsplash\n\r\r\r\r\r\rPhoto by Jeremy Bishop on Unsplash\n\r\r This is a reflection from an earlier job I had. It was at one of those large enterprises with monthly deploys and a development process documentation taking up 5 meters on a wall.\nFor 8 years, consultants from one of the largest IT companies, had failed to deliver a company-wide identity solution (authentication/authorisation) that fulfilled the requirements. It was emphasised how important it was that this was done right (opposite to fail fast). This became more and more frustrating, as it was holding back many other initiatives that needed a unified user management solution. So, one day two of our team members had enough and spent a sprint on investigating an alternative solution.\nA solution was presented that would take a few weeks to implement. With a lot of scepticism, we got an approval to go ahead with this alternative solution under the condition that it should only be considered a temporary solution. This was emphasised a lot, a temporary solution.\nIt got me thinking: as opposed to what? 🤨 A permanent solution? 😲 A permanent software solution? 🤣 Seriously?\n There are only temporary software solutions!\n There are only temporary software solutions! Or rather, solutions that will be used until they are replaced with something else. I guess it might make sense to talk about temporary solutions when we have a plan to replace it, with what and by when. In other words, a deliberate technical debt.\nNext time a colleague talks about a temporary solution, I will be better prepared to challenge him or her with what he/she actually mean.\n","dateformatted":"6, September 2018","dateiso":"2018-09-06T20:14:58+02:00","ref":"/there-are-only-temporary-solutions/","summary":"This is a reflection from an earlier job I had. It was at one of those large enterprises with monthly deploys and a development process documentation taking up 5 meters on a wall. For 8 years, consultants from one of the largest IT companies, had failed to deliver a company-wide...","tags":[],"title":"There Are Only Temporary Solutions"},{"content":"Even though there is an app for my Asus RT-AC68U router with the default firmware, it hasn\u0026rsquo;t worked that well for me and I wanted reliable firmware update notifications. I found that this has been done by others, but still, there are a few steps to go through, so I\u0026rsquo;m documenting them here. This works for several other Asus router models as well, see the list of supported routers.\nInstalling Custom Firmware The first step I took was to install the custom firmware Asuswrt-Merlin. To be able to run a custom script on your router, you need a custom firmware. Installing it is just as easy as downloading the binary and uploading it in the regular router web interface at http://192.168.1.1/Advanced_FirmwareUpgrade_Content.asp.\nOne thing about downloading though, you will need to do a manual download and installation for every update later on, so choosing their OneDrive hosted download location and bookmarking the folder for your router model will save you time, compared to using the horrible SourceForge mirrors where you have to click 15 times and wait a while to actually download something.\nAfter installing the Merlin firmware you should see a new logo in the web interface (which otherwise looks pretty much the same).\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rInstall custom router firmware\r\r\r\r\r\r\r\rInstall custom router firmware\r\r\r Enabling New Firmware Version Check The router\u0026rsquo;s firmware can check for updates itself, but that needs to be enabled. Do it under the Tools menu.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rEnable firmware version check\r\r\r\r\r\r\r\rEnable firmware version check\r\r\r Adding Notification Script I found a sample notification script, Update Notification Example (the script I copied), but to be able to put that in the router, we first need to enable SSH. I changed it from No to LAN only.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rEnable router SSH access\r\r\r\r\r\r\r\rEnable router SSH access\r\r\r After enabling SSH I can log in with the same user as in the web interface and add the script. Note that the script needs to have this exact name and path (/jffs/scripts/update-notification). Also note that the script only handles the notification, the router firmware itself will call this script when an update is available.\nssh admin@192.168.1.1\rnano /jffs/scripts/update-notification\rThe script supports e-mail, Pushbullet and Pushover for notifications. I chose Pushover and followed their guide for creating an API token for the router and added that to the script. The Pushover phone app cost me 50 SEK (4.9 EUR / 5.7 USD) as a one time purchase, but it\u0026rsquo;s free to try for a week. I removed the stuff I didn\u0026rsquo;t need from the original script and ended up with this.\n\r\r\r#!/bin/sh # https://github.com/RMerl/asuswrt-merlin/wiki/Update-Notification-Example pushover_token=\u0026#34;aox5eet52qtquc9d42pjkud5fx4rg6 \u0026lt;- just fake\u0026#34; # Your access token here (https://pushover.net/api) pushover_username=\u0026#34;w1yy39m3ysguhyfyrpk54peve8ioc8 \u0026lt;- just fake\u0026#34; # Pushover User ID (the user/group key (not e-mail address often referred to as USER_KEY) # Retrieve version TMPVERS=$(nvram get webs_state_info) VERS=$TMPVERS ROUTER_IP=$(nvram get lan_ipaddr) echo \u0026#34;Version: $VERS\u0026#34; pushover_message () { curl -s \\ --form-string \u0026#34;token=$pushover_token\u0026#34; \\ --form-string \u0026#34;user=$pushover_username\u0026#34; \\ --form-string \u0026#34;message=New firmware version $VERSis now available for your router at $ROUTER_IP.\u0026#34; \\ https://api.pushover.net/1/messages.json } if [ \u0026#34;$pushover_token\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$pushover_username\u0026#34; ]; then echo \u0026#34;Sending notification\u0026#34; pushover_message else echo \u0026#34;Notification settings not configured (not sending anything)\u0026#34; fi \r\r Make the script executable and run it once to see that it works.\nchmod +x /jffs/scripts/update-notification\r/jffs/scripts/update-notification\rexit\rI think updating your router software is just as important as keeping your computer or phone updated, so now i don\u0026rsquo;t have to remember to manually check for updates. I just got the first \u0026ldquo;real\u0026rdquo; update notification and made the manual upgrade before writing this, works great 😎\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rRouter firmware update notification on my phone\r\r\r\r\r\r\r\rRouter firmware update notification on my phone\r\r\r ","dateformatted":"31, July 2018","dateiso":"2018-07-31T11:14:58+02:00","ref":"/firmware-update-notifications-for-my-asus-router/","summary":"Even though there is an app for my Asus RT-AC68U router with the default firmware, it hasn\u0026rsquo;t worked that well for me and I wanted reliable firmware update notifications. I found that this has been done by others, but still, there are a few steps to go through, so I\u0026rsquo;m documenting them here. This works for several other Asus router models as well, see the list of supported routers.\nInstalling Custom Firmware The first step I took was to install the custom firmware Asuswrt-Merlin.","tags":["Networking","Scripting"],"title":"Firmware Update Notifications for My Asus Router"},{"content":"In my early days as a consultant this product from Microsoft called SharePoint became popular and customers starting to ask for people who could work with it. As a junior, I jumped onboard and could soon call myself a SharePoint developer, one of the hottest thing in the IT consultant market at the time (around 2007).\nThis specialisation got me into interesting development projects and I learned a lot. What was initially a hurdle (the horrible API), soon became familiar and an advantage for me compared to colleagues in the business that hadn\u0026rsquo;t got the same exposure to the product. At this time me and SharePoint had a healthy relationship.\nAs time went by, I more and more missed the ability to take advantage of progress that was made in the industry. What was presented at development conferences didn\u0026rsquo;t apply to my work, because I was stuck in this bubble called SharePoint, where special limitations applied.\nThe combination of SharePoint\u0026rsquo;s release cycles of about every third year and the goal to be backwards compatible with the Iron Age, was a bad recipe for developer experience. For the product team, this meant constantly re-inventing their own wheels instead of using what others had already created, always resulting in slightly oval wheels (compare Require.js to SharePoint\u0026rsquo;s SP.SOD).\nOver-promising of Out-of-the-Box Features When adding successful sales efforts from Microsoft (at least here in the Nordic countries) for using SharePoint for public facing web sites and the SharePoint product team\u0026rsquo;s neglection of standards for HTML, CSS and JavaScript, this gap between common best practices and how things had to be done in SharePoint, became even more frustrating. No feature seemed simple enough to prevent a really complicated implementation in SharePoint. Let me rant a bit\u0026hellip;\nThemes I remember attending a course where a Microsoft representative in real earnest claimed that a feature named Themes was a good approach for branding a public facing internet site. You could just upload a branded PowerPoint template and this theme feature would extract the colours and apply them to the site, fantastic 🙄 I tried uploading my current company\u0026rsquo;s PowerPoint template to try it out, and of course it looked terrible. I cannot imagine even the smallest company thinking that using these themes would be good enough for a public web site. And if so, why not just use WordPress?\nVariations SharePoint\u0026rsquo;s way of supporting multi-language web sites is called Variations. The limitations of this feature, made the developers before me in my last SharePoint assignment, implement an entire separate translation flow in addition to what Variations could do, to be able to support the business requirements. Variations also \u0026ldquo;break\u0026rdquo; every now and then, that\u0026rsquo;s why there is something called variationsfixuptool. When you programmatically ask the system to give you all the (language) variations of a page, it gives you as many as it feels like (sometimes correct, sometimes not).\nHTML master pages I mean, seriously? It was sold to our team as a format in which a design agency could deliver design to us. That is, in a super-odd format that no design agency has ever seen before.\nDisplay templates Yet another weird format with large chunks of commented XML in an HTML file, this time for showing search results. Even Microsoft\u0026rsquo;s documentation page uses Notepad++ for editing the files, since using Visual Studio will mess up the files' indentation. As you can imagine, the debugging experience isn\u0026rsquo;t exactly \u0026ldquo;great\u0026rdquo; 😕 Of course the web part used to show the search results didn\u0026rsquo;t work for anonymous users (quite common on a public web site), so we had to do some strange hack to make that work first. One of the best decisions we made in our team was to finally build our own web interface talking directly to the search API.\nManaged navigation Having worked with Managed navigation is another \u0026ldquo;interesting\u0026rdquo; experience, especially trying to apply it on an existing large web site. This is a way of trying to decouple the site structure from the URL and to hide the ugly /pages/default.aspx format. A deep site structure might have been created to achieve a certain URL, but that can cause a database migration to take several days (like when applying a service pack or cumulative update). The Managed navigation API must also be one of the worst I have worked with. Reading a property can result in several database calls in the background, trashing performance completely. The admin GUI that the editors need to use is also\u0026hellip;challenging.\nWorkaround-Driven Development (WDD) All of the above adds up. At one time it struck me that I spent more time on workarounds to compensate for inflexible out-of-the-box features than it would have taken to write the thing from scratch. Having worked in a great team with creative people, we have solved most of these problems by avoiding to make changes to code tightly coupled with SharePoint. But even though we\u0026rsquo;ve built new REST API\u0026rsquo;s and Angular apps for new things, it was still hosted on this ginormous SharePoint installation.\nWhen I saw a new requirement in the \u0026ldquo;Ready 4 sprint planning\u0026rdquo; column in our board that said \u0026ldquo;Platform modernization\u0026rdquo;, I thought: wow finally! It turned out to be a product upgrade work for SharePoint, SQL Server, Windows Server and the like. We had already spent two months trying to assess the implications of such an upgrade, which I had forgot about at that moment. Keeping up to date with product versions is of course a good thing, but I didn\u0026rsquo;t really believe the immediate security risks with running a slightly older version of SQL Server that some corporate security person claimed (as it came across to me). Upgrading SharePoint would provide no benefits for our ability to meet the business needs, yet constituting a big risk of downtime, a tremendous cost and blocking new features from being delivered.\nPoor Developer Experience In addition to the shortcomings mentioned above, I think the biggest drawback and frustration with developing on the SharePoint platform is the overall \u0026ldquo;heaviness\u0026rdquo; or \u0026ldquo;slowness\u0026rdquo;. Being forced to have your custom assemblies in the GAC is just so cumbersome. Unit testing your code when you have to constantly copy your assembly to the GAC for the test runner to notice the change, alternatively remove it from the GAC and have a broken web site while testing, is slow.\nDespite SharePoint being such a success for Microsoft, the tooling for development has been quite bad before SharePoint Framework (SPFx) came along (which doesn\u0026rsquo;t apply here). Seeing Visual Studio (Not responding)™ freezing with a message \u0026ldquo;Communicating with SharePoint\u0026hellip;\u0026quot; as soon as you try to open a file from Solution Explorer, is devastating. Anyone who has experienced both this and Hot Module Replacement can testify what big of a difference it makes to have a short feedback loop when coding.\nEven though SharePoint development has taken a very sane turn in the last couple of years with the birth of the SPFx, that hasn\u0026rsquo;t been reflected by the market need where I work. Dev jobs are still circling around old and heavily customised on-prem installations.\nThe overall \u0026ldquo;heaviness\u0026rdquo; also means you need a multi-kilo laptop with loads of RAM and disk space to run a local SharePoint farm in virtual machines.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rRunning SharePoint on a laptop\r\r\r\r\r\r\rRunning SharePoint on a laptop\r\r\r Brave New World So, I finally decided to get a divorce from SharePoint. It will cost me a significant decrease in salary and all the uncertainties of jumping in to something where I will be a complete novice on the technology used, but I just had to do it. To spice it up a bit, I will also leave the consulting business and take a job at an IT department, with staff liability.\nAfter my summer vacation I will join Academic Work as a team manager for an internal development team. At this point I\u0026rsquo;m feeling a bit scared of all the things I have to learn: React, Elasticsearch, managing staff, Docker, internal politics, DotNet Core, lot of Azure things etc. Heck, even Git isn\u0026rsquo;t something I know properly. But I\u0026rsquo;m still convinced that I will grow both personally and professionally and become a better developer in the next coming years. 🤩\n","dateformatted":"29, June 2018","dateiso":"2018-06-29T23:18:39+02:00","ref":"/getting-a-divorce-from-sharepoint/","summary":"In my early days as a consultant this product from Microsoft called SharePoint became popular and customers starting to ask for people who could work with it. As a junior, I jumped onboard and could soon call myself a SharePoint developer, one of the hottest thing in the IT consultant market at the time (around 2007).\nThis specialisation got me into interesting development projects and I learned a lot. What was initially a hurdle (the horrible API), soon became familiar and an advantage for me compared to colleagues in the business that hadn\u0026rsquo;t got the same exposure to the product.","tags":["SharePoint","Career"],"title":"Getting a Divorce From SharePoint 💔"},{"content":"For quite a while I\u0026rsquo;ve been slightly annoyed by the behaviour that the keyboard shortcut in Visual Studio Code didn\u0026rsquo;t open the integrated terminal if I also had an external terminal open, it was switching to that one instead.\nGoogling the problem didn\u0026rsquo;t help me come closer to an explanation, which I suspected had to do with the language/keyboard specific nature of this issue. As you can see in the image below, the default keyboard shortcut to open the integrated terminal on my machine is Ctrl + ö.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rIntegrated terminal shortcut on my machine\r\r\r\r\r\r\rIntegrated terminal shortcut on my machine\r\r\r Now when I was in the process of setting up oh-my-posh in ConEmu and playing with Quake style, I found an explanation. ConEmu/Cmder uses the same keyboard shortcut for Minimize/Restore as VS Code does for its integrated terminal. Since ConEmu listens for this shortcut even when the program isn\u0026rsquo;t active (in focus), it overrides the shortcut in VS Code.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rConEmu keyboard shortcut settings with conflicting configuration\r\r\r\r\r\r\rConEmu keyboard shortcut settings with conflicting configuration\r\r\r Changing the shortcut in ConEmu solves the problem. One annoyance less 😊\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rConEmu keyboard shortcut settings without conflicting configuration\r\r\r\r\r\r\rConEmu keyboard shortcut settings without conflicting configuration\r\r\r ","dateformatted":"24, June 2018","dateiso":"2018-06-24T05:46:00+01:00","ref":"/integrated-terminal-shortcut-in-vscode-opens-external-terminal/","summary":"For quite a while I\u0026rsquo;ve been slightly annoyed by the behaviour that the keyboard shortcut in Visual Studio Code didn\u0026rsquo;t open the integrated terminal if I also had an external terminal open, it was switching to that one instead.\nGoogling the problem didn\u0026rsquo;t help me come closer to an explanation, which I suspected had to do with the language/keyboard specific nature of this issue. As you can see in the image below, the default keyboard shortcut to open the integrated terminal on my machine is Ctrl + ö.","tags":["ConEmu","Cmder","VS Code"],"title":"Integrated Terminal Shortcut in VS Code Opens External Terminal"},{"content":"The thing I\u0026rsquo;m most satisfied with in a long time is the introduction of a fake backend for my team\u0026rsquo;s frontend development. When you have another system that you depend on and that you have no control over, it\u0026rsquo;s always worth considering if that system can be replaced with a fake version for development.\nThis is especially valuable when this backend system(s) is highly configurable and can be in a lot of different \u0026ldquo;states\u0026rdquo; that you cannot control. The web app we\u0026rsquo;re currently building is a business-to-business web shop and orders need to be validated deep down in SAP. Which customers are allowed to pay by invoice and/or by credit card, their credit card limit, if they are allowed to place an order even when the credit card limit is exceeded, if they are allowed to return products, see order history or download invoices as PDF, delivery information and terms, discount rates, shipping costs etc, are all configurable settings. This can be set per country, customer and user. Some users can place products in the shopping cart, but are not allowed to send the order. I\u0026rsquo;ve found that the test environment for some countries are only available \u0026ldquo;during office hours\u0026rdquo; and I had to ask what time zone they were referring to.\nIf we add the usual technical challenges such as VPN and corporate proxies, localhost seems like an attractive place to host the \u0026ldquo;backend\u0026rdquo; during development.\nThe easiest way I found to create such a fake backend was with Node.js + Express. I chose to serve static json files with some additional logic so that you can add a product to the cart and get the updated active cart when you ask for it. In the code below you can see this behaviour when deleting the cart. But I don\u0026rsquo;t have any persistence here, so restarting the app will always reset it to the same state. This means that I can go directly to the checkout route in my web UI without adding items to the cart (they are already there). Here are the benefits I see with this approach.\n Return all possible HTTP codes (or not respond at all) Live reload when changing response, independent of frontend Control exactly how many milli-seconds it takes to respond (spinners) Reproduce bugs with saved json response Return all possible HTTP codes (or not respond at all) One thing that is otherwise cumbersome to test is how the frontend app behaves for different responses from the REST service. With this setup it\u0026rsquo;s very easy to simulate a 403, 404, 500 or whatever. You can let the fake backend immediately return a 408 (timeout), the fastest possible timeout. You can stop the fake backend to see what the users will see it the backend service doesn\u0026rsquo;t respond at all.\nLive reload when changing response, independent of frontend By letting the fake backend utilise live reload your changes to the fake backend code or any of the statically served json responses will immediately take effect on the next request it gets.\nControl exactly how many milli-seconds it takes to respond (spinners) A general drawback with local development environments are that developers can overlook slowness of their application. Real users will not access your service against localhost and thus have longer response times. By setting exactly how many milli-seconds it should take for your fake backend to respond, you can play around with spinners and loading messages to see that your app doesn\u0026rsquo;t appear dead when a button is clicked.\nReproduce bugs with saved json response If I can reproduce a bug in any of our test environments, I don\u0026rsquo;t have to replicate the same configuration with country, customer and user in my local environment against the test backend. Instead, since we\u0026rsquo;re console logging all responses, see Using Angular HTTP Interceptor for Logging, it\u0026rsquo;s easy to right-click on a response in Chrome Dev Tools, copy it and save it as a json file that I can attach to the bug report. Then I or someone else on the team can dump that file into the fake backend and make the relevant service method respond with that data. We then have a situation where we can reproduce the bug as many times as we need without any external dependencies.\nThe code So, here is the small sample of the fake backend we\u0026rsquo;re using (all but three methods have been removed for brevity).\n\r\r\rlet express = require(\u0026#39;express\u0026#39;); let bodyParser = require(\u0026#39;body-parser\u0026#39;); let fs = require(\u0026#39;fs\u0026#39;); let path = require(\u0026#39;path\u0026#39;); let app = express(); app.use(bodyParser.json()); app.use(bodyParser.urlencoded({ extended: false })); app.use(function (req, res, next) { setTimeout(next, 400) }); app.use(function (req, res, next) { res.header(\u0026#34;Access-Control-Allow-Origin\u0026#34;, \u0026#34;*\u0026#34;); res.header(\u0026#34;Access-Control-Allow-Headers\u0026#34;, \u0026#34;Origin, X-Requested-With, Content-Type, Accept\u0026#34;); next(); }); let activeCartFilePath = path.join(__dirname, \u0026#39;json/active-shopping-cart.json\u0026#39;); let activeCartFile = fs.readFileSync(activeCartFilePath); let activeCart = JSON.parse(activeCartFile); app.get(\u0026#39;/\u0026#39;, function (req, res, next) { res.status(200).send(\u0026#39;Welcome to the Mocked Shopping Service!. Look at app.js to see the available endpoints.\u0026#39;); }); app.get(\u0026#39;/cart\u0026#39;, function (req, res) { res.json(activeCart); }) app.delete(\u0026#39;/cart\u0026#39;, (req, res) =\u0026gt; { let emptyCartPath = path.join(__dirname, \u0026#39;json/empty-cart.json\u0026#39;); let emptyFile = fs.readFileSync(emptyCartPath); let emptyCart = JSON.parse(emptyFile); activeCart = emptyCart; res.sendStatus(200); //res.sendStatus(500); }) app.listen(54321);\r\r As you can see above, I\u0026rsquo;ve set the fake backend to listen on port 54321 and to accept calls from any domain and port number. A parameter passed to the frontend project determines if this fake backend or a real one should be used. app.js is the file above, and below you have package.json where nodemon is used to achieve live reload.\n\r\r\r{ \u0026#34;name\u0026#34;: \u0026#34;shopping-api-mock\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Mocked API to use when developing frontend shopping in Angular\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;nodemon app.js\u0026#34; }, \u0026#34;author\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;body-parser\u0026#34;: \u0026#34;^1.17.2\u0026#34;, \u0026#34;express\u0026#34;: \u0026#34;^4.15.3\u0026#34;, \u0026#34;nodemon\u0026#34;: \u0026#34;^1.11.0\u0026#34; } }\r\r ","dateformatted":"4, March 2018","dateiso":"2018-03-04T18:16:00+01:00","ref":"/create-fake-backend-with-node-express/","summary":"The thing I\u0026rsquo;m most satisfied with in a long time is the introduction of a fake backend for my team\u0026rsquo;s frontend development. When you have another system that you depend on and that you have no control over, it\u0026rsquo;s always worth considering if that system can be replaced with a fake version for development.\nThis is especially valuable when this backend system(s) is highly configurable and can be in a lot of different \u0026ldquo;states\u0026rdquo; that you cannot control.","tags":["NodeJS","JavaScript"],"title":"Create Fake Backend with Node Express"},{"content":"A thing we\u0026rsquo;ve found handy in the Angular application I\u0026rsquo;m currently working on, is the console logging of HTTP requests and responses. This makes it quick to determine if a problem exists in the Angular app or in the REST service we\u0026rsquo;re calling (that we develop alongside the Angular app). Since logging it this way will show exactly which method is being called, the data being sent, headers and everything you need, there is no risk of mistaking one server interaction with another one.\nThe implementation below also includes a fix for GET calls using Internet Explorer, that sometimes seems to do some undesirable caching. By having this somewhat dirty code in an HTTP interceptor, we can at least keep it in one place and not have to think about it for every new HTTP GET call we add.\nBy the way, you need to use Angular 4.3.4+ for this to work.\n\r\r\rimport { Injectable, NgModule } from \u0026#39;@angular/core\u0026#39;; import { Observable } from \u0026#39;rxjs/Observable\u0026#39;; import { HttpHandler, HttpEvent, HttpInterceptor, HttpRequest, HTTP_INTERCEPTORS, HttpResponse } from \u0026#39;@angular/common/http\u0026#39;; @Injectable() export class LoggingHttpInterceptor implements HttpInterceptor { intercept(req: HttpRequest\u0026lt;any\u0026gt;, next: HttpHandler): Observable\u0026lt;HttpEvent\u0026lt;any\u0026gt;\u0026gt; { console.log(req); if (req.method === \u0026#39;GET\u0026#39;) { const time = new Date().getTime().toString(); const dupReq = req.clone( { params: req.params.set(\u0026#39;nocache\u0026#39;, time) }); return next.handle(dupReq).do((httpEvent: HttpEvent\u0026lt;any\u0026gt;) =\u0026gt; this.logResponse(httpEvent)); } return next.handle(req).do((httpEvent: HttpEvent\u0026lt;any\u0026gt;) =\u0026gt; this.logResponse(httpEvent)); } private logResponse(httpEvent: HttpEvent\u0026lt;any\u0026gt;): void { if (httpEvent instanceof HttpResponse) { console.log(httpEvent); } } } @NgModule({ providers: [ { provide: HTTP_INTERCEPTORS, useClass: LoggingHttpInterceptor, multi: true } ] }) export class HttpInterceptorModule { }\r\r ","dateformatted":"9, February 2018","dateiso":"2018-02-09T11:35:22+01:00","ref":"/using-angular-http-interceptor-for-logging/","summary":"A thing we\u0026rsquo;ve found handy in the Angular application I\u0026rsquo;m currently working on, is the console logging of HTTP requests and responses. This makes it quick to determine if a problem exists in the Angular app or in the REST service we\u0026rsquo;re calling (that we develop alongside the Angular app). Since logging it this way will show exactly which method is being called, the data being sent, headers and everything you need, there is no risk of mistaking one server interaction with another one.","tags":["Angular","TypeScript"],"title":"Using Angular HTTP Interceptor for Logging"},{"content":"I\u0026rsquo;m building a new version of a web shop in Angular that should be used on three different web sites for three of my client\u0026rsquo;s subsidiary companies. In the current solution, everything but the CSS is common, even the HTML. That has proven to lack the necessary flexibility when the different subsidiaries have different needs and their design agencies are told that they are not allowed change the mark-up. Every change also has to be approved by all three subsidiaries, which takes time.\nSince we\u0026rsquo;re using the same REST API for all subsidiaries and the front-end logic should generally be the same, we don\u0026rsquo;t want to create three separate Angular projects, since that would be a lot of code duplication. Code sharing with multiple apps in the same project isn\u0026rsquo;t really top of mind in Angular. .angular-cli.json supports the definition of multiple apps as described by stories multiple apps, but the Angular CLI still assumes one app named app, when generating a new component for example. By the way, I\u0026rsquo;m using Angular 5.0.1 at the time of writing this.\n\r\r\r{ \u0026#34;$schema\u0026#34;: \u0026#34;./node_modules/@angular/cli/lib/config/schema.json\u0026#34;, \u0026#34;project\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;shopping\u0026#34; }, \u0026#34;apps\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;subsidiary1-shopping\u0026#34;, \u0026#34;root\u0026#34;: \u0026#34;src\u0026#34;, \u0026#34;outDir\u0026#34;: \u0026#34;../../../dist/subsidiary1-shopping\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;subsidiary1/index.html\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;subsidiary1/main.ts\u0026#34;, ... }, { \u0026#34;name\u0026#34;: \u0026#34;subsidiary2-shopping\u0026#34;, \u0026#34;root\u0026#34;: \u0026#34;src\u0026#34;, \u0026#34;outDir\u0026#34;: \u0026#34;../../../dist/subsidiary2-shopping\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;subsidiary2/index.html\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;subsidiary2/main.ts\u0026#34;, ... } ] {...} }\r\r For services, pipes and plain classes code sharing isn\u0026rsquo;t a problem since they can be defined at the project level and referenced in the respective app modules, but the component logic is still bound to the component and we need to have different components to have different HTML and CSS for the different apps.\nBase components to the rescue By creating base components at the project level that aren\u0026rsquo;t rendered directly and don\u0026rsquo;t have any mark-up or styling, we can create child components in the respective subsidiary apps that extend a base component and defines the template and CSS.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rAngular project tree (simplified).\r\r\r\r\r\r\r\rAngular project tree (simplified).\r\r\r The base component contains all the logic and the corresponding spec file defines all the tests for the component and is therefore also shared code.\n\r\r\r@Component({ selector: \u0026#39;shopping-cart-base\u0026#39;, template: \u0026#39;\u0026#39; }) export class ShoppingCartBaseComponent implements OnInit, OnDestroy { // Shared component logic, hundreds of lines of code ... }\r\r The child component implemented by each subsidiary company (app), has an empty implementation and a single test case that only tests if the component can be created (what you get automatically when creating a component with Angular CLI).\n\r\r\r@Component({ selector: \u0026#39;shopping-cart\u0026#39;, templateUrl: \u0026#39;./shopping-cart.component.html\u0026#39;, styleUrls: [\u0026#39;./shopping-cart.component.scss\u0026#39;], animations: [fadeInAnimation, slideDownAnimation], }) export class ShoppingCartComponent extends ShoppingCartBaseComponent { }\r\r If there is a need to have diverging logic as well, we can easily override the base component\u0026rsquo;s property or method by simply defining it in the child component. Or if we want some additional logic in only one of the apps, that\u0026rsquo;s just as easy by adding it to that app\u0026rsquo;s child component. The best of both worlds!\n","dateformatted":"18, November 2017","dateiso":"2017-11-18T11:51:22+01:00","ref":"/code-sharing-with-multi-app-angular-project-using-base-components/","summary":"I\u0026rsquo;m building a new version of a web shop in Angular that should be used on three different web sites for three of my client\u0026rsquo;s subsidiary companies. In the current solution, everything but the CSS is common, even the HTML. That has proven to lack the necessary flexibility when the different subsidiaries have different needs and their design agencies are told that they are not allowed change the mark-up. Every change also has to be approved by all three subsidiaries, which takes time.","tags":["Angular","TypeScript","Unit testing"],"title":"Code Sharing with Multi-App Angular Project Using Base Components"},{"content":"After a long time of unreliable results with Web Compiler, especially in TFS, I decided to replace it with node-sass.\nWeb Compiler is an extension to Visual Studio that listens to changes in your .scss files (among others) and compiles them. It can also be configured to run as part of your TFS build. With our solution this has however been highly unreliable, where Web Compiler claims that files have been compiled, but the changes you made are not reflected in the resulting bundles. This has caused extra work in our team of about 15 developers, where a bug fix you made is overwritten by some other change and not included in the CSS bundle in the integration environment the next day.\nSince I hadn\u0026rsquo;t got the impression that this was a big issue for other team members in our local development environment, I decided to keep Web Compiler in Visual Studio and use its compilerconfig.json as configuration for which bundles to create. In reality we have quite a few more bundles for different parts of the application, so that was also a reason to keep the existing configuration.\n\r\r\r\r[ { \u0026#34;outputFile\u0026#34;: \u0026#34;Layouts/OurSolution/bundles/main.css\u0026#34;, \u0026#34;inputFile\u0026#34;: \u0026#34;Layouts/OurSolution/CSS/main.scss\u0026#34;, \u0026#34;minify\u0026#34;: { \u0026#34;enabled\u0026#34;: true }, \u0026#34;sourceMap\u0026#34;: true }, { \u0026#34;outputFile\u0026#34;: \u0026#34;Layouts/OurSolution/CSS/bundles/fonts.css\u0026#34;, \u0026#34;inputFile\u0026#34;: \u0026#34;Layouts/OurSolution/CSS/fonts.scss\u0026#34;, \u0026#34;minify\u0026#34;: { \u0026#34;enabled\u0026#34;: true }, \u0026#34;sourceMap\u0026#34;: true }, { \u0026#34;outputFile\u0026#34;: \u0026#34;Layouts/OurSolution/CSS/bundles/admin-pages.css\u0026#34;, \u0026#34;inputFile\u0026#34;: \u0026#34;Layouts/OurSolution/CSS/pages/admin-pages.scss\u0026#34;, \u0026#34;minify\u0026#34;: { \u0026#34;enabled\u0026#34;: true }, \u0026#34;sourceMap\u0026#34;: true }, { \u0026#34;outputFile\u0026#34;: \u0026#34;SPIs/Styles/DesignArtifacts/css/print.css\u0026#34;, \u0026#34;inputFile\u0026#34;: \u0026#34;SPIs/Styles/DesignArtifacts/css/print.scss\u0026#34;, \u0026#34;minify\u0026#34;: { \u0026#34;enabled\u0026#34;: true }, \u0026#34;sourceMap\u0026#34;: true } ]\r\r compilerconfig.json\nThis is more of a quick hack than well-crafted code, but it has turned out to work great. Now I can rely on my changes to reach the integration and test environments after the next nightly deploy.\n\r\r\rlet fs = require(\u0026#39;fs\u0026#39;); let path = require(\u0026#39;path\u0026#39;); let sass = require(\u0026#39;node-sass\u0026#39;); let colors = require(\u0026#39;colors/safe\u0026#39;); colors.setTheme({ silly: \u0026#39;rainbow\u0026#39;, input: \u0026#39;grey\u0026#39;, verbose: \u0026#39;cyan\u0026#39;, prompt: \u0026#39;grey\u0026#39;, info: \u0026#39;green\u0026#39;, data: \u0026#39;grey\u0026#39;, help: \u0026#39;cyan\u0026#39;, warn: \u0026#39;yellow\u0026#39;, debug: \u0026#39;blue\u0026#39;, error: \u0026#39;red\u0026#39; }); let compilerConfigFilePath = path.resolve(__dirname, \u0026#39;compilerconfig.json\u0026#39;); let compilerConfig = require(compilerConfigFilePath); function addMinExtension(fileName) { return fileName.substr(0, fileName.lastIndexOf(\u0026#34;.\u0026#34;)) + \u0026#34;.min.css\u0026#34;; } function parseConfigItem(item) { if (!item || !item.inputFile || !item.outputFile) { let invalidItem = (item \u0026amp;\u0026amp; item.inputFile) || \u0026#39;item\u0026#39;; console.log(colors.warn(\u0026#34;Ignoring invalid \u0026#34; + invalidItem + \u0026#39; in \u0026#39; + compilerConfigFilePath)); return null; } return { \u0026#39;inputFile\u0026#39;: item.inputFile, \u0026#39;outputFile\u0026#39;: item.outputFile, \u0026#39;outputFileMinified\u0026#39;: addMinExtension(item.outputFile), }; } function saveFile(dataToSave, filePath, message, callback) { fs.unlink(filePath, (err) =\u0026gt; { if (err \u0026amp;\u0026amp; err.code !== \u0026#39;ENOENT\u0026#39;) throw err; let options = { flag : \u0026#39;w\u0026#39; }; fs.writeFile(filePath, dataToSave, options, (err) =\u0026gt; { if (err) console.log(colors.error(err)); else { console.log(colors.info(message)); callback \u0026amp;\u0026amp; callback(); } }); }); } function compileFile(file, minified) { if (!file) return; let outputStyle = minified ? \u0026#39;compressed\u0026#39; : \u0026#39;expanded\u0026#39;; let sourceMap = minified; let sourceMapEmbed = !minified; let outputFileName = minified ? file.outputFileMinified : file.outputFile; let result = sass.render({ file: file.inputFile, outputStyle: outputStyle, outFile: outputFileName, sourceMap: sourceMap, sourceMapEmbed: sourceMapEmbed }, (error, result) =\u0026gt; { if (error) { let errorToLog = error.formatted || error; console.log(colors.error(errorToLog)); } else { let outputFilePath = path.resolve(__dirname, outputFileName); let compiledMessage = \u0026#39;Saved compiled file \u0026#39; + outputFileName; saveFile(result.css.toString(), outputFilePath, compiledMessage); if (result.map) { let mapFileName = outputFileName + \u0026#39;.map\u0026#39;; let savedMessage = \u0026#39;Saved map file \u0026#39; + mapFileName; saveFile(result.map.toString(), mapFileName, savedMessage) } }}); } let files = compilerConfig.map(x =\u0026gt; parseConfigItem(x)); files.forEach((file) =\u0026gt; { compileFile(file, false); compileFile(file, true); });\r\r ","dateformatted":"30, October 2017","dateiso":"2017-10-30T05:48:41+01:00","ref":"/replacing-webcompiler-with-node-sass/","summary":"After a long time of unreliable results with Web Compiler, especially in TFS, I decided to replace it with node-sass.\nWeb Compiler is an extension to Visual Studio that listens to changes in your .scss files (among others) and compiles them. It can also be configured to run as part of your TFS build. With our solution this has however been highly unreliable, where Web Compiler claims that files have been compiled, but the changes you made are not reflected in the resulting bundles.","tags":["NodeJS","CSS","JavaScript"],"title":"Replacing Web Compiler With Node Sass"},{"content":"Quite recently I migrated this blog from WordPress to Hugo. Since I didn\u0026rsquo;t want to use a theme built by someone else, I had to add things like CSS and JavaScript myself. To be able to work with this locally in an efficient way and to be able to produce a complete build output in a reproducible manner, I had to automate the build steps. With WordPress I used Gulp for this, but I thought that might not be needed, so I made an attempt to do this using only npm scripts.\nWhat I needed support for, was the following:\n Modern JavaScript to ES 5 compilation Sass to CSS compilation Live reload on all assets Search indexing Bundling and minification These are the scripts I ended up with. The full package.json can be found here.\n\r\r\r\u0026#34;scripts\u0026#34;: { \u0026#34;clean\u0026#34;: \u0026#34;rm -f static/*.js static/*.css\u0026#34;, \u0026#34;clean:public\u0026#34;: \u0026#34;rm -rf public\u0026#34;, \u0026#34;js:build\u0026#34;: \u0026#34;browserify assets/scripts/main.js --debug -o static/site.js -t [ babelify --presets [ es2015 ] ]\u0026#34;, \u0026#34;js:build:prod\u0026#34;: \u0026#34;browserify assets/scripts/main.js -t [ babelify --presets [ es2015 ] ] | uglifyjs -mc \u0026gt; static/site.js\u0026#34;, \u0026#34;js:watch\u0026#34;: \u0026#34;onchange assets/scripts -- npm run js:build\u0026#34;, \u0026#34;sass:watch\u0026#34;: \u0026#34;onchange \\\u0026#34;assets/styles/**/*.sass\\\u0026#34; -- npm run sass:build\u0026#34;, \u0026#34;sass:build\u0026#34;: \u0026#34;node-sass assets/styles/main.sass static/site.css \u0026amp;\u0026amp; node-sass assets/styles/print.sass static/print.css\u0026#34;, \u0026#34;sass:build:prod\u0026#34;: \u0026#34;node-sass assets/styles/main.sass static/site.css --output-style compressed \u0026amp;\u0026amp; node-sass assets/styles/print.sass static/print.css --output-style compressed\u0026#34;, \u0026#34;build:assets\u0026#34;: \u0026#34;npm run clean \u0026amp;\u0026amp; npm run sass:build:prod \u0026amp;\u0026amp; npm run js:build:prod\u0026#34;, \u0026#34;hugo:watch\u0026#34;: \u0026#34;hugo serve --config config-dev.toml --bind=0.0.0.0\u0026#34;, \u0026#34;serve\u0026#34;: \u0026#34;concurrently --kill-others \\\u0026#34;npm run js:watch\\\u0026#34; \\\u0026#34;npm run sass:watch\\\u0026#34; \\\u0026#34;npm run hugo:watch\\\u0026#34;\u0026#34;, \u0026#34;index:prod\u0026#34;: \u0026#34;node build/index-search.js public/search-index.json\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;node build/index-search.js static/search-index.json\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;node build/save-build-timestamp.js\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;npm run clean \u0026amp;\u0026amp; npm run sass:build \u0026amp;\u0026amp; npm run js:build \u0026amp;\u0026amp; hugo \u0026amp;\u0026amp; npm run serve\u0026#34;, \u0026#34;build\u0026#34;: \u0026#34;npm run build:assets \u0026amp;\u0026amp; npm run clean:public \u0026amp;\u0026amp; hugo \u0026amp;\u0026amp; npm run index:prod \u0026amp;\u0026amp; npm run timestamp\u0026#34; }\r\r Structure of scripts To be able to control the order of execution and to avoid having one horribly long line, the tasks are broken down in smaller pieces and then composed accordingly. To run the blog on my local machine I run npm run start (yes, the run part can be skipped in this case). This simply runs five commands in order, among which four of them are other npm scripts: clean, sass:build, js:build and serve.\nThe other main script is build, which is run by my build and hosting service Netlify. More on this later.\nCleaning To have a clean start when doing sequential builds, I have two clean scripts. The reason for this is explained below, but the output folder is named public in this setup, so that\u0026rsquo;s what needs to get cleaned.\nBuilding JavaScript To produce a JavaScript bundle that can run in web browsers, I use browserify and babelify to be able to use newer JavaScript syntax. This is done in script js:build.\nBuilding CSS Compiling Sass to CSS is straight forward and done with node-sass. The script sass:build specifies both output bundles, site.css and print.css.\nThe Hugo stuff Hugo produces HTML from my content files in Markdown with the associated page templates. Since Hugo is the most performant part of this setup and includes a great web server, I use that to run the site locally. Looking at the script hugo:watch you can see that I also have a different config file for Hugo when running locally. This is to disable Google Analytics and Disqus, which is pointless when running locally.\nSearch indexing The technique I use to provide search might be worthy a separate post, but in essence it\u0026rsquo;s a separate Hugo template that outputs all blog posts into one file, data-to-index.json. That file is then indexed using Lunr through a script I wrote myself, index-search.js. To be able to have search locally, I have a separate clean script that does not remove the entire public folder, since that would get me into a catch 22 situation. So to work with the \u0026ldquo;front-end\u0026rdquo; part of search, I run hugo \u0026amp;\u0026amp; npm run index \u0026amp;\u0026amp; npm run start. That way the index is produced once and I can still have live reload on everything else. The timestamp script simply adds a timestamp to the index file name to avoid caching issues.\nLive reload One of the best things in recent years for web development in my opinion, is live reload. To be able to save a file and very quickly see the changes reflected without any additional action, makes it so much easier to stay focused on what you\u0026rsquo;re doing. Here I use onchange to listen to changes in the source files. The script js:watch watches the js files and simply calls the js:build script when a change is detected.\nProducing a complete build to deploy The only thing special to the scripts with a :prod suffix, is that they add minification to the bundles. By running npm run build I get a public folder ready to be deployed.\n","dateformatted":"17, September 2017","dateiso":"2017-09-17T17:15:54+02:00","ref":"/build-steps-using-npm-scripts-for-my-hugo-blog/","summary":"Quite recently I migrated this blog from WordPress to Hugo. Since I didn\u0026rsquo;t want to use a theme built by someone else, I had to add things like CSS and JavaScript myself. To be able to work with this locally in an efficient way and to be able to produce a complete build output in a reproducible manner, I had to automate the build steps. With WordPress I used Gulp for this, but I thought that might not be needed, so I made an attempt to do this using only npm scripts.","tags":["Hugo","NodeJS","JavaScript","CSS"],"title":"Build Steps Using NPM Scripts for My Hugo Blog"},{"content":"","dateformatted":"5, September 2017","dateiso":"2017-09-05T07:43:59+02:00","ref":"/kortkommandon-windows7/","summary":"","tags":null,"title":"Kortkommandon Windows 7"},{"content":"A week before my annual subscription of CrashPlan would expire I got an e-mail informing me that the CrashPlan for Home service was discontinued. My subscription was extended by 60 days to give me enough time to find another service. I\u0026rsquo;ve used it for a couple of years and have been quite happy with it, except maybe for the micro stuttering I experienced. It had the possibility to back up to a local external drive as well as online (offsite) with plenty of configurable options.\nNo compelling alternatives Obviously CrashPlan found that the consumer market isn\u0026rsquo;t an attractive business and having looked at the competing services, this seems to be a general truth. The generous storage offerings from Amazon, Google and Microsoft seems to be difficult to compete with. It turns out that most (well, all I have found) true online backup services have some of these drawbacks:\n File size upload limitations Limited file versioning Bandwidth throttling Refusing to back up some file extensions Only allows backing up one drive Prehistoric GUI Enterprise pricing Dishonest marketing If you want to go for any online backup service, you really need to read through the fine print to find the limitations. I found Cloudwards' Best Cloud Backup Services 2017 to be a good comparison of the different backup offerings available.\nWhat are my real backup needs? The machine I\u0026rsquo;m looking for a backup solution for, is a traditional computer standing on the floor under my desk with a bunch of drives in it. Rethinking my needs, I realised that what I actually need are two things, a backup of most of my files to a local backup so I can restore fairly quick (1) and an archival solution for self-created content that is irreplaceable (2). The latter mostly consists of raw files from my camera.\nChosen solution Since I already use OneDrive, I chose to extend that storage and move everything important into it. Unfortunately OneDrive doesn\u0026rsquo;t allow files to be stored on different drives, so I uploaded my photos from previous years through the web interface and set them not to sync locally. In addition to this I enabled File History in Windows and made sure all important files were included (many more than I have in OneDrive). This way I can restore any file from my external drive quickly, and in case something really bad happens I will have to download them from OneDrive. This is good enough for me the and still adheres to the 3-2-1 principal. Everything I have in OneDrive I also have on my computer and on my external backup drive. Anything not in OneDrive I can live without, it will just be a bit time-consuming to reinstall and redownload some stuff.\nUpdate 2017-08-27: Having uploaded an additional 278 GB to OneDrive through the web interface I have to say it worked great, pretty much maxing out my 100 MBit/second upload connection. However, two files didn\u0026rsquo;t upload and the tab I used to upload completely hang in Google Chrome, occupying 2.5 GB of RAM and 100% of one CPU core. Uploading those two files again (which were listed as errors) worked without problem.\n","dateformatted":"26, August 2017","dateiso":"2017-08-26T14:53:58+02:00","ref":"/switching-from-crashplan-and-other-backup-services/","summary":"A week before my annual subscription of CrashPlan would expire I got an e-mail informing me that the CrashPlan for Home service was discontinued. My subscription was extended by 60 days to give me enough time to find another service. I\u0026rsquo;ve used it for a couple of years and have been quite happy with it, except maybe for the micro stuttering I experienced. It had the possibility to back up to a local external drive as well as online (offsite) with plenty of configurable options.","tags":["Backup","CrashPlan","OneDrive"],"title":"Switching from CrashPlan and other Backup Services"},{"content":"I have recently migrated my blog from WordPress to Hugo. That is, switching from a database-based web content management system with loads of themes, plugins and a large user base to a statically generated site with no server-side logic and a small feature set where I must build most things myself.\nThe switch was by no means necessary, I had cheap hosting at a web hotel I will still use for other sites after the migration, speed was good with WP Super Cache and so on. But there are still some great benefits I see now about a month after the switch.\nBenefits Simpler development environment. I no longer need to have WordPress installed on my local machines and sync content between environments. Since I have my own desktop PC, a laptop from my employer and a laptop from my customer that I switch between depending on where I am, it\u0026rsquo;s a hassle to have to install the WAMP stack on them and have the content databases somewhat synced. Docker or traditional virtual machines makes this a bit easier, but it\u0026rsquo;s still something don\u0026rsquo;t have to deal with anymore. Hugo is just a single binary. If you want your own theme and adjustments you will of course need tools for that, but that\u0026rsquo;s equal between WordPress and Hugo in my case.\n Hugo is just a single binary\n Content are just Markdown files. A related benefit to not having a database is that my content is now just a bunch of Markdown files checked into a Git repository. I expect migrating from Hugo to another tool in the future would not be too heavy work.\nMore \u0026ldquo;professional\u0026rdquo; deployment pipeline. With a static site generator that doesn\u0026rsquo;t have special tooling for content editing, there is no difference between changing a CSS rule and adding a new blog post. For any change you do, you have to rebuild your entire site. This makes an automated deployment pipeline more important since you need it not only to deploy new code, but also to add new content. This might be a problem if you\u0026rsquo;re not used to these tools, but as a developer I only experience the increased control I get. Most traditional and cheap hosting services (that hosts WordPress) I found does not provide tools for automated deployments.\n The full build, including Hugo page creation, Sass compilation, Javascript transpilation and creating a complete search index takes about 14 seconds\n I now use Netlify for hosting, which also offers building and deploying my site, all for free. Hugo creates all my pages, currently 255 in total, in 487 ms. The full build, including Hugo page creation, Sass compilation, Javascript transpilation and creating a complete search index takes about 14 seconds. Everything except Hugo itself is using Node. Of course, not all of this have to be done when working with the site locally and using live reload, but this is what Netlify does for the build part. So my deploy flow is as follows.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r Utilising CDN and SSL is also a benefit of using any of the services that will host your static content. With my old hosting company I would either have to pay a premium fee to get SSL with my custom domain name, or do a lot of manual work on a regular basis with certs from Let\u0026rsquo;s Encrypt. With Netlify it\u0026rsquo;s just a checkbox. If you have visitors from different parts of the world, a CDN is gold and with static content you can have your entire site served from a CDN.\nChallenges Search is not as easy without server-side logic. With WordPress I had an out-of-the-box search feature that was completely sufficient for a simple blog, but with a static site you have nothing built in. There are however a few alternatives for providing search with Hugo. The Hugo community forum as an example, is using the hosted service Algolia. I chose to use Lunr, which basically means that I\u0026rsquo;m creating a search index as part of my build process and that the entire index is downloaded by the web browser and search is performed locally in the browser. This works fine for a small blog, but for a larger site it will probably work better with a hosted service. My index is currently 493 KB (62.9 KB compressed download), but it only gets downloaded once you click Search in the menu.\nComments also requires work. Same as with search, comments is also something that WordPress has built-in that you will have to solve yourself. I added Disqus since it\u0026rsquo;s easy to get going with, easy to migrate old comments into and widely spread. Seeing how the number of web requests increased after adding Disqus was however not as pleasant. But in practice I think all of those requests is more of a privacy issue than a performance issue.\nSummary In summary I find this setup more enjoyable and more appropriate for my needs with its vastly reduced technical complexity. No more updates to WordPress or its plugins. For more alternatives to how you can build your personal site, check out the comments to the post What tool/framework/cms/etc do you use to build your own personal website? I will probably follow up this post with a more in-depth description of the build process I use (what I had to write myself), but if you\u0026rsquo;re familiar with Go, How I Build My Static Assets for Hugo, might be interesting.\n","dateformatted":"7, August 2017","dateiso":"2017-08-07T21:25:57+02:00","ref":"/switching-from-wordpress-to-hugo/","summary":"I have recently migrated my blog from WordPress to Hugo. That is, switching from a database-based web content management system with loads of themes, plugins and a large user base to a statically generated site with no server-side logic and a small feature set where I must build most things myself.\nThe switch was by no means necessary, I had cheap hosting at a web hotel I will still use for other sites after the migration, speed was good with WP Super Cache and so on.","tags":["Hugo","WordPress"],"title":"Switching from WordPress to Hugo"},{"content":"As I have worked more and more with CSS during the last year, both at work and with an updated version of this blog, I have come to the following conclusions regarding the Sass vs SCSS syntax.\nSCSS is the obvious default choice as it\u0026rsquo;s a more natural extension of CSS and that you can simply rename an existing .css file. In a team with several developers focused more on server-side, it\u0026rsquo;s usually easier to explain SCSS than Sass syntax.\nWhen starting from scratch however, I have personally found Sass to be more enjoyable. It\u0026rsquo;s easier to move rules around when you don\u0026rsquo;t have to add and remove curly brackets ({) and I find it a bit more readable. That I\u0026rsquo;m using a Swedish keyboard where any bracket (curly or square) is a bit fiddly to type, might also contribute to this feeling that the indented style is faster.\nThe Sassy Way\u0026rsquo;s old post Sass vs. SCSS: which syntax is better? has a good description about this rather irrelevant topic 🙂.\nThis is the link.sass file I currently have for general link styling on this blog (as an example).\n\r\r\ra color: $light-blue text-decoration: none transition: color, text-shadow 200ms, outline 60ms outline: none display: inline-block \u0026amp;:after display: block content: \u0026#34;\u0026#34; height: 2px width: 0% background-color: $orange transition: width 200ms ease \u0026amp;:visited color: $medium-blue \u0026amp;:focus, \u0026amp;:active, \u0026amp;:hover color: $dark-blue text-shadow: 0 0 1px transparentize($light-blue, 0.2) text-decoration: none outline: none \u0026amp;:after width: 100%\r\r ","dateformatted":"12, July 2017","dateiso":"2017-07-12T11:03:44Z","ref":"/sass-vs-scss/","summary":"As I have worked more and more with CSS during the last year, both at work and with an updated version of this blog, I have come to the following conclusions regarding the Sass vs SCSS syntax.\nSCSS is the obvious default choice as it\u0026rsquo;s a more natural extension of CSS and that you can simply rename an existing .css file. In a team with several developers focused more on server-side, it\u0026rsquo;s usually easier to explain SCSS than Sass syntax.","tags":["CSS"],"title":"Sass vs SCSS"},{"content":"I recently started to look at migrating this blog from WordPress to a static site engine. Hugo got my attention and I decided to try it out. One thing I have in WordPress is a tag cloud. I couldn\u0026rsquo;t find an example of how to create one with varying font sizes in Hugo, so I tried creating my own. Hugo discussion forum topics Weighted tag cloud and Tag Cloud talk about this.\nHugo is written in Go and thus uses Go templates. Go is a language I haven\u0026rsquo;t written a single line of code in, but I looked at how the tag cloud is implemented in WordPress and tried to steal the logic, see the function wp_generate_tag_cloud. Here is what I came up with:\nUpdate 2017-07-03: Code is updated to solve an issue that would cause a crash when all posts have the same number of tags (causing a division by 0). Thanks to @MunifTanjim in the Hugo discussion thread Weighted tag cloud.\nUpdate 2017-07-27: Artem Sidorenko posted an improved version of this code where he uses logarithmic distribution for calculation of tag sizes. It doesn\u0026rsquo;t make a big difference with my current content, but clearly does on his, so maybe on yours too.\n\r\r\r1{{- if gt (len .Site.Taxonomies.tags) 0 -}} 2 {{- $fontUnit := \u0026#34;rem\u0026#34; -}} 3 {{- $largestFontSize := 1.8 -}} 4 {{- $smallestFontSize := 1.0 -}} 5 {{- $fontSizeSpread := sub $largestFontSize $smallestFontSize -}} 6 \u0026lt;!--\u0026lt;div\u0026gt;Font size unit: {{ $fontUnit }}\u0026lt;/div\u0026gt; 7 \u0026lt;div\u0026gt;Font min size: {{ $smallestFontSize }}\u0026lt;/div\u0026gt; 8 \u0026lt;div\u0026gt;Font max size: {{ $largestFontSize }}\u0026lt;/div\u0026gt; 9 \u0026lt;div\u0026gt;Font size spread: {{ $fontSizeSpread }}\u0026lt;/div\u0026gt;--\u0026gt; 10 11 {{- $maxCount := 1 -}} 12 \u0026lt;!--\u0026lt;div\u0026gt;Max tag count: {{ $maxCount }}\u0026lt;/div\u0026gt;--\u0026gt; 13 14 {{- $minCount := 1 -}} 15 \u0026lt;!--\u0026lt;div\u0026gt;Min tag count: {{ $minCount }}\u0026lt;/div\u0026gt;--\u0026gt; 16 17 {{- $countSpread := sub $maxCount $minCount -}} 18 \u0026lt;!--\u0026lt;div\u0026gt;Tag count spread: {{ $countSpread }}\u0026lt;/div\u0026gt;--\u0026gt; 19 20 {{- $.Scratch.Set \u0026#34;sizeStep\u0026#34; 0 -}} 21 {{- if gt $countSpread 0 -}} 22 {{- $.Scratch.Set \u0026#34;sizeStep\u0026#34; ( div $fontSizeSpread $countSpread ) -}} 23 {{- end -}} 24 {{- $sizeStep := ( $.Scratch.Get \u0026#34;sizeStep\u0026#34; ) -}} 25 \u0026lt;!--\u0026lt;div\u0026gt;Font step: {{ $sizeStep }}\u0026lt;/div\u0026gt;--\u0026gt; 26 27 \u0026lt;div class=\u0026#34;widget\u0026#34;\u0026gt; 28 \u0026lt;div class=\u0026#34;widget-title\u0026#34;\u0026gt;Tags\u0026lt;/div\u0026gt; 29 \u0026lt;div class=\u0026#34;tag-cloud-tags widget-content\u0026#34;\u0026gt; 30 {{- range $name, $taxonomy := $.Site.Taxonomies.tags -}} 31 {{- $currentTagCount := len $taxonomy.Pages -}} 32 {{- $currentFontSize := (add $smallestFontSize (mul (sub $currentTagCount $minCount) $sizeStep) ) -}} 33 \u0026lt;!--Current font size: {{$currentFontSize}}--\u0026gt; 34 \u0026lt;a href=\u0026#34;{{ \u0026#34;/tags/\u0026#34; | relLangURL }}{{ $name | urlize }}\u0026#34; aria-label=\u0026#34;{{ $name }} ({{$currentTagCount}} posts)\u0026#34; style=\u0026#34;font-size:{{$currentFontSize}}{{$fontUnit}}\u0026#34;\u0026gt;{{- $name -}}\u0026lt;/a\u0026gt; 35 {{- end -}} 36 \u0026lt;/div\u0026gt; 37 \u0026lt;/div\u0026gt; 38{{- end -}}\r\r You can uncomment the commented lines for debugging. Also, if the tag keys instead of the tag names are rendered, set preserveTaxonomyNames = true in your config.toml or preserveTaxonomyNames: true in your config.yaml file. This took a while for me as a beginner to figure out.\n","dateformatted":"3, June 2017","dateiso":"2017-06-03T06:50:51Z","ref":"/hugo-tag-could/","summary":"I recently started to look at migrating this blog from WordPress to a static site engine. Hugo got my attention and I decided to try it out. One thing I have in WordPress is a tag cloud. I couldn\u0026rsquo;t find an example of how to create one with varying font sizes in Hugo, so I tried creating my own. Hugo discussion forum topics Weighted tag cloud and Tag Cloud talk about this.","tags":["Hugo","Tag cloud"],"title":"Hugo Tag Could"},{"content":"For some time I have tried to figure out what\u0026rsquo;s been causing the micro stuttering I\u0026rsquo;ve experienced on my Windows 10 machine. Under moderate load applications have stopped responding for short periods of time, most obvious is audio playback where an unpleasant sound has interrupted playback. I could not see any correlation between this behaviour and a specific application, high CPU, disk or memory utilisation.\nMy machine is a desktop computer I have built myself from parts, so I thought I might have made some bad decision regarding hardware compatibility. It clearly wasn\u0026rsquo;t a pure performance issue since it happened even during YouTube playback and the machine has NVMe storage, 64 GB of RAM, a 4 core CPU etc. I started to update all drivers, motherboard firmware etc, but without result.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rI had the problem even with less load, but it was easier to provoke the reaction with a higher load\r\r\r\r\r\r\r\rI had the problem even with less load, but it was easier to provoke the reaction with a higher load\r\r\r Then I noticed an interesting behaviour. There were spikes in disk activity even on drives that shouldn\u0026rsquo;t be doing anything at the moment. My Disk 5 (E:) is a USB connected drive that I wasn\u0026rsquo;t reading or writing to at the moment. Sure, it could be anti-virus scanning or similar, but I noticed that the process for my backup software CrashPlan was fairly active. So I stopped that program, and voilà – problem gone!\nIt turned out that I had CrashPlan set to run backup continuously, Always. Changing it to Between specified times solved it for me. An interesting observation is that CrashPlan isn\u0026rsquo;t configured to use Disk 5 (E:) in any way, but that\u0026rsquo;s still what caused the spikes in disk usage.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r Changing other settings, such as User is away when not active for [15 minutes] and When user is present, use up to [0 percent CPU] did not help either. So finally my machine is running as smooth as it should and I will hopefully remember this setting if I have to reinstall the machine at some point, and so do you, if you happen to run in to this very specific problem.\n","dateformatted":"20, March 2017","dateiso":"2017-03-20T06:50:51Z","ref":"/micro-stuttering-caused-by-crashplan/","summary":"For some time I have tried to figure out what\u0026rsquo;s been causing the micro stuttering I\u0026rsquo;ve experienced on my Windows 10 machine. Under moderate load applications have stopped responding for short periods of time, most obvious is audio playback where an unpleasant sound has interrupted playback. I could not see any correlation between this behaviour and a specific application, high CPU, disk or memory utilisation.\nMy machine is a desktop computer I have built myself from parts, so I thought I might have made some bad decision regarding hardware compatibility.","tags":["Backup","CrashPlan"],"title":"Micro Stuttering Caused by CrashPlan"},{"content":"Lately I\u0026rsquo;ve been working on switching to Azure Media Services from another video platform on my customer\u0026rsquo;s web site. I\u0026rsquo;ve found some challenges related to sizing of the player in different browsers with different playback methods (HTML5, Flash and Silverlight). Particularly the size of the player when exiting full screen mode has been flaky. I can\u0026rsquo;t say for sure that this isn\u0026rsquo;t the fault of the web site it lives on, but I don\u0026rsquo;t see anything indicating that either.\nThe problem is that the player keeps the height of the entire screen after exiting full screen mode. Video height is by the way also handled \u0026ldquo;manually\u0026rdquo; (setting a known video height with JavaScript) when resizing the browser window at Microsoft\u0026rsquo;s demo site at the time of writing this. I\u0026rsquo;m currently using version 1.8.1 of the player by the way.\nSolving the issue There is great documentation of the player at https://amp.azure.net/libs/amp/latest/docs/ where I found the exitFullscreen event. This is where I can manually set the correct height, which I get from the player itself once it\u0026rsquo;s loaded, at the loadedmetadata event. We have an Angular v1 controller that does a few more things, but these are the relevant parts related to the issue.\nPart of Azure Media Player Angular v1 Controller \r\r\r1var playerOptions = { 2 \u0026#34;techOrder\u0026#34;: [\u0026#34;azureHtml5JS\u0026#34;, \u0026#34;html5\u0026#34;, \u0026#34;flashSS\u0026#34;, \u0026#34;silverlightSS\u0026#34;], 3 \u0026#34;logo\u0026#34;: { enabled: false }, 4 \u0026#34;autoplay\u0026#34;: $attrs.autoplay === \u0026#34;true\u0026#34;, 5 \u0026#34;controls\u0026#34;: !($attrs.autoplay === \u0026#34;true\u0026#34; \u0026amp;\u0026amp; $attrs.showcontrols === \u0026#34;false\u0026#34;), 6 \u0026#34;width\u0026#34;: \u0026#34;100%\u0026#34;, 7 \u0026#34;height\u0026#34;: \u0026#34;auto\u0026#34;, 8 \u0026#34;poster\u0026#34;: assets.ThumbnailUrl 9} 10var videoElementIdSelector = \u0026#34;#\u0026#34; + $attrs.videocontainerid; 11var myPlayer = amp(videoElementIdSelector, playerOptions); 12 13var subtitles = []; 14var assetSubtitles = assets.Subtitles || []; 15assetSubtitles.forEach(function (subtitle, i) { 16 subtitles.push({ 17 kind: \u0026#34;subtitles\u0026#34;, src: getSrc(subtitle.Src), 18\tsrclang: subtitle.SrcLang, label: subtitle.Title 19 }); 20}); 21 22myPlayer.src({ 23 src: assets.Url, 24 type: \u0026#34;application/vnd.ms-sstr+xml\u0026#34; 25}, subtitles); 26 27// AMP bug fix start 28// Sometimes the video container doesn\u0026#39;t get resized after existing fullscreen mode. 29$(myPlayer).bind(\u0026#34;loadedmetadata\u0026#34;, function () { 30 $scope.correctVideoHeight = $(videoElementIdSelector).height(); 31}); 32 33$(myPlayer).bind(\u0026#34;fullscreenchange\u0026#34;, function () { 34 if (!myPlayer.isFullScreen()) { 35 var videoContainer = $(videoElementIdSelector); 36 if (videoContainer.height() !== $scope.correctVideoHeight) { 37 videoContainer.height($scope.correctVideoHeight); 38 } 39 } 40}); 41// AMP bug fix end \r\r ","dateformatted":"12, February 2017","dateiso":"2017-02-12T21:46:47Z","ref":"/azure-media-player-full-screen-resizing-fix/","summary":"Lately I\u0026rsquo;ve been working on switching to Azure Media Services from another video platform on my customer\u0026rsquo;s web site. I\u0026rsquo;ve found some challenges related to sizing of the player in different browsers with different playback methods (HTML5, Flash and Silverlight). Particularly the size of the player when exiting full screen mode has been flaky. I can\u0026rsquo;t say for sure that this isn\u0026rsquo;t the fault of the web site it lives on, but I don\u0026rsquo;t see anything indicating that either.","tags":["Azure Media Player","JavaScript"],"title":"Azure Media Player Full Screen Resizing Fix"},{"content":"During the last couple of years I have noticed more and more developers switching to Mac, especially among those in the SharePoint field that traditionally have been very loyal to Microsoft. I see a correlation between this and the trend away from Visual Studio bound development. After playing around with NodeJS for awhile I understand why and hope for a change. Hopefully I can save you some googling with the links in this post if you run into the same issues as I have.\nAfter having watched many of the great clips by DevTips on YouTube and having done more front-end work professionally, I wanted to set up a simple lab environment on my local machine with Sass and BrowserSync. I mean, how hard can it be?\nPlatform independent NodeJS I started by trying out a bunch of Yeoman generators that included what I needed. Most of them installed fine, but none of them fully worked after installation on my Windows 10 machine. When running the build task (usually with Gulp), I got different errors in different dependant Node modules and were stuck.\nI started to think about why \u0026ldquo;everyone\u0026rdquo; how maintains these projects are using Macs. Could it be because Node is \u0026ldquo;platform independent\u0026rdquo; to the extent that it works on every Mac? (sorry for the irony) But Windows 10 has a Linux sub-system that I have already activated\u0026hellip;maybe I will have more success using that? I came across Stefan Bauer\u0026rsquo;s great post A bash on Windows and the new SharePoint Framework. which explains the problems with NodeJS on Windows quite well. I had noticed the problems related to node-gyp already, but he explains it well.\nTrying Bash on Ubuntu on Windows When I tried installing Node on Ubuntu on Windows I immediately ran into some Ubuntu weirdness related to Node and NPM, which is described here: Yeoman: Getting it to Work on Ubuntu. Having worked through that, my Sass compilation failed due to a dependency on the network which is not implemented in the Linux sub-system on Windows. See issue os.networkInterfaces error in NodeJS on GitHub. There are a few suggested workarounds for this as well, but at this time it\u0026rsquo;s starting to feel absurd. Should I really be doing workarounds related to network interfaces in Linux to be able to do not-so-advanced front-end work on Windows? Maybe I should just stick with Codepen?\nOn the positive side, it seems that the issue with Ubuntu on Windows, which to be fair is still in beta, is resolved and available in Windows Insider build 14965 according to this User Voice suggestion (Enable network connection enumeration). I\u0026rsquo;ll guess I just have to wait for that and hope I can avoid these rabbit trails in the future.\nUpdate: I found a Yeoman generator named Yeogurt that worked fine with the latest Node and NPM versions on Windows 10.\n","dateformatted":"7, January 2017","dateiso":"2017-01-07T11:22:55Z","ref":"/frustration-with-nodejs-on-windows/","summary":"During the last couple of years I have noticed more and more developers switching to Mac, especially among those in the SharePoint field that traditionally have been very loyal to Microsoft. I see a correlation between this and the trend away from Visual Studio bound development. After playing around with NodeJS for awhile I understand why and hope for a change. Hopefully I can save you some googling with the links in this post if you run into the same issues as I have.","tags":["Windows Subsystem for Linux","NodeJS","Windows 10"],"title":"Frustration with NodeJS on Windows"},{"content":"Having run my home office monitoring service for more than a month, I can now reflect back on the project. Even though I have an MSDN subscription trough work with a bunch of Azure credit included to spend every month, I decided to use a Pay-As-You-Go subscription for this project. I wanted to be sure that the services I used was available to me even if I would loose the MSDN subscription.\nCost The monthly bill from Microsoft was 851.98 SEK (90.11 EUR or 100.60 USD). 64% of it was spent on running the web site (Pricing Tier: Basic: 1 Small), 26% for Stream Analytics and 10% for Event Hub. The forecast for this month is 541.61 SEK (57.28 EUR or 63.96 USD), so probably the usage is lower now than when I was setting it up and posted the blog posts about it. This is still a bit much for a completely useless service built for learning purposes, but at least I know how to set it up now. Unfortunately there is no easy way to transfer the services between different Active Directory tenants, see Move resources to new resource group or subscription, so I would need to set it up from scratch if I were to \u0026ldquo;move\u0026rdquo; the services to my MSDN subscription.\nArchitecture Looking at the architecture from the perspective of what this solution does, it\u0026rsquo;s easy to claim that it\u0026rsquo;s over-engineered. I think this is a good question to ask in any project: Could this be simplified?\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r For simply showing sensor data on a web page, I could have run a web server (preferably in Python) on the Raspberry Pi itself. Applying as many design patterns and new technologies as possible isn\u0026rsquo;t necessary helping my clients at work. Having that said, this was a hobby project, so it\u0026rsquo;s fine to apply some Learning-Driven Development.\n","dateformatted":"15, December 2016","dateiso":"2016-12-15T14:58:59Z","ref":"/my-iot-exploration-part-5-costs-and-architecture-refection/","summary":"Having run my home office monitoring service for more than a month, I can now reflect back on the project. Even though I have an MSDN subscription trough work with a bunch of Azure credit included to spend every month, I decided to use a Pay-As-You-Go subscription for this project. I wanted to be sure that the services I used was available to me even if I would loose the MSDN subscription.","tags":["Azure IoT Hub","Internet of Things","Raspberry Pi"],"title":"My IoT Exploration – Part 5 – Costs and Architecture Refection"},{"content":"If you have a Visual Studio project that uses Chutzpah for JavaScript tests, things recently got a lot easier with a long-awaited update.\nProblem When all tests pass with the Chutzpah test runner everything is fine, but when you need to debug a test, things haven\u0026rsquo;t been as easy. Debugging the JS code in Visual Studio is something I never got working and never really cared about anyway. The best debugging tool for JavaScript is of course the web browser, but when selecting Open in browser in Visual Studio, Chutzpah has served the HTML test page (Jasmine in my case) through the FILE:/// protocol. If you use fixtures, json files or similar, you\u0026rsquo;ve had to run the web browser with disabled security.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r The path opened in the browser has looked something like this: file:///C:/Workspaces/Development/Apps/CoolProject/JavascriptSpecs/_Chutzpah.32ac122b2d7b8f062865a46153b768fde2c181d4.test.html\nAs a result of this more tests failed when I started debugging in the browser compared to what the test runner in Visual Studio complained about.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Solution As of the 4.3.0 release of Chutzpah you can enable the tests to be served through HTTP:// instead, but it isn\u0026rsquo;t enabled by default. The BrowserArguments I\u0026rsquo;ve used to disable security can be removed. See Matthew Manela\u0026rsquo;s post about it for configuration options.\n\r\r\r\u0026#34;Server\u0026#34;: { \u0026#34;Enabled\u0026#34;: true }, \u0026#34;BrowserArguments\u0026#34;: { \u0026#34;chrome\u0026#34;: \u0026#34;--allow-file-access-from-files --allow-file-access\u0026#34; }\r\r Now the Open in browser opens a path similar to this: http://localhost:39597/Workspaces/Development/Apps/CoolProject/JavascriptSpecs/_Chutzpah.35722df6731f3d62f68cbf6a873ca82068c8446e.test.html and only the test that should fail, fails.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r So, make sure you have the latest version and enable the server mode in Chutzpah.json, then it\u0026rsquo;s all good. Thank you Matthew for this great improvement!\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r ","dateformatted":"9, December 2016","dateiso":"2016-12-09T14:29:49Z","ref":"/easier-debugging-of-js-tests-in-visual-studio-with-chutzpah-4-3-0/","summary":"If you have a Visual Studio project that uses Chutzpah for JavaScript tests, things recently got a lot easier with a long-awaited update.\nProblem When all tests pass with the Chutzpah test runner everything is fine, but when you need to debug a test, things haven\u0026rsquo;t been as easy. Debugging the JS code in Visual Studio is something I never got working and never really cared about anyway. The best debugging tool for JavaScript is of course the web browser, but when selecting Open in browser in Visual Studio, Chutzpah has served the HTML test page (Jasmine in my case) through the FILE:/// protocol.","tags":["Chutzpah","Jasmine","Unit testing","JavaScript"],"title":"Easier Debugging of JS tests in Visual Studio with Chutzpah 4.3.0+"},{"content":"This is a summary post of my exploration of Internet of Things. A small hobby project I have used to learn new technologies slightly outside my comfort zone.\n My IoT Exploration – Part 1 – The Failure My IoT Exploration – Part 2 – Raspberry Pi Sense HAT My IoT Exploration – Part 3 – Sending Data to Cloud My IoT Exploration – Part 4 – Presenting the Data My IoT Exploration – Part 5 – Costs and Architecture Refection ","dateformatted":"13, November 2016","dateiso":"2016-11-13T19:03:06Z","ref":"/my-iot-exploration/","summary":"This is a summary post of my exploration of Internet of Things. A small hobby project I have used to learn new technologies slightly outside my comfort zone.\n My IoT Exploration – Part 1 – The Failure My IoT Exploration – Part 2 – Raspberry Pi Sense HAT My IoT Exploration – Part 3 – Sending Data to Cloud My IoT Exploration – Part 4 – Presenting the Data My IoT Exploration – Part 5 – Costs and Architecture Refection ","tags":["Internet of Things"],"title":"My IoT Exploration"},{"content":"This is my fourth post about exploring Internet of Things, previous posts can be found here:\n My IoT Exploration – Part 1 – The Failure My IoT Exploration – Part 2 – Raspberry Pi Sense HAT My IoT Exploration – Part 3 – Sending Data to Cloud For the last piece of this project I pretty much followed the approach described in Visualizing IoT Data with Web App. Since I wanted to learn how to create and deploy Azure Web Apps, this was a good fit. The data going in to IoT Hub is passed on to Stream Analytics where I have a query that outputs data that I can read in my web app. This is the updated design sketch.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r The query I have in Stream Analytics is really simple, just passing on the values it receives to an Event Hub. I guess I can do more cool things here later, but for now it\u0026rsquo;s fine to just pass the data on so I can show it in near real time. I\u0026rsquo;m setting the TumblingWindow, the time interval for when Stream Analytics is outputting data, to match the interval that I\u0026rsquo;m sending data from my Raspberry Pi to IoT Hub. This is also very much a question of cost, the more data you push in and out of Azure, the more it will cost you. I might dig a bit further into the cost aspect in an upcoming post, but for now I conclude that this is sufficient for my hobby project. After all, the conditions hopefully won\u0026rsquo;t change terribly fast in my home office.\n\r\r\rWITHProcessedDataas(SELECTAVG(temperature)Temperature,AVG(humidity)Humidity,AVG(pressure)Pressure,System.TimestampASTimestampFROM[HomeInput]GROUPBYTumblingWindow(second,30))SELECT*INTO[HomeOutput]FROMProcessedData\r\r Now it\u0026rsquo;s time for some coding again and I\u0026rsquo;m doing this with NodeJS as in the guide Visualizing IoT Data with Web App. There is a template for NodeJS + Express to choose in the Azure portal. I first tried the command line route to provision the web site, but I found that the command created a bunch of stuff (like DefaultAppServicePlan) that I didn\u0026rsquo;t ask for. I felt I had more control creating this through the portal. Since I\u0026rsquo;m using Socket.io (see the code on GitHub, link below), I also need to enable Web sockets (which isn\u0026rsquo;t on by default).\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r Since the app is reading the data from the event hub, I also need a connection string to it and store that somewhere. In Azure it\u0026rsquo;s only a matter of adding an app setting in the same settings page as above.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r For development I have a json file that contains the connection string. The code first looks for the app setting and secondly for the config file. Just make sure you exclude that config file when you commit the project to source control.\n\r\r\rvar getConnectionString = function(settings) { if (settings.OFFICE_MONITORING_CONNSTRING) { return settings.OFFICE_MONITORING_CONNSTRING; } else { var configFilePath = path.resolve(__dirname, \u0026#34;config/azure.json\u0026#34;); var configContent = fs.readFileSync(configFilePath, \u0026#34;utf8\u0026#34;); var configContentJSON = JSON.parse(configContent); return configContentJSON.AzureConnectionString; } };\r\r The entire code can be found in my GitHub project Azure-IoT-Dashboard\n For deployment I set up the web app to deploy directly from my GitHub project whenever I commit a change. This is also the reason I am only using JavaScript and CSS for this instead of TypeScript and SCSS. This allows for the deployment to be a simple copy operation. I noticed that about six seconds after I pushed changes from Visual Studio Code on my local machine to GitHub, the deployment was finished in Azure. I will probably add a compilation step to this as well later on, but for now this is sufficient.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rScreenshot of the end result – Home office monitoring\r\r\r\r\r\r\r\rScreenshot of the end result – Home office monitoring\r\r\r So, here I have reached the end of this project. I read data from the Sense HAT on my Raspberry Pi with Python, send that data to a local NodeJS service written in TypeScript, then to Azure IoT Hub and finally (via Stream Analytics) visualising it on a web page. I\u0026rsquo;m using the C3 JavaScript library to create graphs for temperature, humidity and air pressure. Even though there are many things that can be improved here, I\u0026rsquo;ve learned a lot from this project and I\u0026rsquo;ve had great fun doing it. The end result can be seen at http://office.sommerfeld.nu/ (can\u0026rsquo;t guarantee it will be up forever).\n","dateformatted":"13, November 2016","dateiso":"2016-11-13T18:08:36Z","ref":"/my-iot-exploration-part-4-presenting-the-data/","summary":"This is my fourth post about exploring Internet of Things, previous posts can be found here:\n My IoT Exploration – Part 1 – The Failure My IoT Exploration – Part 2 – Raspberry Pi Sense HAT My IoT Exploration – Part 3 – Sending Data to Cloud For the last piece of this project I pretty much followed the approach described in Visualizing IoT Data with Web App. Since I wanted to learn how to create and deploy Azure Web Apps, this was a good fit.","tags":["Azure IoT Hub","Internet of Things","NodeJS"],"title":"My IoT Exploration – Part 4 – Presenting the Data"},{"content":"This is my third post about exploring Internet of Things, previous posts can be found here:\n My IoT Exploration – Part 1 – The Failure My IoT Exploration – Part 2 – Raspberry Pi Sense HAT To be able to see what my Raspberry Pi is monitoring when I\u0026rsquo;m not home, I need a hosted service that I can send my data to. I decided to go with Microsoft Azure IoT Hub.\nRegistering my Raspberry Pi in Azure IoT Hub I used the Azure Portal to set up the IoT Hub service like this:\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r Once it was created I went back to the console to get a connection string for my Raspberry. This can be done from any computer, by the way.\nnpm install -g iothub-explorer\rI copied the connection string to IoT Hub that can be found in the portal and created a connection string to my device (that I decided to call RaspberrySenseHat) like this.\niothub-explorer [Connection string to IoT Hub that you find in the portal] create RaspberrySenseHAT --connection-string\rWhen this is done I have a connection string to the device, that I will use later.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r Creating a data relay in NodeJS The next step was to create a NodeJS application that should receive data from my Python program, do some optional logging and pass it on to Azure. I hadn\u0026rsquo;t written a single application in NodeJS before, but I have done my fair share of JavaScript, so I figured I\u0026rsquo;d better use TypeScript for this to make sure I maximise the learning 🙂 I found Tony Sneed\u0026rsquo;s Yeoman Generator for TypeScript Projects Using Visual Studio Code, that was a really good starting point. It includes tasks in Gulp for building, running tests etc, so it\u0026rsquo;s great if you\u0026rsquo;re using Visual Studio Code (another tool I wanted to get familiar with).\nMy full implementation can be found in this GitHub project: Receiving environmental data and pushing it to Azure IoT Hub. The readme contains installation instructions, so I\u0026rsquo;m not going to repeat all of those here. I will however point out a few mistakes I made along the way.\nDeployment One newbie mistake I made early on was to try to move the entire project folder from my Windows machine to the Raspberry Pi. This causes two problems, firstly the node_modules folder tends to be deeper than Windows can handle (moving or copying the folder). See Why does the 260 character path length limit exist in Windows? Secondly, the Node modules should be built on the target machine. In this case I was even developing on a x64 Windows 10 machine and then running it on a Linux ARM machine, the Raspberry Pi.\n\r\r\r\u0026#34;devDependencies\u0026#34;: { \u0026#34;browser-sync\u0026#34;: \u0026#34;^2.11.0\u0026#34;, \u0026#34;del\u0026#34;: \u0026#34;^2.2.0\u0026#34;, \u0026#34;es6-module-loader\u0026#34;: \u0026#34;^0.17.10\u0026#34;, \u0026#34;event-stream\u0026#34;: \u0026#34;^3.3.2\u0026#34;, \u0026#34;glob\u0026#34;: \u0026#34;^6.0.4\u0026#34;, \u0026#34;gulp\u0026#34;: \u0026#34;^3.9.0\u0026#34;, \u0026#34;tslint\u0026#34;: \u0026#34;^3.2.1\u0026#34;, \u0026#34;tslint-stylish\u0026#34;: \u0026#34;^2.1.0-beta\u0026#34;, \u0026#34;typescript\u0026#34;: \u0026#34;^2.0.3\u0026#34;, \u0026#34;yargs\u0026#34;: \u0026#34;^3.31.0\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;azure-iot-device\u0026#34;: \u0026#34;^1.0.13\u0026#34;, \u0026#34;azure-iot-device-amqp\u0026#34;: \u0026#34;^1.0.13\u0026#34;, \u0026#34;azure-iot-device-amqp-ws\u0026#34;: \u0026#34;^1.0.14\u0026#34;, \u0026#34;azure-iot-device-http\u0026#34;: \u0026#34;^1.0.14\u0026#34;, \u0026#34;azure-iot-device-mqtt\u0026#34;: \u0026#34;^1.0.13\u0026#34;, \u0026#34;body-parser\u0026#34;: \u0026#34;~1.0.1\u0026#34;, \u0026#34;express\u0026#34;: \u0026#34;^4.14.0\u0026#34; }\r\r To avoid this and also to avoid having to check everything into source control, pulling it down and building it locally for every single change, I looked more carefully at the dependencies. If you look at the package.json file, there are dependencies and devDependencies. Make sure that only the things needed to run the application is listed in dependencies. Also, set the environment variable NODE_ENV=production in the target machine (the Raspberry Pi in this case) and make sure that package.json is included in the build output folder (dist in my project). Then you can copy the build output folder to the target machine and simply run npm install.\nGreen light At this point I\u0026rsquo;m successfully sending data to Azure and the LED on the Raspberry Pi shows a green light.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Next post: My IoT Exploration – Part 4 – Presenting the Data\n","dateformatted":"9, November 2016","dateiso":"2016-11-09T08:48:02Z","ref":"/my-iot-exploration-part-3-sending-data-to-cloud/","summary":"In part 3 of my IoT exploration I connect my Raspberry Pi to Azure IoT Hub and create a NodeJS app that relays data from my Python program to Azure.","tags":["Internet of Things","NodeJS","Raspberry Pi","TypeScript"],"title":"My IoT Exploration – Part 3 – Sending Data to Cloud"},{"content":"When my desktop application for Facebook Messenger, Messenger for Desktop, wanted me to install an update and I found out that the update wanted to install a web browser extension and otherwise pollute my computer, I uninstalled it.\nHaving a desktop app can sometimes be convenient, even though it has basically the same functionality as the corresponding web interface. So, I googled and found Franz, a desktop messaging app that supports a bunch of messaging services and is available for Windows, Mac and Linux. So far I like it.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rImage taken from http://meetfranz.com/franz-presskit.zip\r\r\r\r\r\r\rImage taken from http://meetfranz.com/franz-presskit.zip\r\r\r ","dateformatted":"6, November 2016","dateiso":"2016-11-06T11:41:01Z","ref":"/switching-to-franz/","summary":"When my desktop application for Facebook Messenger, Messenger for Desktop, wanted me to install an update and I found out that the update wanted to install a web browser extension and otherwise pollute my computer, I uninstalled it.\nHaving a desktop app can sometimes be convenient, even though it has basically the same functionality as the corresponding web interface. So, I googled and found Franz, a desktop messaging app that supports a bunch of messaging services and is available for Windows, Mac and Linux.","tags":["Franz"],"title":"Switching to Franz"},{"content":"In my current assignment we\u0026rsquo;re investigating how we can use Microsoft\u0026rsquo;s Recommendations API for providing recommended links on our web site, in a hopefully intelligent way. We need to feed the service with some usage data and since we use Google Analytics, we started to look at how we could extract data from there. This led me to the blog post 4 Ways to Export Your Google Analytics Data with R. If you just want to try which parameters you can use and are only interested in a smaller set of data, this tool (web page) works fine: Google Analytics Query Explorer. In this case we needed more data, so I made an attempt with R, a language I had never heard of before.\n Download R binaries, https://cran.rstudio.com/. Download and install R Studio, https://www.rstudio.com/products/rstudio/download/. Modify the following code to fit your needs and run in in R Studio. The R code I have commented out the stuff I ran the first time, but kept it for reference. This code extracts data in the specified date interval one day at a time and appends it to a CSV file. A thing we discovered is that we wanted the ClientId out of Google Analytics to be able to see some correlation between sessions, but that it\u0026rsquo;s freaking impossible to extract that unless you send it in as a custom dimension. This is what we have done for dimension2 (remove that from the query if you don\u0026rsquo;t have it). See Exposing ClientID in Google Analytics and User ID Reference for more info on this.\n\r\r\r# Install this stuff the first time #install.packages(\u0026#34;devtools\u0026#34;) #install.packages(\u0026#34;curl\u0026#34;) #install.packages(\u0026#34;bitops\u0026#34;) #devtools::install_github(\u0026#34;skardhamar/rga\u0026#34;) library(devtools) library(curl) library(rga) #rga.open(instance = \u0026#34;ga\u0026#34;) # run once to authenticate id \u0026lt;- \u0026#34;79442828\u0026#34; # the view in Google Analytics startDate \u0026lt;- as.Date(\u0026#34;2016-01-01\u0026#34;) endDate \u0026lt;- as.Date(\u0026#34;2016-10-20\u0026#34;) dateRange \u0026lt;- seq(from = startDate, to = endDate, by = \u0026#34;day\u0026#34;) for(dateIndex in seq_along(dateRange)) { currentDate \u0026lt;- dateRange[dateIndex] gaData \u0026lt;- ga$getData(id, batch = TRUE, walk = TRUE, start.date = currentDate, end.date=(currentDate + 1), metrics = \u0026#34;ga:visits\u0026#34;, dimensions = \u0026#34;ga:pagePath,ga:dimension2,ga:date,ga:hour,ga:minute\u0026#34;, filter = \u0026#34;ga:pagePath!@/products/pages/productdetails.aspx;ga:pagePath!@/pages/login.aspx;ga:pagePath!@/mypage;ga:pagePath!=/pages/default.aspx;ga:pagePath!@/_layouts/;ga:pagePath!~^\\\\/{1}[a-z]{2}[\\\\-]{1}[a-z]{2}\\\\/pages\\\\/default.aspx{1};ga:pagePath!~^\\\\/{1}[a-z]{2}[\\\\-]{1}[a-z]{2}$;ga:pagePath!~^\\\\/{1}[a-z]{2}[\\\\-]{1}[a-z]{2}[?]{1}\\\\S*\u0026#34;, sort = \u0026#34;-ga:date,-ga:hour,-ga:minute\u0026#34;) write.table(gaData, \u0026#34;C:/export/GA export.csv\u0026#34;, sep = \u0026#34;,\u0026#34;, col.names = F, append = T) } print(\u0026#34;done\u0026#34;)\r\r I guess the most complicated part of this (at least for me) are the regular expressions used for filtering, but those are of course specific to our needs in this situation.\nUseful links 4 Ways to Export Your Google Analytics Data with R Core Reporting API – Reference Guide Google Analytics Regular Expressions Cheat Sheet Regular Expressions 101 ","dateformatted":"3, November 2016","dateiso":"2016-11-03T21:27:30Z","ref":"/extracting-data-from-google-analytics/","summary":"In my current assignment we\u0026rsquo;re investigating how we can use Microsoft\u0026rsquo;s Recommendations API for providing recommended links on our web site, in a hopefully intelligent way. We need to feed the service with some usage data and since we use Google Analytics, we started to look at how we could extract data from there. This led me to the blog post 4 Ways to Export Your Google Analytics Data with R.","tags":["Google Analytics"],"title":"Extracting Data from Google Analytics"},{"content":"Continuing my Internet of Things journey from my earlier post My IoT Exploration – Part 1 – The Failure, I got a Raspberry Pi with a Sense HAT. The Sense HAT has all the features I need, integrated in one shield: temperature, humidity and barometric pressure and a LED as a bonus. Given my goal that I wanted to push the data to a hosted service, I now had a much better foundation for succeeding with that. This post is about the first step – reading the sensors and some design decisions.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rRaspberry Pi with Sense HAT\r\r\r\r\r\r\rRaspberry Pi with Sense HAT\r\r\r Design decisions I learned that Python is the preferred language for the Raspberry Pi and that the Sense HAT had a Python library, so I decided to stick with that (including the NOOBS Linux distribution). However, since I had no previous experience with Python and it is a whitespace significant language (which felt awkward to me), I decided to limit the Python code to the Sense HAT specific stuff. For the rest I will use NodeJS.\nI later found out that there is a Windows IoT class library in C# for Sense HAT, so if that\u0026rsquo;s more appealing to you, you could use Windows and C# instead. Getting Started with Windows 10 IoT could be a good start.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Installing stuff on the Pi Before I can dive into the code I need to install some stuff on the Raspberry Pi.\nInstall Sense HAT library sudo apt-get update sudo apt-get install sense-hat\rsudo reboot\rDeveloping and deploying I don\u0026rsquo;t have the Raspberry Pi connected to a screen, so I\u0026rsquo;ll also install Samba (sudo apt-get install samba samba-common-bin), this is just a convenience thing for copying files. It also lets me edit the Python code directly from my Windows machine. Since this is all bound to one piece of physical hardware, I will not be setting up a sophisticated deployment procedure or anything similar, an SSH shell and some file copying will do.\nMy first ever Python program The Python program reads values from the sensors, shows a status message on the LED and sends it on to another service on localhost using HTTP POST (which will be covered in a future post). Since all of this is done on the same machine with no listening services against the Internet, I haven\u0026rsquo;t bothered much with security. Conceptually the NodeJS piece could be on a separate machine, but that would of course require an authentication mechanism etc. The source code can be found in it\u0026rsquo;s entirety at https://github.com/henriksommerfeld/rpi-monitoring.\nReading temperature As I discussed in the previous post Difference between Arduino and Raspberry Pi for a High-Level Programmer, reading sensors can be tricky. The Sense HAT has a design flaw when it comes to the temperature sensor – it\u0026rsquo;s close to the CPU. It\u0026rsquo;s also a noticeable difference if you use a case or not, so you have to fiddle a bit with the values until they make sense. Some googling led me to this formula (can\u0026rsquo;t remember where, sorry). It reads the temperature from the pressure sensor which is further away from the CPU and subtracts the CPU temperature:\n\r\r\rdef get_cpu_temperature(): full_temperature_string = os.popen(\u0026#39;vcgencmd measure_temp\u0026#39;).readline() tempterature_string = (full_temperature_string.replace(\u0026#34;temp=\u0026#34;,\u0026#34;\u0026#34;).replace(\u0026#34;\u0026#39;C\\n\u0026#34;,\u0026#34;\u0026#34;)) return (float(tempterature_string)) def get_ambient_temperature(): cpu_temp = get_cpu_temperature() temperature_from_sensor = sense.get_temperature_from_pressure() ambient_temp = temperature_from_sensor - ((cpu_temp - temperature_from_sensor)/ 1.5) return ambient_temp temperature = get_ambient_temperature()\r\r The LED I\u0026rsquo;m using the LED as a basic status display, like this:\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rError. Things are not working.\r\r\r\r\r\r\rError. Things are not working.\r\r\r \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rWarning. Reading sensors is working, Python program is not able to send data to NodeJS program.\r\r\r\r\r\r\rWarning. Reading sensors is working, Python program is not able to send data to NodeJS program.\r\r\r \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rOK. Everything is working fine.\r\r\r\r\r\r\rOK. Everything is working fine.\r\r\r One more thing about the LED is worth mentioning. I found it helpful to clear the LED when terminating the program execution. I typically ran the python command from an SSH session when developing, and catching the KeyboardInterrupt exception (Ctrl + C) made sure that the LED didn\u0026rsquo;t stay on after terminating the program.\n\r\r\rdef main(): try: read_sensors() except(KeyboardInterrupt, SystemExit): screen.clear() except Exception as e: print(e) screen.show_error_message()\r\r To conclude, at this stage we are reading sensor values and trying to send them off to another local service, which however, does not yet exist.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Next step is described here: My IoT Exploration – Part 3 – Sending Data to Cloud\n","dateformatted":"1, November 2016","dateiso":"2016-11-01T05:38:21Z","ref":"/my-iot-exploration-part-2-raspberry-pi-sense-hat/","summary":"Continuing my Internet of Things journey from my earlier post My IoT Exploration – Part 1 – The Failure, I got a Raspberry Pi with a Sense HAT. The Sense HAT has all the features I need, integrated in one shield: temperature, humidity and barometric pressure and a LED as a bonus. Given my goal that I wanted to push the data to a hosted service, I now had a much better foundation for succeeding with that.","tags":["Hardware","Internet of Things","Raspberry Pi","Python"],"title":"My IoT Exploration – Part 2 – Raspberry Pi Sense HAT"},{"content":"Before I got an Arduino for Christmas I hadn\u0026rsquo;t heard about it. I had heard about Raspberry Pi though, and I was told that it was roughly the same thing.\nIt\u0026rsquo;s true that the two devices have some common characteristics, but to me the differences are fundamental. As someone who does high-level programming for a living, I am appealed by the idea of being able to read values from analogue sensors in my code, and the plethora of sensors available for the Arduino is impressive. I do however, want to do as little of that electronics and low-level coding as possible, so that I can get on to the interesting stuff — the applications that use that data in some way.\nI should say that I\u0026rsquo;m not entirely scared of electronics. I had a summer job during my time at the university where I mounted vehicle computers for Combat Vehicle 90, which included soldering, attaching circuits and loading the software and so on. But at that time I had instructions (though often quite bad) to follow and colleagues to ask.\nComputers on the other hand, are something I\u0026rsquo;ve been playing with for most of my life. Since I\u0026rsquo;ve had home servers with FreeBSD, GNU/Linux, Mac OS X and Windows, that\u0026rsquo;s familiar territory.\nMy conclusion is that using sensors is hard. They need to be accurate enough and many of them requires calibration, sometimes repeated calibrations every x unit of time. To do proper calibration of a pH sensor for example, you need liquids with different known pH values that you can match to the sensor\u0026rsquo;s values you get from the code library. For gas sensors this will be even more cumbersome.\nAlso, many of the sensors I found when I started looking, are not intended for more than experimentation, i.e. connecting it to a breadboard with a mess of wires to connect it to the Arduino. Of course this is fine for when you are actually trying it out, but I had the goal of doing something \u0026ldquo;real\u0026rdquo; — something more rugged, without having custom circuits manufactured for me.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rMessy Arduino setup\rPhoto by Gaël Chardon, published on Flickr under CC license\n\r\r\r\r\r\rMessy Arduino setup\rPhoto by Gaël Chardon, published on Flickr under CC license\n\r\r So, short version:\n Arduino is a micro controller that you load a program on to. Raspberry Pi is a computer with a full operating system.\n I guess the key is to have a good way of sending data between the two, so that you can use them both together. I found this to be a good comparison:\nhttp://www.digitaltrends.com/computing/arduino-vs-raspberry-pi/\n","dateformatted":"20, October 2016","dateiso":"2016-10-20T16:17:44Z","ref":"/difference-between-arduino-and-raspberry-pi-for-a-high-level-programmer/","summary":"Before I got an Arduino for Christmas I hadn\u0026rsquo;t heard about it. I had heard about Raspberry Pi though, and I was told that it was roughly the same thing.\nIt\u0026rsquo;s true that the two devices have some common characteristics, but to me the differences are fundamental. As someone who does high-level programming for a living, I am appealed by the idea of being able to read values from analogue sensors in my code, and the plethora of sensors available for the Arduino is impressive.","tags":["Arduino","Hardware","Raspberry Pi"],"title":"Difference between Arduino and Raspberry Pi for a High-Level Programmer"},{"content":"Last Christmas I got an Arduino starter kit for present. It took me a long time to finally open the box and have an idea about what I should do with it. What I\u0026rsquo;ve found challenging about learning new stuff that I don\u0026rsquo;t have an immediate use for, is the lack of a clear goal. Making an LED blink isn\u0026rsquo;t all that exiting in my mind and even though I like the projects in the starter kit, I didn\u0026rsquo;t do them until I had a project of my own figured out.\nMy first project idea The idea I got was to make a monitoring system for our aquarium at home. I defined the goal something like this:\n Measure values with sensors and upload it to a hosted service. Then show it as a graph on a web page.\n I found the Open Aquarium product package and figured that it should be possible to get some interesting characteristics of the water quality. Temperature was a fine start, but it would be more cool and useful if I could get a pH reading as well. I found there was a kit for that too, the Open Aquarium Aquaponics.\nThe guide on the Cooking Hacks web site is aiming for a fully automated fish tank. That was never my goal, as stated above. The guide also covers how to get the data to a web server and visualising it that way, but after a quick reading I found that this “solution” was rather silly. It includes setting up a PHP site that receives data through GET requests and puts it directly into a database. I wanted something cloud hosted that I could access without running a web server at home accessible from the outside.\nAfter seeing the limited support for sending data directly from the Arduino to the outside in a secure way, I figured that I could probably send it to a more powerful device first, then upload it to the cloud. Passing serial data between devices was however, not an appealing option. I would basically have to come up with my own protocol to determine the start and end of a “message” and so on. Ideally the box with the Arduino would be easy to stuff away, without too many cables or a computer near the tank. So I got a Wi-Fi module as well.\nThe failure After doing the projects in the Arduino starter kit, to get some familiarity with the basics, I got the products mentioned previously and started out.\nThe first insight was, that reading values of sensors is hard. Even though there are nice code libraries that does the job, there are other challenges. Reading temperature is fine, but other sensors such as pH, need to be calibrated and the values I got from this consumer-quality product was varying so much that I wouldn\u0026rsquo;t be able to draw any conclusions from it. Then my wife, head of the aquarium, came home.\n – Wife: What\u0026rsquo;s that?– Me: A pH sensor for the aquarium– Wife: It\u0026rsquo;s huge — not in my aquarium!\n One of the main purposes of our aquarium is aesthetics. After looking at the image in the Open Aquarium guide again, I smiled and saw a clear problem. (In case the page or image is removed later, it\u0026rsquo;s a fish tank polluted with sensors, cables, feeder, heater, cooling fan etc.) You would need a really big aquarium to have room for fish and plants after putting in all those gadgets.\nThe second problem was the Wi-Fi module I had. I was able to connect it to my home Wi-Fi by sending commands directly to it from my laptop, but to have it connect automatically was harder. There was not even a function in the Wi-Fi library for detecting connectivity, you would need to put in the delays of the right length between commands and hope that the connection was successful. I found a forum thread by someone who eventually got it working, but the hassle, including attaching and detaching the shields for every small adjustment to the code, just didn\u0026rsquo;t make sense to me. At this point, I gave up and had to call it a failure.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rArduino with Wi-Fi module\r\r\r\r\r\r\rArduino with Wi-Fi module\r\r\r Changing approach After a while I realised that I had learned something, so maybe it wasn\u0026rsquo;t a failure after all. I started to look into the Raspberry Pi and if I could re-define the project. The entire idea of monitoring the aquarium with all its inherited challenges (combining electronics and water for example), seemed bad. The usefulness wasn\u0026rsquo;t there, we had the aquarium running stable for a long time already and involving that wasn\u0026rsquo;t necessary for achieving the goal I had defined. So I decided to monitor temperature (and maybe some other stuff) in my home office instead. More about this later in an upcoming post (My IoT Exploration – Part 2 – Raspberry Pi Sense HAT).\n","dateformatted":"13, October 2016","dateiso":"2016-10-13T17:00:14Z","ref":"/my-iot-exploration-part-1-the-failure/","summary":"Last Christmas I got an Arduino starter kit for present. It took me a long time to finally open the box and have an idea about what I should do with it. What I\u0026rsquo;ve found challenging about learning new stuff that I don\u0026rsquo;t have an immediate use for, is the lack of a clear goal. Making an LED blink isn\u0026rsquo;t all that exiting in my mind and even though I like the projects in the starter kit, I didn\u0026rsquo;t do them until I had a project of my own figured out.","tags":["Arduino","Hardware","Internet of Things"],"title":"My IoT Exploration – Part 1 – The Failure"},{"content":"A web feature I liked when it came was the ability to resize textboxes. Especially multi-line textboxes (\u0026lt;textarea\u0026gt;\u0026lt;/textarea\u0026gt;) have a tendency to be too small and this feature really helps there. To my disappointment I find that many websites disable this feature to favour design at the expense of usability.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rInappropriately sized textbox with disabled resizing\r\r\r\r\r\r\r\rInappropriately sized textbox with disabled resizing\r\r\r To re-enable this I use the Stylebot extension for Chrome. I have added a rule to the global stylesheet (applies to all websites). \r\r\rtextarea { resize: vertical; }\r\r\nIf you have a similar extension for other browsers, I\u0026rsquo;d like to hear about it. I tried to find one for Firefox, but the ones I found seemed to have been broken for a while, so I gave up on that.\n","dateformatted":"23, September 2016","dateiso":"2016-09-23T10:10:21Z","ref":"/enable-textarea-resize/","summary":"A web feature I liked when it came was the ability to resize textboxes. Especially multi-line textboxes (\u0026lt;textarea\u0026gt;\u0026lt;/textarea\u0026gt;) have a tendency to be too small and this feature really helps there. To my disappointment I find that many websites disable this feature to favour design at the expense of usability.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rInappropriately sized textbox with disabled resizing\r\r\r\r\r\r\r\rInappropriately sized textbox with disabled resizing\r\r\r To re-enable this I use the Stylebot extension for Chrome.","tags":null,"title":"Enable Textarea Resize"},{"content":"A way of bringing up motivation when you have been working with the same software system for an extended period of time might be to add an Easter egg. When you know the system and the people around it well, you have a good opportunity to implement an Easter egg in a good way. These are the things I try to keep in mind when creating an Easter egg.\n1. Do not break anything The first and most important rule when creating an Easter egg, is to not cause a bug. If your change breaks the shopping basket preventing customers from buying your company\u0026rsquo;s products, you will have a hard time justifying your little joke. Source control will most likely make it obvious what caused the problem, so don\u0026rsquo;t cause a bug. Make sure to test your egg and the surrounding features well.\n2. Nothing offensive Think about the what will happen if the system owner, your boss or an end user of the system, finds the egg. Can you defend it? If your egg involves insulting the end user or showing something obscene and your company\u0026rsquo;s most important customer finds it by mistake, you might have a problem.\n3. Performance Make sure it doesn\u0026rsquo;t have too big of an impact on performance. It\u0026rsquo;s not a good idea to make the user download a heavy image immediately when they visit your site, just because you would like it to appear as an Easter egg at some part of your site.\n4. Hide it well enough The egg should only trigger on what you intended. Input boxes are generally good because you can trigger the egg when a certain input is given. Again, test surrounding functionality to make sure the egg doesn\u0026rsquo;t pop up unexpectedly. You should probably also avoid good coding practices if you want to hide it from you colleagues. For example, if you create a nicely separated well named component, it will be found more easily. What would you think if you found this on a page or in your source control system?\n\r\r\r\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;firstname\u0026#34; easter-egg /\u0026gt;\r\r Tip for web Easter eggs Since I work with a web based system I have a natural interface for adding a hidden function. Thinking of it, I guess it\u0026rsquo;s even easier if you\u0026rsquo;re working on an API, where your consumers will be devs who will just be happy finding an Easter egg. Anyway, I like CSS Shake – it\u0026rsquo;s lightweight if you only need one of the effects and it\u0026rsquo;s visually amusing.\nTo get some more inspiration, check out 11 Best easter eggs on the web and in software: hidden secrets and surprises.\n","dateformatted":"6, September 2016","dateiso":"2016-09-06T19:48:29Z","ref":"/my-guideline-for-creating-easter-eggs/","summary":"A way of bringing up motivation when you have been working with the same software system for an extended period of time might be to add an Easter egg. When you know the system and the people around it well, you have a good opportunity to implement an Easter egg in a good way. These are the things I try to keep in mind when creating an Easter egg.\n1. Do not break anything The first and most important rule when creating an Easter egg, is to not cause a bug.","tags":null,"title":"My Guideline for Creating Easter Eggs"},{"content":"This is just a short reminder for myself. I just found myself reinstalling Windows and thus also my favourite console, Cmder. I didn\u0026rsquo;t remember what to do to make it findable by Windows search, so here\u0026rsquo;s how:\n1. Add path to PATH environment variable This can be either for your specific user or for the whole system.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r 2. Log off and on again 3. Voilà! \r\r\r\rWin+ cmder\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r ","dateformatted":"19, August 2016","dateiso":"2016-08-19T20:00:37Z","ref":"/making-cmder-findable-by-windows-search/","summary":"This is just a short reminder for myself. I just found myself reinstalling Windows and thus also my favourite console, Cmder. I didn’t remember what to do to make it findable by Windows search, so here’s how: 1. Add path to environment variable 2. Log off and on again 3. Voilà!","tags":["Cmder","Windows search","Windows 10"],"title":"Making Cmder Findable by Windows Search"},{"content":"I have stored settings in applications built on SharePoint in a number of ways over the years, including SharePoint lists, the property bag and even web.config. As I have done most of the recent development using AngularJS and also had the need to store and fetch configuration values beyond simple data types, I have start using another way of storing these settings.\nBy storing text files with json configuration objects in a document library I get a couple of benefits. First of all I do not need server access or super-high permissions, which is always good, but a must if we\u0026rsquo;re talking about a cloud hosted solution. Secondly, I can have configuration beyond what I have usually used with SharePoint lists, where it has typically been booleans and integer values (like switching something on and off, setting number of items displayed in a listing etc.)\nIn a recent requirement I had to present users with the ability to choose a language if they belonged to specific countries during newsletter sign up. For some countries the newsletter was available in multiple languages, for most countries in one specific language and in English for the rest.\nStep one I started out by creating a hard-coded object to figure out how it needed to look like. When everything was working I added a method to my service that fetches the configuration file from a document library in SharePoint named Configuration.\n\r\r\r// MAN = My Abbreviated Namespace // Obviously I have stripped out other methods here angular.module(\u0026#39;MAN\u0026#39;).service(\u0026#39;newsSubscriptionService\u0026#39;, [\u0026#39;$http\u0026#39;, function ($http) { var service = {}; service.getCountryLanguageSettings = function () { return $http({ method: \u0026#39;GET\u0026#39;, url: \u0026#34;/configuration/newsletter-subscription-countries-languages.txt\u0026#34; }); }; return service; }]);\r\r Step two I created a the document library Configuration in SharePoint with the appropriate permissions. Since this isn\u0026rsquo;t an especially user-friendly configuration format I restricted the write permissions to the dev team. The idea here is not to change these settings often, but rather to be able to do it without a deploy. Then I added the file with the configuration object, in this case an array with countries and their respective languages. Since SharePoint by default considers a lot of file extensions to be harmful (like .json and .config for example), I just made a txt file. This file is a bit longer in reality, but you get the point. Since English is the default language I do not store that.\n\r\r\r[ { \u0026#34;CountryCode\u0026#34; : \u0026#34;de\u0026#34;, \u0026#34;Languages\u0026#34;: [ { \u0026#34;Locale\u0026#34;: \u0026#34;de-DE\u0026#34;, \u0026#34;Title\u0026#34;: \u0026#34;Deutsch\u0026#34; } ] }, { \u0026#34;CountryCode\u0026#34; : \u0026#34;be\u0026#34;, \u0026#34;Languages\u0026#34;: [ { \u0026#34;Locale\u0026#34;: \u0026#34;nl-NL\u0026#34;, \u0026#34;Title\u0026#34;: \u0026#34;Nederlands\u0026#34; } ] }, { \u0026#34;CountryCode\u0026#34; : \u0026#34;nl\u0026#34;, \u0026#34;Languages\u0026#34;: [ { \u0026#34;Locale\u0026#34;: \u0026#34;nl-NL\u0026#34;, \u0026#34;Title\u0026#34;: \u0026#34;Nederlands\u0026#34; } ] }, { \u0026#34;CountryCode\u0026#34; : \u0026#34;lu\u0026#34;, \u0026#34;Languages\u0026#34;: [ { \u0026#34;Locale\u0026#34;: \u0026#34;nl-NL\u0026#34;, \u0026#34;Title\u0026#34;: \u0026#34;Nederlands\u0026#34; } ] }, { \u0026#34;CountryCode\u0026#34; : \u0026#34;ch\u0026#34;, \u0026#34;Languages\u0026#34;: [ { \u0026#34;Locale\u0026#34;: \u0026#34;de-DE\u0026#34;, \u0026#34;Title\u0026#34;: \u0026#34;Deutsch\u0026#34; }, { \u0026#34;Locale\u0026#34;: \u0026#34;fr-FR\u0026#34;, \u0026#34;Title\u0026#34;: \u0026#34;Français\u0026#34; }, { \u0026#34;Locale\u0026#34;: \u0026#34;it-IT\u0026#34;, \u0026#34;Title\u0026#34;: \u0026#34;Italiano\u0026#34; } ] } ] \r\r As any good solution, this is far from rocket science, that\u0026rsquo;s why I like it.\n","dateformatted":"15, August 2016","dateiso":"2016-08-15T19:06:27Z","ref":"/storing-configuration-data-in-sharepoint-lists/","summary":"I have stored settings in applications built on SharePoint in a number of ways over the years, including SharePoint lists, the property bag and even web.config. As I have done most of the recent development using AngularJS and also had the need to store and fetch configuration values beyond simple data types, I have start using another way of storing these settings.\nBy storing text files with json configuration objects in a document library I get a couple of benefits.","tags":["SharePoint","JavaScript"],"title":"Storing Configuration Data in SharePoint Lists"},{"content":"Hi have recently bought an ultra-wide monitor for my desk at home, a Dell U3415W. I\u0026rsquo;m using it both for work as a coder and for my personal needs such as photo editing, web browsing and watching films. I have previously used a dual monitor setup, so I wanted to find a way of organising windows to be used side-by-side in a productive manner.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rDell U3415W Monitor, 34″ 3440×1440\r\r\r\r\r\r\r\rDell U3415W Monitor, 34″ 3440×1440\r\r\r Of course Windows has had the feature to add two windows side-by-side built-in since Windows 7. With Windows 10 we now also have the annoying feature of splitting the screen in four (like the Windows logo in the image above), which sort of breaks the keyboard shortcuts (\r\r\r\rWin+ ←and \r\r\r\rWin+ →) and is something I find useless unless you have a 4K monitor or higher.\nBut, to be able to use an ultra-wide monitor most effectively, I like to split the screen space into three columns. To make most use of this, it\u0026rsquo;s even better if you can combine these thirds in any way you like, e.g. having one window occupying 2/3 and one window occupying 1/3 of the screen.\nI use WinDock to achieve this and here\u0026rsquo;s my configuration file. By dragging a window to the bottom I make it use 1/3 of the screen and by dragging it to the top I make it use 2/3 of the screen.\nUltra-wide WinDock profile.json \r\r\r{ \u0026#34;name\u0026#34;:\u0026#34;21:9 Profile\u0026#34;, \u0026#34;rules\u0026#34;:[ { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 50, 0, 100, 100 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;right\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;edge\u0026#34;, \u0026#34;values\u0026#34;:[ 0, 100 ] } }, { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 0, 0, 50, 100 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;left\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;edge\u0026#34;, \u0026#34;values\u0026#34;:[ 0, 100 ] } }, { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 50, 50, 100, 100 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;right\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;edge\u0026#34;, \u0026#34;values\u0026#34;:[ 50, 100 ] } }, { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 0, 0, 67, 100 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;top\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;edge\u0026#34;, \u0026#34;values\u0026#34;:[ 0, 33 ] } }, { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 33, 0, 100, 100 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;top\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;edge\u0026#34;, \u0026#34;values\u0026#34;:[ 67, 100 ] } }, { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 0, 0, 100, 100 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;top\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;edge\u0026#34;, \u0026#34;values\u0026#34;:[ 33, 67 ] } }, { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 0, 0, 33, 100 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;bottom\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;edge\u0026#34;, \u0026#34;values\u0026#34;:[ 0, 33 ] } }, { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 33, 0, 67, 100 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;bottom\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;edge\u0026#34;, \u0026#34;values\u0026#34;:[ 33, 67 ] } }, { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 67, 0, 100, 100 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;bottom\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;edge\u0026#34;, \u0026#34;values\u0026#34;:[ 67, 100 ] } }, { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 50, 0, 100, 50 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;top_left\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;corner\u0026#34; } }, { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 0, 0, 50, 50 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;top_right\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;corner\u0026#34; } }, { \u0026#34;dock\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;values\u0026#34;:[ 0, 50, 50, 100 ] }, \u0026#34;trigger\u0026#34;:{ \u0026#34;monitor\u0026#34;:0, \u0026#34;pos\u0026#34;:\u0026#34;bottom_right\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;corner\u0026#34; } } ] }\r\r\n","dateformatted":"30, April 2016","dateiso":"2016-04-30T16:59:15Z","ref":"/windock-configuration-for-my-219-screen/","summary":"Hi have recently bought an ultra-wide monitor for my desk at home, a Dell U3415W. I\u0026rsquo;m using it both for work as a coder and for my personal needs such as photo editing, web browsing and watching films. I have previously used a dual monitor setup, so I wanted to find a way of organising windows to be used side-by-side in a productive manner.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\rDell U3415W Monitor, 34″ 3440×1440\r\r\r\r\r\r\r\rDell U3415W Monitor, 34″ 3440×1440\r\r\r Of course Windows has had the feature to add two windows side-by-side built-in since Windows 7.","tags":["Hardware","WinDock","Windows 10"],"title":"WinDock Configuration for my 21:9 Screen"},{"content":"As my Mac Mini from 2010 has gotten slower and slower I\u0026rsquo;ve been thinking about a replacement for it as the HTPC (Home Theatre Personal Computer) I\u0026rsquo;ve been using it for. I was running Plex (both server and client) on it connected to a “dumb TV” and I\u0026rsquo;ve been fully happy with that. So, when I draw the conclusion that the old computer didn\u0026rsquo;t cut it any more, I had a few different options. The needs I wanted to meet were:\n Easy way to watch streaming video such as Netflix, HBO and SVT Play (Swedish Public Service TV) Host local video and play it on the TV Playing video from streaming services For this I bought the Chromecast that is connected to the TV (or actually to the sound system that in turn is connected to the TV). Then it\u0026rsquo;s easy to cast from any of the streaming apps that supports Google Cast.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rPhoto by TechStage published on Flickr under CC license\n\r\r\r\r\r\rPhoto by TechStage published on Flickr under CC license\n\r\r Local hosting of video Since I live in an apartment which doesn\u0026rsquo;t fit a large or noisy NAS or other server, I ended up using the media server feature of my router (Asus RT-AC68U) and connected a USB powered 2.5″ HDD to it. I can access the disk via DLNA and SMB – I guess most modern routers have a similar feature. So for uploading or watching stuff from my computer I just access the files through \\\\192.168.1.1 in Windows as I would have with any other NAS.\nPlaying video on phone and dumb TV To play the video files on the TV I go through the phone using the Infuse app (only on iOS unfortunately). Infuse supports both Google Cast and AirPlay, so from there it\u0026rsquo;s just a matter of casting to Chromecast in the same way I do from Netflix or any other of the streaming apps. Of course I can just watch it directly on the phone if I would prefer that.\nNon-technical spouse perspective This works surprisingly well and it was an easy transition for my wife who\u0026rsquo;s already gotten used to casting from the different apps on her phone. From a “non-technical spouse perspective” Infuse is just like Netflix, but for home videos. The only thing I needed to do was to create bookmarks in the Infuse app to the relevant folders, since the media server feature on the router shows a maze of empty non-removable folders that makes it quite hard to find what you are looking for.\nThings I had to buy In contrast to my initial fear of having to buy a new computer or “smart” TV, this was quite inexpensive. The most expensive thing I bought was a new USB hard drive. I could probably have used the old one I had connected to my Mac Mini, but I suspected it was going to die soon (after many years of great service). In total I ended up buying:\n Chromecast Infuse app USB HDD (could have used the old one) Update I have now also added Videostream for Chromecast to this mix for playing video files that Infuse doesn\u0026rsquo;t support. I especially like how Videostream registers in the operating system so that you can search for “video” to find it (using \r\r\r\rWinor ⌘ + space) (as opposed to starting Chrome and look for the extension).\n","dateformatted":"11, February 2016","dateiso":"2016-02-11T21:26:01Z","ref":"/inexpensive-home-media-solution/","summary":"As my Mac Mini from 2010 has gotten slower and slower I\u0026rsquo;ve been thinking about a replacement for it as the HTPC (Home Theatre Personal Computer) I\u0026rsquo;ve been using it for. I was running Plex (both server and client) on it connected to a “dumb TV” and I\u0026rsquo;ve been fully happy with that. So, when I draw the conclusion that the old computer didn\u0026rsquo;t cut it any more, I had a few different options.","tags":["Chromecast","Hardware","Infuse"],"title":"Inexpensive Home Media Solution"},{"content":"I have recently got the chance to upgrade my work computer from an old, heavy and ugly machine with almost no battery life and a terrible screen to something new and shiny. I ended up getting the third generation Lenovo X1 Carbon (unfortunately without touch screen for budget reasons). This is my experience so far of that machine running Windows 10.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r I really like the X1 Carbon. Lightweight with a good screen and good keyboard, and good performance for an ultrabook. Even my MacBook loving wife thought it looked cool.\nScreen and scaling I have the 2560×1440 screen set at 125% scaling and I\u0026rsquo;m sort of glad that I didn\u0026rsquo;t get the 3200×1800 Dell XPS 13 I would have got if it had been available. I still have to log out and in again to Windows after connecting my two 1920×1200 monitors (using the ThinkPad OneLink Pro Dock docking station), otherwise everything is horribly blurry. Even many of the Store apps are not using the screen resolution in a good way. The Facebook app is for instance showing the feed in a narrow column with small text in the middle of the screen with a lot of dead space on the sides.\nLogging in The possibility to log in with a PIN code is something I really like, especially when using an external keyboard with a full numerical keypad, this makes the login quick and simple. When using only the laptop I use the fingerprint reader which I find even simpler. I think this is a great improvement for security, especially as you are encouraged to use your Microsoft account for logging in to Windows. If you want to have a good password on your Microsoft account and you would need to enter that password every time your laptop wakes up, you would go crazy, so this is a great improvement.\nCalendar app One of the things I have been missing in Windows is a calendar application that supports all relevant providers. I\u0026rsquo;ve had my Office 365, Gmail and iCloud in one calendar view in iOS for a long time. Now I can have that in Windows too, and I can see the upcoming appointment on the lock screen. Fantastic!\nSetting up a good command line experience The PowerShell console has been vastly improved in Windows 10 with the possibility to resize the window and have persisted command history between sessions. Of course it’s laughable if you come from a UNIX based system that these are new features released in 2015, but it’s still a great improvement. However, I still prefer Cmder that has everything in one console. One-Get is also looking promising, but as long as it doesn’t have and update command I’ll stick with Chocolatey and its command line tools. Installing and upgrading programs through the command line feels so much simpler to me than visiting a web site, finding the right download link and doing the next, next, finish routine.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r I found Setup Windows 10 for Modern/Hipster Development to be a good resource for finding some nice tools.\n","dateformatted":"21, September 2015","dateiso":"2015-09-21T09:59:55Z","ref":"/lenovo-x1-carbon-with-windows-10/","summary":"I have recently got the chance to upgrade my work computer from an old, heavy and ugly machine with almost no battery life and a terrible screen to something new and shiny. I ended up getting the third generation Lenovo X1 Carbon (unfortunately without touch screen for budget reasons). This is my experience so far of that machine running Windows 10.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r I really like the X1 Carbon.","tags":["Cmder","Hardware","Windows 10"],"title":"Lenovo X1 Carbon with Windows 10"},{"content":"Some time ago I got the idea to ramp up my skills in front-end web development and the tooling used outside my own world of SharePoint. I decided to create my own WordPress theme, but use as much of existing tooling and templates as I could.\nSage got my attention, so I downloaded that and started to look into its components. This was all new territory for me with Bower, Gulp, PHP, NodeJS and so on.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r One thing I had noticed with my previous WordPress site was that the number of script and styling references (js and css files) grew as plug-ins were added. By having my own theme and taking control of the HTML being generated I hoped to be able to bundle the Javascript and CSS files to reduce the number of resources that visitors had to download.\nOne plug-in I was trying out was Crayon Syntax Hightlighter, so let\u0026rsquo;s look at what references it adds to my pages.\n\r\r\r\u0026lt;link rel=\u0026#39;stylesheet\u0026#39; id=\u0026#39;crayon-css\u0026#39; href=\u0026#39;http://127.0.0.1/henrik.sommerfeld.nu/wp-content/plugins/crayon-syntax-highlighter/css/min/crayon.min.css?ver=2.7.1\u0026#39; type=\u0026#39;text/css\u0026#39; media=\u0026#39;all\u0026#39; /\u0026gt; \u0026lt;link rel=\u0026#39;stylesheet\u0026#39; id=\u0026#39;crayon-theme-classic-css\u0026#39; href=\u0026#39;http://127.0.0.1/henrik.sommerfeld.nu/wp-content/plugins/crayon-syntax-highlighter/themes/classic/classic.css?ver=2.7.1\u0026#39; type=\u0026#39;text/css\u0026#39; media=\u0026#39;all\u0026#39; /\u0026gt; \u0026lt;link rel=\u0026#39;stylesheet\u0026#39; id=\u0026#39;crayon-font-consolas-css\u0026#39; href=\u0026#39;http://127.0.0.1/henrik.sommerfeld.nu/wp-content/plugins/crayon-syntax-highlighter/fonts/consolas.css?ver=2.7.1\u0026#39; type=\u0026#39;text/css\u0026#39; media=\u0026#39;all\u0026#39; /\u0026gt; \u0026lt;script type=\u0026#39;text/javascript\u0026#39; src=\u0026#39;http://127.0.0.1/henrik.sommerfeld.nu/wp-content/plugins/crayon-syntax-highlighter/js/min/crayon.min.js?ver=2.7.1\u0026#39;\u0026gt;\u0026lt;/script\u0026gt;\r\r That\u0026rsquo;s four additional files for the visitor to download right there. So let\u0026rsquo;s first look at how to get rid of the styles (I leave the js file for now).\nRemoving the CSS links I learned that there was a queueing mechanism in WordPress that makes it possible to dequeue styles that I don\u0026rsquo;t want. By hooking into that queue I should be able to get rid of the link style tags rendered on the page.\nUsage \r\r\r\u0026lt;?php wp_dequeue_style( $handle ) ?\u0026gt;\r\r\nThe function reference page for wp_dequeue_style (quoted above) leaves me with two unanswered questions:\n Where should I add this code? What is this $handle thing? Sage has an extras.php file that suits for this. If you\u0026rsquo;re not using Sage, it\u0026rsquo;s probably functions.php that suits best. We need to hook into the queueing pipeline right before the links are rendered. To avoid having to wade through the plug-in code to find the handle, instead I use the path from the rendered links above. This is what I added to my extras.php for removing the styles:\nFirst and last line is only added here fo my syntax highlighter to understand it\u0026rsquo;s PHP, please ignore.\n\r\r\r\u0026lt;?php function remove_plugin_styles() { global $wp_styles; $paths_to_remove = array( \u0026#39;/crayon-syntax-highlighter/\u0026#39;, // Additional references here ); foreach($wp_styles -\u0026amp;gt; registered as $registered) { foreach ($paths_to_remove as $path) { if (strpos($registered-\u0026amp;gt;src,$path) !== false) { wp_dequeue_style( $registered-\u0026amp;gt;handle ); wp_deregister_style( $registered-\u0026amp;gt;handle ); if (WP_ENV === \u0026#39;development\u0026#39;) { echo \u0026#34;\\n\u0026amp;lt;!-- Removed style reference: \u0026#34; . $registered-\u0026amp;gt;src . \u0026#34; (\u0026#34; . $registered-\u0026amp;gt;handle . \u0026#34;) --\u0026amp;gt;\u0026#34;; } } } } } add_action(\u0026#39;wp_print_styles\u0026#39;, __NAMESPACE__ . \u0026#39;\\\\remove_plugin_styles\u0026#39;); ?\u0026gt;\r\r If I reload the page source I can see that the links are gone and I can see my debug info instead:\n\r\r\r\u0026lt;!-- Removed style reference: http://127.0.0.1/henrik.sommerfeld.nu/wp-content/plugins/crayon-syntax-highlighter/css/min/crayon.min.css (crayon) --\u0026gt; \u0026lt;!-- Removed style reference: http://127.0.0.1/henrik.sommerfeld.nu/wp-content/plugins/crayon-syntax-highlighter/themes/classic/classic.css (crayon-theme-classic) --\u0026gt; \u0026lt;!-- Removed style reference: http://127.0.0.1/henrik.sommerfeld.nu/wp-content/plugins/crayon-syntax-highlighter/fonts/consolas.css (crayon-font-consolas) --\u0026gt;\r\r Adding the references to my bundle Sage already provides me with a manifest.json file where I can define the assets I want to be compiled and copied to a dist folder that I later can push to my test and production environment. So I decided to use an extras.css file for all the additional resources that I could load at the end of the page (that are not essential to the first page rendering)\n\r\r\r{ ... \u0026#34;extras.css\u0026#34;: { \u0026#34;files\u0026#34;: [ \u0026#34;styles/extras.scss\u0026#34; ], \u0026#34;vendor\u0026#34;: [ \u0026#34;../../plugins/crayon-syntax-highlighter/themes/classic/classic.css\u0026#34;, \u0026#34;../../plugins/crayon-syntax-highlighter/fonts/consolas.css\u0026#34;, \u0026#34;../../plugins/crayon-syntax-highlighter/css/min/crayon.min.css\u0026#34; ] }, ... }\r\r One thing to be aware of here is that the CSS files most likely contains references to images or font files that needs to be placed at the correct relative paths to the dist (output) folder where the extras.css ends up. This could also be an issue when upgrading the plug-in later on. But from my perspective that\u0026rsquo;s not really a problem since I\u0026rsquo;m the only user of the theme and when there\u0026rsquo;s an update to a plug-in I use, I upgrade it in my development environment, rebuild my theme, test it and make adjustments if needed, before I upgrade the plug-in on the production site.\n","dateformatted":"21, June 2015","dateiso":"2015-06-21T20:02:23Z","ref":"/bundling-resources-from-plug-ins-in-my-wordpress-theme/","summary":"Some time ago I got the idea to ramp up my skills in front-end web development and the tooling used outside my own world of SharePoint. I decided to create my own WordPress theme, but use as much of existing tooling and templates as I could.\nSage got my attention, so I downloaded that and started to look into its components. This was all new territory for me with Bower, Gulp, PHP, NodeJS and so on.","tags":["Cmder","Sage","WordPress"],"title":"Bundling resources from plug-ins in my WordPress theme"},{"content":"Recently I got a request to find out the usage of page layouts on a large SharePoint site. I Googled and found a few scripts, but none that did all I wanted, e.g. to limit the inventory to a certain sub-site. So I wrote my own one.\n\r\r\rfunction Get-PageLayoutsInUse { [CmdletBinding()] Param ( [Parameter(Mandatory=$true)] [string] $SiteUrl, [Parameter(Mandatory=$false)] [string] $MatchUrlPart ) Add-PSSnapin \u0026#34;Microsoft.SharePoint.PowerShell\u0026#34; -ErrorAction SilentlyContinue $objSite = Get-SPSite $SiteUrl [Microsoft.Sharepoint.Publishing.PublishingSite]$publishingSite = New-Object Microsoft.SharePoint.Publishing.PublishingSite($objSite) $layoutsInUse = @() if ([Microsoft.SharePoint.Publishing.PublishingWeb]::IsPublishingWeb($publishingSite.RootWeb) -eq $true) { $pageLayouts = $publishingSite.GetPageLayouts($false) foreach($layout in $pageLayouts) { Write-Host \u0026#34;Looking for references to \u0026#34; $layout.ServerRelativeUrl [Microsoft.SharePoint.SPFile]$file = $publishingSite.RootWeb.GetFile($layout.ServerRelativeUrl); if($MatchUrlPart) { $links = $file.BackwardLinks | where {$_.ServerRelativeUrl -like $MatchUrlPart } } else { $links = $file.BackwardLinks } $layoutReportItem = @{Title=$layout.Title;File=$layout.ServerRelativeUrl;Count=$links.Count} $newobject = New-Object PSObject -Property $layoutReportItem $layoutsInUse += $newobject } } $a = @{Expression={$_.LayoutName}} $layoutsInUse | Sort-Object Count -Descending | Select-Object -Property * }\r\r This has been tested in a SharePoint 2013 on-prem farm run locally on a server in the farm. If run from C: it can be used like this and exported to CSV for the entire site:\n. C:\\Get-PageLayoutsInUse.ps1\rGet-PageLayoutsInUse -SiteUrl \u0026quot;http://company.com\u0026quot; | Export-Csv C:\\page\\_layouts\\_on\\_entire\\_site.csv –NoTypeInformation\rOr for the sub-site Products in English:\n. C:\\Get-PageLayoutsInUse.ps1\rGet-PageLayoutsInUse -SiteUrl \u0026quot;http://company.com\u0026quot; -MatchUrlPart \u0026quot;\\*/en-gb/products/\\*\u0026quot; | Export-Csv C:\\page\\_layouts\\_under_products.csv –NoTypeInformation\r","dateformatted":"24, March 2015","dateiso":"2015-03-24T23:05:12Z","ref":"/inventory-of-page-layout-usage-in-a-sharepoint-site/","summary":"Recently I got a request to find out the usage of page layouts on a large SharePoint site. I Googled and found a few scripts, but none that did all I wanted, e.g. to limit the inventory to a certain sub-site. So I wrote my own one.","tags":["PowerShell","Scripting","SharePoint","Windows Server"],"title":"Inventory of Page Layout Usage in a SharePoint Site"},{"content":"Quite a few organisations seems to find this thing called “Internet” a scary thing that employees can only be given access to by grace of the mighty network administrators. As a consultant I have worked for a few of those organisations and felt the frustration when a blog is blocked or network traffic is so slow that you\u0026rsquo;d guess that it\u0026rsquo;s manually monitored before accepted. I would personally never try to bypass these controls of course, but hypothetically one could do like the following.\nYou would need a proxy of your own somewhere on the internet that you can access from the corporate network to act as a relay to the free unrestricted internet. It could be a virtual machine running in a cloud service, or a computer you have at home. For this post I\u0026rsquo;m going to assume a Mac Mini at home running Tinyproxy.\nSetting up your relay For installing applications apart from those in App Store I prefer Homebrew.\nbrew install tinyproxy\rSince I want this service to run at all times independent of which user is logged in, a .plist file should be added to the /Library/LaunchAgents folder. This could be done with brew services, but for some reason that didn\u0026rsquo;t work for me. Running sudo brew services start tinyproxy just created an empty file for me, /Library/LaunchDaemons/homebrew.mxcl.tinyproxy.plist. So I added the contents myself, ending up with this:\n\r\r\r\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC -//Apple Computer//DTD PLIST 1.0//EN http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Label\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;homebrew.mxcl.tinyproxy.plist\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ProgramArguments\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;/usr/local/opt/tinyproxy/sbin/tinyproxy\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;-c\u0026lt;/string\u0026gt; \u0026lt;string\u0026gt;/usr/local/opt/tinyproxy/etc/tinyproxy.conf\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;RunAtLoad\u0026lt;/key\u0026gt; \u0026lt;true/\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt;\r\r I leave the config file for tinyproxy pretty much untouched, running at port 8888. I connect to it locally to verify that it\u0026rsquo;s running, telnet 127.0.0.1 8888.\nI also need an SSH server running on the machine to connect to from the outside and port forwarding set up in my router. This can be a bit of trial and error to find out through witch ports you can access your home server from the corporate network, port 80 (HTTP) and 443 (HTTPS) will often work. In my case it didn\u0026rsquo;t, but I found that on port 23 (telnet) I was able to reach my home server, so I set it up in my router like this (192.168.1.123 is my Mac Mini\u0026rsquo;s IP address on the home network):\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Connecting to the relay from the corporate computer The corporate machine provided to me is a Windows 7 machine in this case, where I start by installing Tunnelier (which also exists in a portable version, if you’re not a local admin). I start by setting up the tunnel to my Mac Mini at home by connecting to its dynamic DNS name on the port I forwarded in my router.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r To be able to get outside the corporate network I have to use their proxy, so I enter the same proxy settings I find in Internet Explorer in Tunnelier as well:\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r To access Tinyproxy I need to set up some local port forwarding under the client-to-server tab. The destination port should be the one where tinyproxy is listening locally on my Mac Mini, in this case 8888. I also have a VNC server on port 5900 in this screenshot so I can connect graphically to my Mini. The local ports on the Windows 7 machine can be any that is not used already by another service, I chose 1053.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r In Internet Explorer (which is really Windows global proxy settings) I enter 127.0.0.1 on port 1053 as my new proxy.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r Since I must be able to access internal corporate network resources I cannot route all traffic through my home server, so I add exceptions for the corporate domain and IP address range (123.4.* in this screenshot).\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r One last thing I could do to make things quicker is to create scripts for turning the proxy settings on and off. If you need to connect to a WiFi in a hotel or on a train where the network is open but requires you to visit a web page to enter some login information, the proxy must be turned off to access that web page. I put these scripts on the desktop so I can right-click them and choose \u0026ldquo;Run with PowerShell\u0026rdquo;.\nhome-server.ps1 \r\r\r$ProxyServer = \u0026#34;127.0.0.1\u0026#34; $ProxyPort = \u0026#34;1053\u0026#34; $Path = \u0026#34;HKCU:SoftwareMicrosoftWindowsCurrentVersionInternet Settings\u0026#34; $Proxy = $ProxyServer + \u0026#34;:\u0026#34; + $ProxyPort \\$Exceptions = \u0026#34;\u0026lt;-loopback\u0026gt;;_.company.com;123.4._\u0026#34; # Enable an explicit proxy Set-ItemProperty -Path $path -Name ProxyEnable -Value 1 Set-ItemProperty -Path $path -Name ProxyServer -Value $Proxy Set-ItemProperty -Path $path -Name ProxyOverride -Value \\$Exceptions\r\r no-proxy.ps1 \r\r\r\\$Path = \u0026#34;HKCU:SoftwareMicrosoftWindowsCurrentVersionInternet Settings\u0026#34; # Disable an explicit proxy Set-ItemProperty -Path \\$path -Name ProxyEnable -Value 0\r\r A final word of warning is that this will of course be against the corporate IT policy, that’s why I would never do this in real life.\n","dateformatted":"1, March 2015","dateiso":"2015-03-01T09:32:04Z","ref":"/bypassing-a-corporate-proxy/","summary":"Quite a few organisations seems to find this thing called “Internet” a scary thing that employees can only be given access to by grace of the mighty network administrators. As a consultant I have worked for a few of those organisations and felt the frustration when a blog is blocked or network traffic is so slow that you\u0026rsquo;d guess that it\u0026rsquo;s manually monitored before accepted. I would personally never try to bypass these controls of course, but hypothetically one could do like the following.","tags":["macOS","Networking","PowerShell","Proxy","Scripting"],"title":"Bypassing a Corporate Proxy"},{"content":"When Microsoft recently released a new Outlook app for iOSI decided to try it out. I previously used both the built-in Mail app from Apple and the Gmail app side by side, so if Outlook was good enough I could maybe switch to one single mail app on the phone. You can read some more about it here: A deeper look at Outlook for iOS and Android.\nTrying out the new features it struck me that a lot of people must have a hard time managing their e-mail. All e-mail applications seem to have features I don’t understand. Messages can be flagged, starred, marked as important/prioritised and sorted into different “sub-inboxes”. The new Outlook app has something called Focused inbox, which is explained like this:\n Focused Inbox intelligently presorts your email so you can focus on what matters. It places your most important emails in “Focused” and the rest in “Other.” Focused Inbox works across all email accounts, personal and professional.\n I find these features a way of avoiding to answer the two basic questions about every input in your life – “What is this” and “What am I going to do with it?”\nWhat am I supposed to do with the messages in “Other”? And why should an algorithm decide what’s important to me rather than deciding that myself? Since I usually have zero messages in my inbox, I don’t see much value in this.\nAnother feature that all e-mail apps seem to have is archiving. Archiving meaning throw everything into one big bucket so that I don’t have to see it. Don’t mistake this for an archivist’s definition of archiving, by the way. Promoting archiving before deleting is of course a commercial decision for a company like Google that make money from analysing our e-mails and using that to sell ads. Sure, most e-mail services have great search functionality which should reduce the need for deleting stuff. However, I found that deleting what should be deleted makes it a lot easier to find what I am looking for, whether I use search or folders/tags or a combination (which I have found to be the best option).\nMore important than getting the perfect e-mail app I would say is to Getting Your Inbox to Zero. Do that first, then the rest are just details. I must say however, that I like the Outlook app for iOS. Some features are great and the things I found annoying, I could turn off. I consider that a feature by itself – that I can turn off features I don\u0026rsquo;t want to use.\n","dateformatted":"30, January 2015","dateiso":"2015-01-30T22:30:18Z","ref":"/unnecessary-e-mail-features/","summary":"When Microsoft recently released a new Outlook app for iOSI decided to try it out. I previously used both the built-in Mail app from Apple and the Gmail app side by side, so if Outlook was good enough I could maybe switch to one single mail app on the phone. You can read some more about it here: A deeper look at Outlook for iOS and Android.\nTrying out the new features it struck me that a lot of people must have a hard time managing their e-mail.","tags":null,"title":"All these Unnecessary E-mail Features"},{"content":"When you build a web site on a [CMS][1] and a theme made by someone else you always have some limitations. For this blog I use WordPress and the [Montezuma theme][2], and this is how I customised the default CSS grid options to my liking.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rThese are the grid options you can choose from in version 1.2.4 of the Montezuma theme. Not bad at all, but if you want to use a responsive layout (and yes, you should) you have to choose between a fixed 960 px maximum width or use the 100% width option which can become ridiculous on large high resolution screens. So, what I did was to choose the 100% option and then add `max-width` to constrain the width. This was an easy way to achieve what I wanted with a minimal adjustment and have relative widths, which I really prefer in a CSS grid system.\rUsing _Chrome Developer Tools_ I identified the CSS rule that determines the page width.\r\r\r\r.row, .row5, .lw { width: 100%; margin: 0 auto; }\r\r\rNow the only remaining thing was to add a new rule with the same selector defining the max-width I found appropriate – which I did to in the content.css file like this.\r\r\r\r.row, .row5, .lw { max-width: 1220px; }\r\r\r[1]: http://en.wikipedia.org/wiki/Content_management_system\r[2]: https://wordpress.org/themes/montezuma\r\r\r\r\r\rThese are the grid options you can choose from in version 1.2.4 of the Montezuma theme. Not bad at all, but if you want to use a responsive layout (and yes, you should) you have to choose between a fixed 960 px maximum width or use the 100% width option which can become ridiculous on large high resolution screens. So, what I did was to choose the 100% option and then add `max-width` to constrain the width. This was an easy way to achieve what I wanted with a minimal adjustment and have relative widths, which I really prefer in a CSS grid system.\rUsing _Chrome Developer Tools_ I identified the CSS rule that determines the page width.\r\r\r\r.row, .row5, .lw { width: 100%; margin: 0 auto; }\r\r\rNow the only remaining thing was to add a new rule with the same selector defining the max-width I found appropriate – which I did to in the content.css file like this.\r\r\r\r.row, .row5, .lw { max-width: 1220px; }\r\r\r[1]: http://en.wikipedia.org/wiki/Content_management_system\r[2]: https://wordpress.org/themes/montezuma\r\r ","dateformatted":"16, November 2014","dateiso":"2014-11-16T17:05:57Z","ref":"/using-a-responsive-css-grid-with-relative-measures/","summary":"When you build a web site on a [CMS][1] and a theme made by someone else you always have some limitations. For this blog I use WordPress and the [Montezuma theme][2], and this is how I customised the default CSS grid options to my liking.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rThese are the grid options you can choose from in version 1.2.4 of the Montezuma theme.","tags":["CSS","Montezuma","Responsive design","WordPress"],"title":"Using a Responsive CSS Grid with Relative Measures"},{"content":"Recently I helped my sister with a new computer. Her previous one was typical low-end desktop machine that you would find in any of the big stores. As such, it was bulky and with time had become very noisy. So when I took the \u0026ldquo;assignment\u0026rdquo; to find a new one I was looking for something configurable, smaller, with a decent value for money and something my sister would perceive as \u0026ldquo;new\u0026rdquo;.\nI have previously built a few desktop machines for myself (with performance in mind rather than low price), but that has also taught me the possible hassle with ordering motherboard, CPU, RAM and so on separately if the machine doesn\u0026rsquo;t boot up when you\u0026rsquo;re done. I wanted to avoid those kind of warranty issues where I have to prove to the store which of the components that is faulty and that I didn\u0026rsquo;t cause the malfunctioning. I also found it frustrating when looking through the assortment range of the on-line stores I use to buy from – a surprisingly large number of machines still have HDD\u0026rsquo;s as their only drive, many of them include very little RAM and some still have USB 2.0 ports (in addition to some USB 3.0 ports). The latter is more of an annoyance than a big problem, but I still cannot understand why on earth you would put USB 2.0 ports in a new machine today, are they really that much cheaper?\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r So, I ended up buying the Gigabyte GB-BXi7H-4500 barebone mini-pc. A really small machine that could still fit a 2.5\u0026quot; SSD I had lying around. The assembling is very easy with clear instructions included. I installed Windows 8.1 on it and the drivers were easy to find on the Gigabyte web site, which isn\u0026rsquo;t always the case. That on the other hand is a must, because the drivers included in the package are on a DVD and this device doesn\u0026rsquo;t have an optical drive (better luck with the thinking next time Gigabyte).\nWith 8 GB RAM, an SSD and a Core i7 CPU I think my sister\u0026rsquo;s computer needs will be satisfied for a few years to come.\n","dateformatted":"11, September 2014","dateiso":"2014-09-11T05:31:08Z","ref":"/building-a-mini-pc-in-2014/","summary":"Recently I helped my sister with a new computer. Her previous one was typical low-end desktop machine that you would find in any of the big stores. As such, it was bulky and with time had become very noisy. So when I took the \u0026ldquo;assignment\u0026rdquo; to find a new one I was looking for something configurable, smaller, with a decent value for money and something my sister would perceive as \u0026ldquo;new\u0026rdquo;.","tags":["Brix","Hardware"],"title":"Building a Mini-PC in 2014"},{"content":"As someone working mostly with SharePoint server-side code, unit tests are something that requires quite some investment in time to get rolling with – and consequently not being done. Javascript is a different thing though. Since a big part of most projects using SharePoint is (or should be) done with Javascript, we should be testing that code. (Of course this applies to any system with a web interface, but I assume most of you that don’t have SharePoint in your CV’s are already doing this).\nI want to share my experience from adding tests to a function written without tests in mind. In the code base I’m working with I found that we had a function that was called from a lot of places and wasn’t always working. It looked like this:\n\r\r\rMAN.getQueryVariable = function(variable, unescape) { var query = window.location.search.substring(1); var vars = query.split(\u0026#39;\u0026amp;\u0026#39;); for (var i = 0; i \u0026amp;lt; vars.length; i++) { var pair = vars[i].split(\u0026#39;=\u0026#39;); if (decodeURIComponent(pair[0]) == variable) { if (unescape) return unescape(pair[1]); return decodeURIComponent(pair[1]); } } return null; };\r\r The problem (the unescape parameter) was quite easy to spot, but I can’t say that the function\u0026rsquo;s inner workings are immediately obvious to me. It has a good name though, so I understand what it does at the higher level. So I wanted to write some tests for the function without having to change it in a way that effected the callers.\nI found out that the unescape function isn’t part of the Javascript language, but rather something that exists on the window object and thus constitutes a dependency. Like all dependencies we want to inject them into the function so that we can inject a faked object when testing. I also prefer to be explicit about where functions belong, i.e. using namespaces and referring to window.unescape() rather than just unescape(). So, here’s my slightly modified version ready for testing.\n\r\r\r// MAN = My Abbreviated Namespace var MAN = MAN || {} MAN.getQueryVariable = function (variable, useUnescape, optionalFakeWindow) { var windowInstance = optionalFakeWindow || window; var query = windowInstance.location.search.substring(1); var vars = query.split(\u0026#39;\u0026amp;\u0026#39;); for (var i = 0; i \u0026amp;lt; vars.length; i++) { var pair = vars[i].split(\u0026#39;=\u0026#39;); if (decodeURIComponent(pair[0]) == variable) { if (useUnescape) return windowInstance.unescape(pair[1]); return decodeURIComponent(pair[1]); } } return null; };\r\r I have found that I like Jasmine as my unit testing library for how the tests (or specs) are expressed and I also find the documentation to be in good shape. As the enthusiastic beginner I am in this area I might have ended up with a slightly overkill kind of test suite, considering the amount of test code compared to the amount of tested code. But hey, you got to start somewhere right? Here’s what I ended up with:\n\r\r\rdescribe(\u0026#34;MAN\u0026#34;, function () { describe(\u0026#34;getQueryVariable()\u0026#34;, function () { var fakeWindow; beforeEach(function () { fakeWindow = { location: { search: \u0026#34;?q=searchedValue\u0026amp;df=280010|290010|447|1100\u0026#34; }, unescape: function () { } }; }); it(\u0026#34;should return null when requested parameter is not present in url\u0026#34;, function () { var value = MAN.getQueryVariable(\u0026#34;parametername\u0026#34;, false, fakeWindow); expect(value).toBe(null); }); it(\u0026#34;should return null when no parameter is present in url\u0026#34;, function () { fakeWindow.location.search = \u0026#34;\u0026#34;; var value = MAN.getQueryVariable(\u0026#34;parametername\u0026#34;, false, fakeWindow); expect(value).toBe(null); }); it(\u0026#34;should return value when requested parameter is present in url\u0026#34;, function () { var value = MAN.getQueryVariable(\u0026#34;q\u0026#34;, false, fakeWindow); expect(value).toBe(\u0026#34;searchedValue\u0026#34;); value = MAN.getQueryVariable(\u0026#34;df\u0026#34;, false, fakeWindow); expect(value).toBe(\u0026#34;280010|290010|447|1100\u0026#34;); }); describe(\u0026#34;window.unescape\u0026#34;, function () { /* decodeURIComponent and window.unescape will in some cases return different results, see example from http://www.w3schools.com/jsref/jsref_decodeuricomponent.asp \u0026#34;st%C3%A5le\u0026#34; will return \u0026#34;ståle\u0026#34; and \u0026#34;stÃ¥le\u0026#34; respectively */ beforeEach(function () { spyOn(fakeWindow, \u0026#34;unescape\u0026#34;); }); afterEach(function () { fakeWindow.unescape.reset(); }); it(\u0026#34;should be used when requested\u0026#34;, function () { var useUnEscape = true; var value = MAN.getQueryVariable(\u0026#34;q\u0026#34;, useUnEscape, fakeWindow); expect(fakeWindow.unescape).toHaveBeenCalled(); }); it(\u0026#34;should not be used when not requested\u0026#34;, function () { var useUnEscape = false; var value = MAN.getQueryVariable(\u0026#34;q\u0026#34;, useUnEscape, fakeWindow); expect(fakeWindow.unescape).not.toHaveBeenCalled(); }); }); }); });\r\r Now I’m at the point where I actually dare to change some stuff, and that’s where I want to be with code. If I make a mistake like in the original code, overriding an existing function with a parameter, my tests would tell me. So, the barrier to unit testing Javascript is indeed low and does not require expensive tools (I used Notepad++ and Chrome initially for this before I integrated it into our Visual Studio project). Green makes one happy, get started testing if you haven’t already! You can download this if you want to try it out: js-tests_henrikpalm.se_2013-04-07.zip\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r ","dateformatted":"7, April 2013","dateiso":"2013-04-07T21:41:14Z","ref":"/a-beginners-experiences-of-unit-testing-javascript/","summary":"As someone working mostly with SharePoint server-side code, unit tests are something that requires quite some investment in time to get rolling with – and consequently not being done. Javascript is a different thing though. Since a big part of most projects using SharePoint is (or should be) done with Javascript, we should be testing that code. (Of course this applies to any system with a web interface, but I assume most of you that don’t have SharePoint in your CV’s are already doing this).","tags":["Jasmine","JavaScript","Scripting","Unit testing"],"title":"A beginner’s experiences of unit testing Javascript"},{"content":"The main reason I use VMware Workstation is the user-friendliness compared to Hyper-V or VirtualBox when using multiple machines in a common virtual network. Today I discovered a thing that could be made easier however. I\u0026rsquo;m mainly writing this for my own reference, if I stumble on the same thing sometime in the future, perhaps someone else does to.\nI copied a group of virtual machines to a new disk and when I migrated them from an earlier version (7) I must have made some error, because the reference to the AD machine was still referring to the old disk. This was discovered when I had removed all the machines from the first disk where I now had other stuff occupying that space. VMware couldn\u0026rsquo;t find the machine at the path it expected it to be at. Fair enough, must be a simple thing to change that path, right?\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rCould not open virtual machine. File not found.\r\r\r\r\r\r\rCould not open virtual machine. File not found.\r\r\r With the AD machine selected every option is greyed out, no prompt \u0026ldquo;would you like to browse for the file\u0026rdquo;, nothing. So I started looking for a settings file, searched the registry for that incorrect path and finally I found it. This is how to fix the issue (I can think of more intuitive ways to support this in the product).\n Close VMware Workstation. Open C:\\Users\\YourAccountName\\AppDataRoaming\\VMware\\inventory.vmls as a text file. Search the path from the error message (in my case C:\\VMImages\\TIBP 2.0\\TIBP - AD\\TIBP - AD.vmx) and change it to the correct path. Open VMware Workstation again and\u0026hellip;voilà, it works! 😊 \r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\rCould not open virtual machine. File not found.\r\r\r\r\r\r\rCould not open virtual machine. File not found.\r\r\r ","dateformatted":"21, January 2013","dateiso":"2013-01-21T14:30:06Z","ref":"/vmware-workstation-9-user-hostility/","summary":"The main reason I use VMware Workstation is the user-friendliness compared to Hyper-V or VirtualBox when using multiple machines in a common virtual network. Today I discovered a thing that could be made easier however. I\u0026rsquo;m mainly writing this for my own reference, if I stumble on the same thing sometime in the future, perhaps someone else does to.\nI copied a group of virtual machines to a new disk and when I migrated them from an earlier version (7) I must have made some error, because the reference to the AD machine was still referring to the old disk.","tags":["VMWare Workstation"],"title":"VMware Workstation 9 User Hostility"},{"content":"Automatic recycling of application pools in IIS may be necessary, but it can annoy the users how happen to be up at night or early in the morning (depending on when the recycles are scheduled). There are numerous warm-up scripts out there and I have just stolen one of them. The purpose of this post is to show you how I set it up to run automatically when the application pools are recycled. The end result is to set it up by executing the following PowerShell command (example from production):\n. .\\ScheduleWarmup.ps1 PROD\rThe application I\u0026rsquo;m maintaining is on the Internet and uses SharePoint 2010. Configuration is automated with a bunch of scripts and I have therefore extended our existing \u0026ldquo;script infrastructure\u0026rdquo; with this warm-up script, thus this may seem somewhat overkill for your needs. In the Post-SharePoint-Installation-Scripts folder we have one folder that holds the environment specific settings and one folder for general helper scripts as shown below.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r I have cleaned up these folders from the scripts aren\u0026rsquo;t relevant for this blog post, but what they have in common is that they load the LoadDependencies.ps1 file that loads the needed helper scripts and settings for the current environment (like dev, test, prod). The help script I have kept in the Helpers folder is the Security.ps1 from the Carbon package to check for administrative permissions.\nThe Warm-up Script I have borrowed the actual warm-up script from Jon Badgett\u0026rsquo;s post Easy SharePoint 2010 warmup Script using PowerShell, but since my site allows anonymous access and the web servers are exposed to the Internet I prefer not to use Get-SPAlternateUrl to find out which application pools to hit, because it requires the script to be run by a high privileged account. I\u0026rsquo;ll hard-code the URLs in the settings files instead. So here is my Warmup.ps1:\n\r\r\rfunction Get-WebPage([string]$url) { $wc = new-object net.webclient;\t$wc.credentials = [System.Net.CredentialCache]::DefaultCredentials; $pageContents = $wc.DownloadString($url); $wc.Dispose(); return $pageContents; } foreach ($url in $input) {\t\u0026#34;Warming up \u0026#39;{0}\u0026#39;...\u0026#34; -F $url; $html = Get-WebPage -url $url; }\r\r Settings and Infrastructure Examples of the settings files look like the following. Having the account that will run the scheduled task in the environment specific files enables you to have different accounts for each environment if your site requires authentication. All the environment specific variables have the Env prefix as a naming convention in our scripts to avoid confusion where they are used. The file path to the warm-up script is where you want that script to be when the job is set up, so you can delete the supporting scripts folder later if you want to.\n\r\r\rWrite-Host \u0026#34;Setting Prod environment Properties\u0026#34; $EnvWarmupUrls = @(\u0026#34;http://ourgreatservice.ourcompany.com/Pages/default.aspx\u0026#34;, \u0026#34;http://ourgreatservice-edit.ourcompany.com/Pages/default.aspx\u0026#34;) $EnvWarmupJobAccount = \u0026#34;NT AUTHORITYNETWORKSERVICE\u0026#34; $EnvWarmupScriptPath = \u0026#34;C:Scheduled scriptsWarmup.ps1\u0026#34;\r\r \r\r\rWrite-Host \u0026#34;Setting Dev environment Properties\u0026#34; $EnvWarmupUrls = @(\u0026#34;http://ourgreatservicedev.ourcompany.com/Pages/default.aspx\u0026#34;, \u0026#34;http://ourgreatservicedev-edit.ourcompany.com/Pages/default.aspx\u0026#34;) $EnvWarmupJobAccount = \u0026#34;NT AUTHORITYNETWORKSERVICE\u0026#34; $EnvWarmupScriptPath = \u0026#34;C:TFS{0}MainScriptsPost-SharePoint-InstallationWarmup.ps1\u0026#34; -F [Environment]::UserName\r\r I\u0026rsquo;ll include the LoadDependencies.ps1 here for reference as well:\n\r\r\r$ScriptPath = Split-Path -Parent $MyInvocation.MyCommand.Definition $HelperScriptsPath = $ScriptPath + \u0026#34;Helpers\u0026#34; Clear-Host # Check arguments, otherwise report argument error ---------------------------- if (!($args.count -eq 1)) { Write-Host -foregroundcolor red \u0026#34;Must specify environment : dev|inttest|at|prod \u0026#34; Write-Host return $false } else { $targetEnv = $args[0] if (!($targetEnv -eq \u0026#34;dev\u0026#34; -or $targetEnv -eq \u0026#34;inttest\u0026#34; -or $targetEnv -eq \u0026#34;at\u0026#34; -or $targetEnv -eq \u0026#34;prod\u0026#34;) ) { Write-Host -foregroundcolor red \u0026#34;Invalid argument for environment:\u0026#34; $targetEnv Write-Host -foregroundcolor red \u0026#34;Possible values: dev|inttest|at|prod\u0026#34; Write-Host return $false } $tmpPath = $ScriptPath + \u0026#34;SettingsSettings_\u0026#34; + $targetEnv + \u0026#34;.ps1\u0026#34; if (!(Test-Path $tmpPath)) { Write-Host -foregroundcolor red \u0026#34;Specified settings file \u0026#39;$tmpPath\u0026#39; does not exists.\u0026#34; Write-Host return $false } $SettingsFilepath = $tmpPath Write-Host -foregroundcolor yellow \u0026#34;# Retrieving local settings from\u0026#34; $SettingsFilepath . $SettingsFilepath } # ----------------------------------------------------------------------------- # Assert Admin Privileges ----------------------------------------------------- $helperSecurityScript = $HelperScriptsPath + \u0026#34;Security.ps1\u0026#34; if (!(Test-Path $helperSecurityScript)) { Write-Host \u0026#34;Specified file \u0026#39;$helperSecurityScript\u0026#39; does not exists.\u0026#34; -ForegroundColor Red Write-Host return $false } Import-Module $helperSecurityScript if(-not (Test-AdminPrivileges)) { Write-Host \u0026#34;You are not currently running with administrative privileges. Please re-start PowerShell as an administrator.\u0026#34; -ForegroundColor Red Write-Host return $false; } # ----------------------------------------------------------------------------- return $true\r\r Setting up the Scheduled Task You can create scheduled tasks with PowerShell 3, but only periodically ones (at least to my understanding). I prefer the approach described by Christopher Maish – monitoring the event log for application pool recycles to trigger our Warmup.ps1 script. That got me to choose the SCHTASKS.EXE instead. There are some extra work done here explained in the comments to make the script more fault-tolerant. Here\u0026rsquo;s ScheduleWarmup.ps1:\n\r\r\r$ScriptPath = Split-Path -Parent $MyInvocation.MyCommand.Definition $dependenciesLoaded = . $ScriptPathLoadDependencies.ps1 $args if (!$dependenciesLoaded) { exit; } if (!(Test-Path $EnvWarmupScriptPath)) { Write-Host -ForegroundColor Red \u0026#34;Specified file \u0026#39;$EnvWarmupScriptPath\u0026#39; does not exist.\u0026#34; exit } try { # Since the argument passed to SCHTASKS /TR (our $command variable) # suffers from Windows file path length limitation, we\u0026#39;ll save the URLs we # want to hit in a text file and load them from there. $warmUpScriptFolder = Split-Path -Parent $EnvWarmupScriptPath $urlsFile = \u0026#34;{0}URLsToWarmUp.txt\u0026#34; -F $warmUpScriptFolder $EnvWarmupUrls \u0026amp;gt; $urlsFile # To support file paths with spaces and such and at the same time avoid the need of getting # a PHD in escape characters, we\u0026#39;ll compose a PowerShell script file on the fly that can be executed # as a scheduled task without parameters. $composedScriptFile = \u0026#34;{0}WarmUpComposed.ps1\u0026#34; -F $warmUpScriptFolder $composedScriptFileContent = \u0026#34;Get-Content \u0026#39;{0}\u0026#39; | . \u0026#39;{1}\u0026#39;\u0026#34; -F $urlsFile, $EnvWarmupScriptPath $composedScriptFileContent \u0026amp;gt; $composedScriptFile $command = \u0026#34;powershell -NoLogo -NonInteractive -WindowStyle Hidden -File \u0026#39;{0}\u0026#39;\u0026#34; -F $composedScriptFile $trigger = \u0026#34;*[System[Provider[@Name=\u0026#39;Microsoft-Windows-WAS\u0026#39;] and ((EventID \u0026amp;gt;= 5074 and EventID \u0026amp;lt;= 5081) or EventID=5117 or EventID=5186)]]\u0026#34; $jobName = \u0026#34;Our Great Service IIS Warmup\u0026#34; } catch { throw; } \u0026#34;Deleting previously existing job named \u0026#39;{0}\u0026#39;...\u0026#34; -F $jobName SCHTASKS /Delete /TN $jobName /F \u0026#34;Creating job named \u0026#39;{0}\u0026#39;...\u0026#34; -F $jobName SCHTASKS /Create /TN $jobName /RU $EnvWarmupJobAccount /RP /TR $command /SC ONEVENT /EC System /MO $trigger\r\r The trigger doesn\u0026rsquo;t look for IISRESET and doesn\u0026rsquo;t look for a specific application pool, so feel free to add that to your script. Now we can copy this folder structure (in the screenshot above) to any of our web servers and easily keep the sites warm (it\u0026rsquo;s snowing outside my window), even our dev environment if we want to.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r\r\r Now you should have a job scheduled in Task Scheduler that you can try by recycling an application pool manually in IIS. Make sure you wait for the job to finish before you call it a failure, the history tab in Task Scheduler says Action Completed when it\u0026rsquo;s done. All scripts used in this post can be downloaded as a ZIP file here: ScheduledWarmupScript_1.zip\n","dateformatted":"17, December 2012","dateiso":"2012-12-17T19:57:04Z","ref":"/setting-up-an-iis-warm-up-script-in-an-automated-fashion/","summary":"Automatic recycling of application pools in IIS may be necessary, but it can annoy the users how happen to be up at night or early in the morning (depending on when the recycles are scheduled). There are numerous warm-up scripts out there and I have just stolen one of them. The purpose of this post is to show you how I set it up to run automatically when the application pools are recycled.","tags":["Internet Information Server","PowerShell","Scripting","SharePoint","Warm-up","Windows Server"],"title":"Setting up an IIS Warm-up Script in an automated fashion"},{"content":"While looking over my backup routines, I stumbled on Scott Hanselman\u0026rsquo;s post Automatically Backup your Gmail account on a schedule with GMVault and Windows Task Scheduler. I had never really thought about backing up my Gmail account, but why not. Even though I\u0026rsquo;m mainly a Windows guy, my Windows 8 machine at home is a big tower that requires quite a lot of electricity and is therefore only powered on when needed. My Mac Mini on the other hand is constantly on acting as a server, so why not run GMVault from there?\nMy experience with configuring Mac OS X is limited, but I have gained enough knowledge to know that there is something called launchd that is the Mac replacement for init and crontab on other POSIX systems. So, here is one way to configure GMVault to run as a \u0026ldquo;scheduled task\u0026rdquo; on Mac OS X Mountain Lion (10.8.2). My local user account is henrik, so that\u0026rsquo;s what I\u0026rsquo;ll use in this example.\n Download GMVault and put the files anywhere you like, I choose /opt/local. Run gmvault sync youremail@gmail.com from the shell and wait for as long as it takes. Use the -d option if you don\u0026rsquo;t want the backup at ~/gmvault-db. Create a small script that runs the GMVault command and does some logging (so you can see if it actually works) \r\r\r#!/bin/bash logger \u0026#34;Starting gmvault at $(date)\u0026#34; echo \u0026#34;Running gmvault at $(date)\u0026#34; \u0026gt; /Users/henrik/backup-gmail.log /opt/local/bin/gmvault sync -t quick youremail@gmail.com \u0026gt;\u0026gt; /Users/henrik/backup-gmail.log logger \u0026#34;Finished gmvault at $(date)\u0026#34;\r\r Apparently I had to use absolute paths here. The logging is of course optional, but handy. If only it was this simple to write to the Windows logs with PowerShell! I chose to write the output from GMVault to a separate log file after I first made an error (not having absolute paths) in the script and the logging just informed me that GMVault finished in 0 seconds. Run the script once to verify that it works as expected.\n I chose a GUI approach and used Lingon to create the agent, but you could just as well use your favourite text editor to create the plist file, this is listed below. Create the agent running as your user, it is necessary for the OAuth authentication to work. Make sure to use a unique name, I chose se.henrikpalm.gmailBackup, and use the full path to the script. To test this, it can be good to choose a time for the agent to run that is a few minutes in the future.\n\r\rfigure.lazy {\rdisplay: none;\r}\r\r\r\r\r\r\r The result of the GUI approach above is a file located at ~/Library/LaunchAgents/se.henrikpalm.gmailBackup.plist, containing the following: \r\r\r\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE plist PUBLIC \u0026#34;-//Apple//DTD PLIST 1.0//EN\u0026#34; \u0026#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd\u0026#34;\u0026gt; \u0026lt;plist version=\u0026#34;1.0\u0026#34;\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Label\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;se.henrikpalm.gmailBackup\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;ProgramArguments\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;/Users/henrik/backup-gmail.sh\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; \u0026lt;key\u0026gt;RunAtLoad\u0026lt;/key\u0026gt; \u0026lt;false/\u0026gt; \u0026lt;key\u0026gt;StartCalendarInterval\u0026lt;/key\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;Hour\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;13\u0026lt;/integer\u0026gt; \u0026lt;key\u0026gt;Minute\u0026lt;/key\u0026gt; \u0026lt;integer\u0026gt;0\u0026lt;/integer\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;key\u0026gt;StartOnMount\u0026lt;/key\u0026gt; \u0026lt;false/\u0026gt; \u0026lt;/dict\u0026gt; \u0026lt;/plist\u0026gt;\r\r\n Start the agent.\n launchctl load ~/Library/LaunchAgents/se.henrikpalm.gmailBackup.plist\rAny configuration change in the agent requires it to be reloaded.\r launchctl unload ~/Library/LaunchAgents/se.henrikpalm.gmailBackup.plist; launchctl load ~/Library/LaunchAgents/se.henrikpalm.gmailBackup.plist\rCheck the logs. tail -F -n 20 /var/log/system.log | grep gmvault\rWhen the agent runs you should see something like this:\r Dec 5 13:00:03 Mini.local henrik[93574]: Starting gmvault at Wed Dec 5 13:00:03 CET 2012\rDec 5 13:00:24 Mini.local henrik[93589]: Finished gmvault at Wed Dec 5 13:00:24 CET 2012\rDone! Time Machine will now take care of backing up the local backup 😃 ","dateformatted":"10, December 2012","dateiso":"2012-12-10T21:09:15Z","ref":"/automatically-backup-your-gmail-account-on-a-schedule-with-gmvault-and-mac-os-x-launchd/","summary":"While looking over my backup routines, I stumbled on Scott Hanselman\u0026rsquo;s post Automatically Backup your Gmail account on a schedule with GMVault and Windows Task Scheduler. I had never really thought about backing up my Gmail account, but why not. Even though I\u0026rsquo;m mainly a Windows guy, my Windows 8 machine at home is a big tower that requires quite a lot of electricity and is therefore only powered on when needed.","tags":["Backup","Gmail","GMVault","macOS","Scripting"],"title":"Automatically Backup your Gmail account on a schedule with GMVault and Mac OS X launchd"}]